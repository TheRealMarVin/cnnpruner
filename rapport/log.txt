C:\Users\naked\AppData\Local\conda\conda\envs\7030\python.exe C:/dev/cnnpruner/POC.py
Files already downloaded and verified
Files already downloaded and verified
***  SqueezeNet-0
C:\Users\naked\AppData\Local\conda\conda\envs\7030\lib\site-packages\torchvision\models\squeezenet.py:94: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.
  init.kaiming_uniform(m.weight.data)
C:\Users\naked\AppData\Local\conda\conda\envs\7030\lib\site-packages\torchvision\models\squeezenet.py:92: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, mean=0.0, std=0.01)
number of flops: 516412928.0 	number of params: 1235496.0
C:\dev\cnnpruner\deeplib_ext\CustomDeepLib.py:129: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs = Variable(inputs, volatile=True)
C:\dev\cnnpruner\deeplib_ext\CustomDeepLib.py:130: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  targets = Variable(targets, volatile=True)
Test:
	Score: 0.0
Epoch 0 - Train acc: 42.48 - Val acc: 41.97 - Train loss: 1.5099 - Val loss: 1.5282 - Training time: 99.31s
Epoch 1 - Train acc: 65.35 - Val acc: 64.34 - Train loss: 0.9871 - Val loss: 1.0120 - Training time: 98.71s
Epoch 2 - Train acc: 75.48 - Val acc: 74.16 - Train loss: 0.6953 - Val loss: 0.7534 - Training time: 97.77s
Epoch 3 - Train acc: 85.40 - Val acc: 83.28 - Train loss: 0.4224 - Val loss: 0.4885 - Training time: 97.74s
Epoch 4 - Train acc: 86.97 - Val acc: 84.21 - Train loss: 0.3785 - Val loss: 0.4643 - Training time: 97.64s
Epoch 5 - Train acc: 88.75 - Val acc: 85.87 - Train loss: 0.3263 - Val loss: 0.4212 - Training time: 97.53s
Epoch 6 - Train acc: 85.89 - Val acc: 82.79 - Train loss: 0.3953 - Val loss: 0.5008 - Training time: 97.56s
Epoch 7 - Train acc: 90.26 - Val acc: 86.65 - Train loss: 0.2778 - Val loss: 0.3912 - Training time: 97.12s
Epoch 8 - Train acc: 92.66 - Val acc: 87.96 - Train loss: 0.2199 - Val loss: 0.3445 - Training time: 97.29s
Epoch 9 - Train acc: 91.56 - Val acc: 87.20 - Train loss: 0.2427 - Val loss: 0.3884 - Training time: 97.44s
Epoch 10 - Train acc: 91.00 - Val acc: 86.91 - Train loss: 0.2485 - Val loss: 0.4049 - Training time: 97.10s
Epoch 11 - Train acc: 92.99 - Val acc: 88.21 - Train loss: 0.1970 - Val loss: 0.3644 - Training time: 97.14s
Epoch 12 - Train acc: 93.49 - Val acc: 87.98 - Train loss: 0.1807 - Val loss: 0.3639 - Training time: 97.14s
Epoch 13 - Train acc: 92.75 - Val acc: 87.58 - Train loss: 0.1993 - Val loss: 0.3930 - Training time: 97.26s
Epoch 14 - Train acc: 94.23 - Val acc: 88.08 - Train loss: 0.1646 - Val loss: 0.3638 - Training time: 97.17s
end number of flops: 516412928.0 	number of params: 1235496.0
Final Test:
	Score: 88.22
***  SqueezeNet-30
number of flops: 516412928.0 	number of params: 1235496.0
Epoch 0 - Train acc: 29.72 - Val acc: 30.17 - Train loss: 1.7383 - Val loss: 1.7343 - Training time: 96.82s
Epoch 1 - Train acc: 55.82 - Val acc: 55.18 - Train loss: 1.1959 - Val loss: 1.2059 - Training time: 97.64s
Epoch 2 - Train acc: 76.00 - Val acc: 75.46 - Train loss: 0.7018 - Val loss: 0.7197 - Training time: 97.84s
Epoch 3 - Train acc: 83.71 - Val acc: 82.23 - Train loss: 0.4785 - Val loss: 0.5153 - Training time: 97.87s
Epoch 4 - Train acc: 86.73 - Val acc: 84.89 - Train loss: 0.3837 - Val loss: 0.4346 - Training time: 97.95s
Test:
	Score: 84.69
C:\Users\naked\AppData\Local\conda\conda\envs\7030\lib\site-packages\torch\onnx\symbolic.py:131: UserWarning: ONNX export failed on max_pool2d_with_indices because ceil_mode not supported
  warnings.warn("ONNX export failed on " + op + " because " + msg + " not supported")
6 iterations to reduce 30.00% filters
Perform pruning iteration: 0
Layers that will be pruned {'features.0': 2}
convolution remaining after pruning {'features.0': 62}
Pruning filters.. 
Filters pruned 4.6875%
Test:
	post prune Score: 85.24000000000001
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 88.94 - Val acc: 87.71 - Train loss: 0.3246 - Val loss: 0.3497 - Training time: 96.81s
Test pruning iteration :0
	Score: 86.52
Perform pruning iteration: 1
Layers that will be pruned {'features.0': 2}
convolution remaining after pruning {'features.0': 60}
Pruning filters.. 
Filters pruned 4.6875%
Test:
	post prune Score: 85.49
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 90.18 - Val acc: 88.80 - Train loss: 0.2885 - Val loss: 0.3222 - Training time: 98.25s
Test pruning iteration :1
	Score: 86.99
Perform pruning iteration: 2
Layers that will be pruned {'features.0': 2}
convolution remaining after pruning {'features.0': 58}
Pruning filters.. 
Filters pruned 4.6875%
Test:
	post prune Score: 86.44
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 90.14 - Val acc: 88.44 - Train loss: 0.2826 - Val loss: 0.3320 - Training time: 97.23s
Test pruning iteration :2
	Score: 86.9
Perform pruning iteration: 3
Layers that will be pruned {'features.0': 2}
convolution remaining after pruning {'features.0': 56}
Pruning filters.. 
Filters pruned 4.6875%
Test:
	post prune Score: 88.42
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 91.14 - Val acc: 90.11 - Train loss: 0.2582 - Val loss: 0.2790 - Training time: 98.06s
Test pruning iteration :3
	Score: 87.94999999999999
Perform pruning iteration: 4
Layers that will be pruned {'features.0': 2}
convolution remaining after pruning {'features.0': 54}
Pruning filters.. 
Filters pruned 4.6875%
Test:
	post prune Score: 87.64
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 91.18 - Val acc: 90.24 - Train loss: 0.2544 - Val loss: 0.2836 - Training time: 97.44s
Test pruning iteration :4
	Score: 87.78
Perform pruning iteration: 5
Layers that will be pruned {'features.0': 2}
convolution remaining after pruning {'features.0': 52}
Pruning filters.. 
Filters pruned 4.6875%
Test:
	post prune Score: 88.64
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 92.05 - Val acc: 90.44 - Train loss: 0.2326 - Val loss: 0.2756 - Training time: 97.57s
Test pruning iteration :5
	Score: 87.8
Epoch 0 - Train acc: 93.25 - Val acc: 92.41 - Train loss: 0.1955 - Val loss: 0.2201 - Training time: 97.36s
  Epoch 1 - Train acc: 92.12 - Val acc: 90.68 - Train loss: 0.2230 - Val loss: 0.2644 - Training time: 97.48s
Epoch 2 - Train acc: 94.73 - Val acc: 92.80 - Train loss: 0.1569 - Val loss: 0.2123 - Training time: 97.99s
Epoch 3 - Train acc: 94.57 - Val acc: 91.99 - Train loss: 0.1578 - Val loss: 0.2254 - Training time: 98.77s
end number of flops: 510559040.0 	number of params: 1234968.0
Final Test:
	Score: 88.87
***  densenet 121-0
C:\Users\naked\AppData\Local\conda\conda\envs\7030\lib\site-packages\torchvision\models\densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  nn.init.kaiming_normal(m.weight.data)
number of flops: 2914598912.0 	number of params: 7978856.0
Test:
	Score: 0.0
Epoch 0 - Train acc: 96.31 - Val acc: 93.00 - Train loss: 0.1141 - Val loss: 0.2019 - Training time: 400.98s
Epoch 1 - Train acc: 98.19 - Val acc: 94.58 - Train loss: 0.0567 - Val loss: 0.1630 - Training time: 408.71s
Epoch 2 - Train acc: 99.35 - Val acc: 95.70 - Train loss: 0.0236 - Val loss: 0.1295 - Training time: 394.07s
Epoch 3 - Train acc: 99.40 - Val acc: 95.26 - Train loss: 0.0198 - Val loss: 0.1501 - Training time: 392.88s
Epoch 4 - Train acc: 99.69 - Val acc: 95.62 - Train loss: 0.0125 - Val loss: 0.1436 - Training time: 387.04s
Epoch 5 - Train acc: 99.61 - Val acc: 95.44 - Train loss: 0.0126 - Val loss: 0.1519 - Training time: 387.70s
Epoch 6 - Train acc: 99.88 - Val acc: 96.01 - Train loss: 0.0056 - Val loss: 0.1410 - Training time: 386.19s
Epoch 7 - Train acc: 99.92 - Val acc: 95.89 - Train loss: 0.0036 - Val loss: 0.1457 - Training time: 388.21s
Epoch 8 - Train acc: 99.98 - Val acc: 96.38 - Train loss: 0.0020 - Val loss: 0.1317 - Training time: 386.53s
Epoch 9 - Train acc: 99.94 - Val acc: 96.15 - Train loss: 0.0022 - Val loss: 0.1405 - Training time: 386.18s
Epoch 10 - Train acc: 99.97 - Val acc: 96.08 - Train loss: 0.0017 - Val loss: 0.1484 - Training time: 386.06s
Epoch 11 - Train acc: 99.99 - Val acc: 96.07 - Train loss: 0.0012 - Val loss: 0.1511 - Training time: 386.14s
Epoch 12 - Train acc: 99.95 - Val acc: 95.95 - Train loss: 0.0020 - Val loss: 0.1572 - Training time: 386.85s
 Epoch 13 - Train acc: 99.99 - Val acc: 96.20 - Train loss: 0.0007 - Val loss: 0.1422 - Training time: 395.54s
Epoch 14 - Train acc: 99.98 - Val acc: 96.12 - Train loss: 0.0013 - Val loss: 0.1480 - Training time: 403.52s
end number of flops: 2914598912.0 	number of params: 7978856.0
Final Test:
	Score: 95.99
***  densenet 121-30
number of flops: 2914598912.0 	number of params: 7978856.0
Epoch 0 - Train acc: 96.52 - Val acc: 93.86 - Train loss: 0.1085 - Val loss: 0.1778 - Training time: 398.87s
Epoch 1 - Train acc: 98.03 - Val acc: 94.86 - Train loss: 0.0599 - Val loss: 0.1582 - Training time: 405.51s
Epoch 2 - Train acc: 99.06 - Val acc: 95.38 - Train loss: 0.0307 - Val loss: 0.1337 - Training time: 399.01s
Epoch 3 - Train acc: 99.72 - Val acc: 96.17 - Train loss: 0.0119 - Val loss: 0.1172 - Training time: 397.73s
Epoch 4 - Train acc: 99.74 - Val acc: 95.68 - Train loss: 0.0097 - Val loss: 0.1392 - Training time: 399.05s
Test:
	Score: 95.64
6 iterations to reduce 30.00% filters
Perform pruning iteration: 0
Layers that will be pruned {'features.denseblock4.denselayer7.conv1': 3, 'features.denseblock4.denselayer11.conv1': 17, 'features.denseblock4.denselayer12.conv1': 15, 'features.denseblock4.denselayer13.conv1': 15, 'features.denseblock4.denselayer14.conv1': 14, 'features.denseblock4.denselayer15.conv1': 15, 'features.denseblock1.denselayer1.conv1': 9, 'features.denseblock1.denselayer2.conv1': 12, 'features.denseblock4.denselayer16.conv2': 26, 'features.denseblock4.denselayer5.conv1': 9, 'features.denseblock3.denselayer3.conv1': 4, 'features.denseblock3.denselayer14.conv1': 5, 'features.denseblock4.denselayer8.conv1': 8, 'features.denseblock3.denselayer13.conv1': 3, 'features.denseblock3.denselayer17.conv1': 6, 'features.denseblock2.denselayer6.conv1': 6, 'features.denseblock3.denselayer20.conv1': 3, 'features.denseblock3.denselayer7.conv1': 1, 'features.denseblock3.denselayer9.conv1': 6, 'features.denseblock1.denselayer3.conv1': 10, 'features.denseblock2.denselayer9.conv1': 7, 'features.denseblock2.denselayer1.conv1': 14, 'features.denseblock3.denselayer1.conv1': 4, 'features.denseblock2.denselayer8.conv1': 1, 'features.denseblock1.denselayer6.conv1': 5, 'features.denseblock2.denselayer11.conv1': 3, 'features.denseblock2.denselayer12.conv1': 5, 'features.denseblock4.denselayer2.conv1': 5, 'features.denseblock3.denselayer4.conv1': 7, 'features.denseblock4.denselayer9.conv1': 7, 'features.denseblock4.denselayer6.conv1': 8, 'features.denseblock4.denselayer16.conv1': 14, 'features.denseblock3.denselayer16.conv1': 2, 'features.denseblock4.denselayer3.conv1': 4, 'features.denseblock1.denselayer5.conv1': 7, 'features.denseblock3.denselayer21.conv1': 2, 'features.denseblock2.denselayer3.conv1': 4, 'features.denseblock3.denselayer2.conv1': 5, 'features.denseblock1.denselayer4.conv1': 6, 'features.denseblock4.denselayer1.conv1': 4, 'features.denseblock3.denselayer24.conv1': 3, 'features.denseblock3.denselayer15.conv1': 3, 'features.denseblock4.denselayer10.conv1': 6, 'features.denseblock4.denselayer4.conv1': 11, 'features.denseblock3.denselayer6.conv1': 5, 'features.denseblock2.denselayer4.conv1': 4, 'features.denseblock3.denselayer11.conv1': 2, 'features.denseblock2.denselayer10.conv1': 5, 'features.denseblock3.denselayer8.conv1': 3, 'features.denseblock3.denselayer12.conv1': 2, 'features.denseblock3.denselayer22.conv1': 5, 'features.denseblock2.denselayer5.conv1': 3, 'features.denseblock3.denselayer10.conv1': 4, 'features.denseblock3.denselayer5.conv1': 3, 'features.denseblock2.denselayer7.conv1': 2, 'features.denseblock3.denselayer18.conv1': 4, 'features.denseblock3.denselayer23.conv1': 4, 'features.denseblock3.denselayer19.conv1': 1, 'features.denseblock2.denselayer2.conv1': 1}
convolution remaining after pruning {'features.denseblock4.denselayer7.conv1': 125, 'features.denseblock4.denselayer11.conv1': 111, 'features.denseblock4.denselayer12.conv1': 113, 'features.denseblock4.denselayer13.conv1': 113, 'features.denseblock4.denselayer14.conv1': 114, 'features.denseblock4.denselayer15.conv1': 113, 'features.denseblock1.denselayer1.conv1': 119, 'features.denseblock1.denselayer2.conv1': 116, 'features.denseblock4.denselayer16.conv2': 6, 'features.denseblock4.denselayer5.conv1': 119, 'features.denseblock3.denselayer3.conv1': 124, 'features.denseblock3.denselayer14.conv1': 123, 'features.denseblock4.denselayer8.conv1': 120, 'features.denseblock3.denselayer13.conv1': 125, 'features.denseblock3.denselayer17.conv1': 122, 'features.denseblock2.denselayer6.conv1': 122, 'features.denseblock3.denselayer20.conv1': 125, 'features.denseblock3.denselayer7.conv1': 127, 'features.denseblock3.denselayer9.conv1': 122, 'features.denseblock1.denselayer3.conv1': 118, 'features.denseblock2.denselayer9.conv1': 121, 'features.denseblock2.denselayer1.conv1': 114, 'features.denseblock3.denselayer1.conv1': 124, 'features.denseblock2.denselayer8.conv1': 127, 'features.denseblock1.denselayer6.conv1': 123, 'features.denseblock2.denselayer11.conv1': 125, 'features.denseblock2.denselayer12.conv1': 123, 'features.denseblock4.denselayer2.conv1': 123, 'features.denseblock3.denselayer4.conv1': 121, 'features.denseblock4.denselayer9.conv1': 121, 'features.denseblock4.denselayer6.conv1': 120, 'features.denseblock4.denselayer16.conv1': 114, 'features.denseblock3.denselayer16.conv1': 126, 'features.denseblock4.denselayer3.conv1': 124, 'features.denseblock1.denselayer5.conv1': 121, 'features.denseblock3.denselayer21.conv1': 126, 'features.denseblock2.denselayer3.conv1': 124, 'features.denseblock3.denselayer2.conv1': 123, 'features.denseblock1.denselayer4.conv1': 122, 'features.denseblock4.denselayer1.conv1': 124, 'features.denseblock3.denselayer24.conv1': 125, 'features.denseblock3.denselayer15.conv1': 125, 'features.denseblock4.denselayer10.conv1': 122, 'features.denseblock4.denselayer4.conv1': 117, 'features.denseblock3.denselayer6.conv1': 123, 'features.denseblock2.denselayer4.conv1': 124, 'features.denseblock3.denselayer11.conv1': 126, 'features.denseblock2.denselayer10.conv1': 123, 'features.denseblock3.denselayer8.conv1': 125, 'features.denseblock3.denselayer12.conv1': 126, 'features.denseblock3.denselayer22.conv1': 123, 'features.denseblock2.denselayer5.conv1': 125, 'features.denseblock3.denselayer10.conv1': 124, 'features.denseblock3.denselayer5.conv1': 125, 'features.denseblock2.denselayer7.conv1': 126, 'features.denseblock3.denselayer18.conv1': 124, 'features.denseblock3.denselayer23.conv1': 124, 'features.denseblock3.denselayer19.conv1': 127, 'features.denseblock2.denselayer2.conv1': 127}
Pruning filters.. 
Filters pruned 4.989270386266094%
Test:
	post prune Score: 9.99
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 96.66 - Val acc: 94.58 - Train loss: 0.1164 - Val loss: 0.1740 - Training time: 396.05s
Test pruning iteration :0
	Score: 93.37
Perform pruning iteration: 1
Layers that will be pruned {'features.denseblock3.denselayer8.conv1': 3, 'features.denseblock3.denselayer15.conv1': 3, 'features.denseblock4.denselayer5.conv1': 4, 'features.denseblock4.denselayer8.conv1': 6, 'features.denseblock4.denselayer13.conv1': 16, 'features.denseblock4.denselayer11.conv1': 7, 'features.denseblock1.denselayer5.conv1': 14, 'features.denseblock1.denselayer3.conv1': 13, 'features.denseblock4.denselayer12.conv1': 4, 'features.denseblock3.denselayer24.conv1': 2, 'features.denseblock2.denselayer1.conv1': 9, 'features.denseblock2.denselayer2.conv1': 9, 'features.denseblock3.denselayer20.conv1': 5, 'features.denseblock3.denselayer12.conv1': 3, 'features.denseblock4.denselayer7.conv1': 6, 'features.denseblock1.denselayer1.conv1': 7, 'features.denseblock3.denselayer2.conv1': 9, 'features.denseblock4.denselayer4.conv1': 9, 'features.denseblock2.denselayer4.conv1': 3, 'features.denseblock4.denselayer2.conv1': 7, 'features.denseblock1.denselayer2.conv1': 15, 'features.denseblock3.denselayer3.conv1': 6, 'features.denseblock2.denselayer6.conv1': 12, 'features.denseblock4.denselayer9.conv1': 9, 'features.denseblock2.denselayer11.conv1': 7, 'features.denseblock4.denselayer3.conv1': 10, 'features.denseblock4.denselayer16.conv1': 11, 'features.denseblock3.denselayer10.conv1': 7, 'features.denseblock3.denselayer16.conv1': 4, 'features.denseblock3.denselayer18.conv1': 5, 'features.denseblock3.denselayer17.conv1': 4, 'features.denseblock4.denselayer15.conv1': 5, 'features.denseblock2.denselayer3.conv1': 6, 'features.denseblock3.denselayer11.conv1': 4, 'features.denseblock4.denselayer1.conv1': 6, 'features.denseblock3.denselayer21.conv1': 7, 'features.denseblock3.denselayer13.conv1': 2, 'features.denseblock4.denselayer16.conv2': 1, 'features.denseblock3.denselayer5.conv1': 6, 'features.denseblock2.denselayer12.conv1': 8, 'features.denseblock3.denselayer1.conv1': 2, 'features.denseblock2.denselayer9.conv1': 4, 'features.denseblock2.denselayer7.conv1': 8, 'features.denseblock3.denselayer6.conv1': 4, 'features.denseblock2.denselayer8.conv1': 8, 'features.denseblock3.denselayer23.conv1': 5, 'features.denseblock3.denselayer19.conv1': 5, 'features.denseblock3.denselayer4.conv1': 5, 'features.denseblock4.denselayer14.conv1': 5, 'features.denseblock2.denselayer10.conv1': 4, 'features.denseblock1.denselayer6.conv1': 8, 'features.denseblock3.denselayer9.conv1': 4, 'features.denseblock4.denselayer6.conv1': 8, 'features.denseblock3.denselayer22.conv1': 7, 'features.denseblock1.denselayer4.conv1': 6, 'features.denseblock3.denselayer7.conv1': 4, 'features.denseblock4.denselayer10.conv1': 5, 'features.denseblock2.denselayer5.conv1': 5, 'features.denseblock3.denselayer14.conv1': 1}
convolution remaining after pruning {'features.denseblock3.denselayer8.conv1': 122, 'features.denseblock3.denselayer15.conv1': 122, 'features.denseblock4.denselayer5.conv1': 115, 'features.denseblock4.denselayer8.conv1': 114, 'features.denseblock4.denselayer13.conv1': 97, 'features.denseblock4.denselayer11.conv1': 104, 'features.denseblock1.denselayer5.conv1': 107, 'features.denseblock1.denselayer3.conv1': 105, 'features.denseblock4.denselayer12.conv1': 109, 'features.denseblock3.denselayer24.conv1': 123, 'features.denseblock2.denselayer1.conv1': 105, 'features.denseblock2.denselayer2.conv1': 118, 'features.denseblock3.denselayer20.conv1': 120, 'features.denseblock3.denselayer12.conv1': 123, 'features.denseblock4.denselayer7.conv1': 119, 'features.denseblock1.denselayer1.conv1': 112, 'features.denseblock3.denselayer2.conv1': 114, 'features.denseblock4.denselayer4.conv1': 108, 'features.denseblock2.denselayer4.conv1': 121, 'features.denseblock4.denselayer2.conv1': 116, 'features.denseblock1.denselayer2.conv1': 101, 'features.denseblock3.denselayer3.conv1': 118, 'features.denseblock2.denselayer6.conv1': 110, 'features.denseblock4.denselayer9.conv1': 112, 'features.denseblock2.denselayer11.conv1': 118, 'features.denseblock4.denselayer3.conv1': 114, 'features.denseblock4.denselayer16.conv1': 103, 'features.denseblock3.denselayer10.conv1': 117, 'features.denseblock3.denselayer16.conv1': 122, 'features.denseblock3.denselayer18.conv1': 119, 'features.denseblock3.denselayer17.conv1': 118, 'features.denseblock4.denselayer15.conv1': 108, 'features.denseblock2.denselayer3.conv1': 118, 'features.denseblock3.denselayer11.conv1': 122, 'features.denseblock4.denselayer1.conv1': 118, 'features.denseblock3.denselayer21.conv1': 119, 'features.denseblock3.denselayer13.conv1': 123, 'features.denseblock4.denselayer16.conv2': 5, 'features.denseblock3.denselayer5.conv1': 119, 'features.denseblock2.denselayer12.conv1': 115, 'features.denseblock3.denselayer1.conv1': 122, 'features.denseblock2.denselayer9.conv1': 117, 'features.denseblock2.denselayer7.conv1': 118, 'features.denseblock3.denselayer6.conv1': 119, 'features.denseblock2.denselayer8.conv1': 119, 'features.denseblock3.denselayer23.conv1': 119, 'features.denseblock3.denselayer19.conv1': 122, 'features.denseblock3.denselayer4.conv1': 116, 'features.denseblock4.denselayer14.conv1': 109, 'features.denseblock2.denselayer10.conv1': 119, 'features.denseblock1.denselayer6.conv1': 115, 'features.denseblock3.denselayer9.conv1': 118, 'features.denseblock4.denselayer6.conv1': 112, 'features.denseblock3.denselayer22.conv1': 116, 'features.denseblock1.denselayer4.conv1': 116, 'features.denseblock3.denselayer7.conv1': 123, 'features.denseblock4.denselayer10.conv1': 117, 'features.denseblock2.denselayer5.conv1': 120, 'features.denseblock3.denselayer14.conv1': 122}
Pruning filters.. 
Filters pruned 4.989270386266094%
Test:
	post prune Score: 10.0
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 97.25 - Val acc: 95.50 - Train loss: 0.0931 - Val loss: 0.1451 - Training time: 393.98s
Test pruning iteration :1
	Score: 93.63
Perform pruning iteration: 2
Layers that will be pruned {'features.denseblock3.denselayer12.conv1': 7, 'features.denseblock4.denselayer2.conv1': 5, 'features.denseblock2.denselayer2.conv1': 5, 'features.denseblock4.denselayer3.conv1': 5, 'features.denseblock4.denselayer8.conv1': 7, 'features.denseblock3.denselayer22.conv1': 5, 'features.denseblock4.denselayer11.conv1': 6, 'features.denseblock3.denselayer5.conv1': 6, 'features.denseblock2.denselayer10.conv1': 7, 'features.denseblock4.denselayer6.conv1': 7, 'features.denseblock3.denselayer14.conv1': 5, 'features.denseblock1.denselayer5.conv1': 7, 'features.denseblock2.denselayer5.conv1': 6, 'features.denseblock2.denselayer11.conv1': 7, 'features.denseblock3.denselayer11.conv1': 9, 'features.denseblock4.denselayer15.conv1': 13, 'features.denseblock3.denselayer8.conv1': 3, 'features.denseblock2.denselayer3.conv1': 9, 'features.denseblock4.denselayer9.conv1': 6, 'features.denseblock1.denselayer1.conv1': 17, 'features.denseblock3.denselayer4.conv1': 8, 'features.denseblock3.denselayer16.conv1': 5, 'features.denseblock3.denselayer3.conv1': 10, 'features.denseblock4.denselayer16.conv1': 9, 'features.denseblock2.denselayer8.conv1': 8, 'features.denseblock3.denselayer15.conv1': 4, 'features.denseblock4.denselayer10.conv1': 7, 'features.denseblock4.denselayer4.conv1': 6, 'features.denseblock2.denselayer9.conv1': 8, 'features.denseblock2.denselayer12.conv1': 5, 'features.denseblock4.denselayer14.conv1': 9, 'features.denseblock4.denselayer1.conv1': 4, 'features.denseblock1.denselayer2.conv1': 3, 'features.denseblock2.denselayer1.conv1': 8, 'features.denseblock3.denselayer13.conv1': 7, 'features.denseblock3.denselayer24.conv1': 10, 'features.denseblock1.denselayer4.conv1': 5, 'features.denseblock3.denselayer2.conv1': 9, 'features.denseblock3.denselayer6.conv1': 3, 'features.denseblock2.denselayer6.conv1': 6, 'features.denseblock3.denselayer23.conv1': 2, 'features.denseblock2.denselayer7.conv1': 8, 'features.denseblock3.denselayer1.conv1': 6, 'features.denseblock3.denselayer17.conv1': 7, 'features.denseblock3.denselayer7.conv1': 5, 'features.denseblock4.denselayer13.conv1': 8, 'features.denseblock3.denselayer9.conv1': 7, 'features.denseblock4.denselayer5.conv1': 7, 'features.denseblock1.denselayer6.conv1': 6, 'features.denseblock3.denselayer19.conv1': 6, 'features.denseblock3.denselayer18.conv1': 4, 'features.denseblock3.denselayer21.conv1': 4, 'features.denseblock2.denselayer4.conv1': 4, 'features.denseblock3.denselayer20.conv1': 5, 'features.denseblock1.denselayer3.conv1': 9, 'features.denseblock4.denselayer12.conv1': 4, 'features.denseblock4.denselayer7.conv1': 1, 'features.denseblock3.denselayer10.conv1': 3}
convolution remaining after pruning {'features.denseblock3.denselayer12.conv1': 116, 'features.denseblock4.denselayer2.conv1': 111, 'features.denseblock2.denselayer2.conv1': 113, 'features.denseblock4.denselayer3.conv1': 109, 'features.denseblock4.denselayer8.conv1': 107, 'features.denseblock3.denselayer22.conv1': 111, 'features.denseblock4.denselayer11.conv1': 98, 'features.denseblock3.denselayer5.conv1': 113, 'features.denseblock2.denselayer10.conv1': 112, 'features.denseblock4.denselayer6.conv1': 105, 'features.denseblock3.denselayer14.conv1': 117, 'features.denseblock1.denselayer5.conv1': 100, 'features.denseblock2.denselayer5.conv1': 114, 'features.denseblock2.denselayer11.conv1': 111, 'features.denseblock3.denselayer11.conv1': 113, 'features.denseblock4.denselayer15.conv1': 95, 'features.denseblock3.denselayer8.conv1': 119, 'features.denseblock2.denselayer3.conv1': 109, 'features.denseblock4.denselayer9.conv1': 106, 'features.denseblock1.denselayer1.conv1': 95, 'features.denseblock3.denselayer4.conv1': 108, 'features.denseblock3.denselayer16.conv1': 117, 'features.denseblock3.denselayer3.conv1': 108, 'features.denseblock4.denselayer16.conv1': 94, 'features.denseblock2.denselayer8.conv1': 111, 'features.denseblock3.denselayer15.conv1': 118, 'features.denseblock4.denselayer10.conv1': 110, 'features.denseblock4.denselayer4.conv1': 102, 'features.denseblock2.denselayer9.conv1': 109, 'features.denseblock2.denselayer12.conv1': 110, 'features.denseblock4.denselayer14.conv1': 100, 'features.denseblock4.denselayer1.conv1': 114, 'features.denseblock1.denselayer2.conv1': 98, 'features.denseblock2.denselayer1.conv1': 97, 'features.denseblock3.denselayer13.conv1': 116, 'features.denseblock3.denselayer24.conv1': 113, 'features.denseblock1.denselayer4.conv1': 111, 'features.denseblock3.denselayer2.conv1': 105, 'features.denseblock3.denselayer6.conv1': 116, 'features.denseblock2.denselayer6.conv1': 104, 'features.denseblock3.denselayer23.conv1': 117, 'features.denseblock2.denselayer7.conv1': 110, 'features.denseblock3.denselayer1.conv1': 116, 'features.denseblock3.denselayer17.conv1': 111, 'features.denseblock3.denselayer7.conv1': 118, 'features.denseblock4.denselayer13.conv1': 89, 'features.denseblock3.denselayer9.conv1': 111, 'features.denseblock4.denselayer5.conv1': 108, 'features.denseblock1.denselayer6.conv1': 109, 'features.denseblock3.denselayer19.conv1': 116, 'features.denseblock3.denselayer18.conv1': 115, 'features.denseblock3.denselayer21.conv1': 115, 'features.denseblock2.denselayer4.conv1': 117, 'features.denseblock3.denselayer20.conv1': 115, 'features.denseblock1.denselayer3.conv1': 96, 'features.denseblock4.denselayer12.conv1': 105, 'features.denseblock4.denselayer7.conv1': 118, 'features.denseblock3.denselayer10.conv1': 114}
Pruning filters.. 
Filters pruned 4.989270386266094%
Test:
	post prune Score: 10.0
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 98.05 - Val acc: 96.16 - Train loss: 0.0702 - Val loss: 0.1147 - Training time: 394.24s
Test pruning iteration :2
	Score: 94.69
Perform pruning iteration: 3
Layers that will be pruned {'features.denseblock3.denselayer19.conv1': 9, 'features.denseblock2.denselayer10.conv1': 7, 'features.denseblock3.denselayer12.conv1': 6, 'features.denseblock3.denselayer8.conv1': 12, 'features.denseblock3.denselayer11.conv1': 7, 'features.denseblock1.denselayer3.conv1': 7, 'features.denseblock3.denselayer2.conv1': 13, 'features.denseblock1.denselayer2.conv1': 10, 'features.denseblock1.denselayer6.conv1': 7, 'features.denseblock3.denselayer15.conv1': 8, 'features.denseblock4.denselayer4.conv1': 4, 'features.denseblock4.denselayer7.conv1': 12, 'features.denseblock1.denselayer4.conv1': 7, 'features.denseblock2.denselayer11.conv1': 6, 'features.denseblock4.denselayer16.conv1': 7, 'features.denseblock3.denselayer1.conv1': 11, 'features.denseblock3.denselayer22.conv1': 7, 'features.denseblock2.denselayer3.conv1': 6, 'features.denseblock3.denselayer23.conv1': 7, 'features.denseblock4.denselayer6.conv1': 5, 'features.denseblock2.denselayer2.conv1': 9, 'features.denseblock2.denselayer7.conv1': 6, 'features.denseblock3.denselayer6.conv1': 9, 'features.denseblock4.denselayer15.conv1': 8, 'features.denseblock3.denselayer4.conv1': 7, 'features.denseblock3.denselayer17.conv1': 5, 'features.denseblock1.denselayer1.conv1': 4, 'features.denseblock3.denselayer5.conv1': 9, 'features.denseblock3.denselayer7.conv1': 7, 'features.denseblock4.denselayer3.conv1': 3, 'features.denseblock3.denselayer9.conv1': 5, 'features.denseblock1.denselayer5.conv1': 6, 'features.denseblock2.denselayer1.conv1': 8, 'features.denseblock3.denselayer24.conv1': 5, 'features.denseblock3.denselayer21.conv1': 9, 'features.denseblock3.denselayer14.conv1': 4, 'features.denseblock4.denselayer11.conv1': 3, 'features.denseblock2.denselayer9.conv1': 4, 'features.denseblock4.denselayer10.conv1': 5, 'features.denseblock2.denselayer5.conv1': 7, 'features.denseblock2.denselayer4.conv1': 5, 'features.denseblock3.denselayer13.conv1': 11, 'features.denseblock3.denselayer10.conv1': 3, 'features.denseblock2.denselayer12.conv1': 5, 'features.denseblock4.denselayer13.conv1': 6, 'features.denseblock3.denselayer18.conv1': 6, 'features.denseblock4.denselayer2.conv1': 3, 'features.denseblock4.denselayer5.conv1': 3, 'features.denseblock4.denselayer1.conv1': 5, 'features.denseblock3.denselayer20.conv1': 6, 'features.denseblock2.denselayer8.conv1': 5, 'features.denseblock3.denselayer16.conv1': 6, 'features.denseblock4.denselayer8.conv1': 6, 'features.denseblock3.denselayer3.conv1': 4, 'features.denseblock4.denselayer12.conv1': 8, 'features.denseblock4.denselayer14.conv1': 6, 'features.denseblock2.denselayer6.conv1': 2, 'features.denseblock4.denselayer9.conv1': 1}
convolution remaining after pruning {'features.denseblock3.denselayer19.conv1': 107, 'features.denseblock2.denselayer10.conv1': 105, 'features.denseblock3.denselayer12.conv1': 110, 'features.denseblock3.denselayer8.conv1': 107, 'features.denseblock3.denselayer11.conv1': 106, 'features.denseblock1.denselayer3.conv1': 89, 'features.denseblock3.denselayer2.conv1': 92, 'features.denseblock1.denselayer2.conv1': 88, 'features.denseblock1.denselayer6.conv1': 102, 'features.denseblock3.denselayer15.conv1': 110, 'features.denseblock4.denselayer4.conv1': 98, 'features.denseblock4.denselayer7.conv1': 106, 'features.denseblock1.denselayer4.conv1': 104, 'features.denseblock2.denselayer11.conv1': 105, 'features.denseblock4.denselayer16.conv1': 87, 'features.denseblock3.denselayer1.conv1': 105, 'features.denseblock3.denselayer22.conv1': 104, 'features.denseblock2.denselayer3.conv1': 103, 'features.denseblock3.denselayer23.conv1': 110, 'features.denseblock4.denselayer6.conv1': 100, 'features.denseblock2.denselayer2.conv1': 104, 'features.denseblock2.denselayer7.conv1': 104, 'features.denseblock3.denselayer6.conv1': 107, 'features.denseblock4.denselayer15.conv1': 87, 'features.denseblock3.denselayer4.conv1': 101, 'features.denseblock3.denselayer17.conv1': 106, 'features.denseblock1.denselayer1.conv1': 91, 'features.denseblock3.denselayer5.conv1': 104, 'features.denseblock3.denselayer7.conv1': 111, 'features.denseblock4.denselayer3.conv1': 106, 'features.denseblock3.denselayer9.conv1': 106, 'features.denseblock1.denselayer5.conv1': 94, 'features.denseblock2.denselayer1.conv1': 89, 'features.denseblock3.denselayer24.conv1': 108, 'features.denseblock3.denselayer21.conv1': 106, 'features.denseblock3.denselayer14.conv1': 113, 'features.denseblock4.denselayer11.conv1': 95, 'features.denseblock2.denselayer9.conv1': 105, 'features.denseblock4.denselayer10.conv1': 105, 'features.denseblock2.denselayer5.conv1': 107, 'features.denseblock2.denselayer4.conv1': 112, 'features.denseblock3.denselayer13.conv1': 105, 'features.denseblock3.denselayer10.conv1': 111, 'features.denseblock2.denselayer12.conv1': 105, 'features.denseblock4.denselayer13.conv1': 83, 'features.denseblock3.denselayer18.conv1': 109, 'features.denseblock4.denselayer2.conv1': 108, 'features.denseblock4.denselayer5.conv1': 105, 'features.denseblock4.denselayer1.conv1': 109, 'features.denseblock3.denselayer20.conv1': 109, 'features.denseblock2.denselayer8.conv1': 106, 'features.denseblock3.denselayer16.conv1': 111, 'features.denseblock4.denselayer8.conv1': 101, 'features.denseblock3.denselayer3.conv1': 104, 'features.denseblock4.denselayer12.conv1': 97, 'features.denseblock4.denselayer14.conv1': 94, 'features.denseblock2.denselayer6.conv1': 102, 'features.denseblock4.denselayer9.conv1': 105}
Pruning filters.. 
Filters pruned 4.989270386266094%
Test:
	post prune Score: 10.0
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 97.86 - Val acc: 95.86 - Train loss: 0.0701 - Val loss: 0.1213 - Training time: 392.93s
Test pruning iteration :3
	Score: 94.28999999999999
Perform pruning iteration: 4
Layers that will be pruned {'features.denseblock4.denselayer12.conv1': 3, 'features.denseblock1.denselayer1.conv1': 13, 'features.denseblock2.denselayer1.conv1': 3, 'features.denseblock3.denselayer3.conv1': 8, 'features.denseblock3.denselayer2.conv1': 5, 'features.denseblock2.denselayer10.conv1': 7, 'features.denseblock3.denselayer21.conv1': 8, 'features.denseblock3.denselayer10.conv1': 5, 'features.denseblock3.denselayer20.conv1': 5, 'features.denseblock3.denselayer19.conv1': 7, 'features.denseblock4.denselayer4.conv1': 5, 'features.denseblock4.denselayer1.conv1': 5, 'features.denseblock3.denselayer14.conv1': 8, 'features.denseblock4.denselayer3.conv1': 4, 'features.denseblock4.denselayer14.conv1': 4, 'features.denseblock4.denselayer9.conv1': 4, 'features.denseblock4.denselayer7.conv1': 9, 'features.denseblock3.denselayer17.conv1': 5, 'features.denseblock2.denselayer11.conv1': 6, 'features.denseblock3.denselayer8.conv1': 7, 'features.denseblock4.denselayer11.conv1': 5, 'features.denseblock4.denselayer2.conv1': 6, 'features.denseblock3.denselayer23.conv1': 11, 'features.denseblock3.denselayer4.conv1': 4, 'features.denseblock4.denselayer5.conv1': 6, 'features.denseblock3.denselayer13.conv1': 7, 'features.denseblock3.denselayer6.conv1': 9, 'features.denseblock1.denselayer5.conv1': 6, 'features.denseblock4.denselayer13.conv1': 6, 'features.denseblock2.denselayer8.conv1': 8, 'features.denseblock2.denselayer3.conv1': 7, 'features.denseblock3.denselayer11.conv1': 4, 'features.denseblock2.denselayer5.conv1': 11, 'features.denseblock3.denselayer22.conv1': 5, 'features.denseblock3.denselayer9.conv1': 6, 'features.denseblock4.denselayer15.conv1': 8, 'features.denseblock2.denselayer6.conv1': 5, 'features.denseblock3.denselayer18.conv1': 4, 'features.denseblock4.denselayer6.conv1': 3, 'features.denseblock1.denselayer4.conv1': 13, 'features.denseblock2.denselayer9.conv1': 9, 'features.denseblock2.denselayer7.conv1': 7, 'features.denseblock3.denselayer12.conv1': 11, 'features.denseblock3.denselayer15.conv1': 11, 'features.denseblock3.denselayer7.conv1': 7, 'features.denseblock1.denselayer6.conv1': 11, 'features.denseblock2.denselayer2.conv1': 5, 'features.denseblock2.denselayer4.conv1': 5, 'features.denseblock4.denselayer16.conv1': 5, 'features.denseblock4.denselayer8.conv1': 5, 'features.denseblock1.denselayer3.conv1': 4, 'features.denseblock3.denselayer16.conv1': 8, 'features.denseblock3.denselayer1.conv1': 7, 'features.denseblock4.denselayer10.conv1': 3, 'features.denseblock1.denselayer2.conv1': 5, 'features.denseblock3.denselayer5.conv1': 4, 'features.denseblock3.denselayer24.conv1': 4, 'features.denseblock2.denselayer12.conv1': 6}
convolution remaining after pruning {'features.denseblock4.denselayer12.conv1': 94, 'features.denseblock1.denselayer1.conv1': 78, 'features.denseblock2.denselayer1.conv1': 86, 'features.denseblock3.denselayer3.conv1': 96, 'features.denseblock3.denselayer2.conv1': 87, 'features.denseblock2.denselayer10.conv1': 98, 'features.denseblock3.denselayer21.conv1': 98, 'features.denseblock3.denselayer10.conv1': 106, 'features.denseblock3.denselayer20.conv1': 104, 'features.denseblock3.denselayer19.conv1': 100, 'features.denseblock4.denselayer4.conv1': 93, 'features.denseblock4.denselayer1.conv1': 104, 'features.denseblock3.denselayer14.conv1': 105, 'features.denseblock4.denselayer3.conv1': 102, 'features.denseblock4.denselayer14.conv1': 90, 'features.denseblock4.denselayer9.conv1': 101, 'features.denseblock4.denselayer7.conv1': 97, 'features.denseblock3.denselayer17.conv1': 101, 'features.denseblock2.denselayer11.conv1': 99, 'features.denseblock3.denselayer8.conv1': 100, 'features.denseblock4.denselayer11.conv1': 90, 'features.denseblock4.denselayer2.conv1': 102, 'features.denseblock3.denselayer23.conv1': 99, 'features.denseblock3.denselayer4.conv1': 97, 'features.denseblock4.denselayer5.conv1': 99, 'features.denseblock3.denselayer13.conv1': 98, 'features.denseblock3.denselayer6.conv1': 98, 'features.denseblock1.denselayer5.conv1': 88, 'features.denseblock4.denselayer13.conv1': 77, 'features.denseblock2.denselayer8.conv1': 98, 'features.denseblock2.denselayer3.conv1': 96, 'features.denseblock3.denselayer11.conv1': 102, 'features.denseblock2.denselayer5.conv1': 96, 'features.denseblock3.denselayer22.conv1': 99, 'features.denseblock3.denselayer9.conv1': 100, 'features.denseblock4.denselayer15.conv1': 79, 'features.denseblock2.denselayer6.conv1': 97, 'features.denseblock3.denselayer18.conv1': 105, 'features.denseblock4.denselayer6.conv1': 97, 'features.denseblock1.denselayer4.conv1': 91, 'features.denseblock2.denselayer9.conv1': 96, 'features.denseblock2.denselayer7.conv1': 97, 'features.denseblock3.denselayer12.conv1': 99, 'features.denseblock3.denselayer15.conv1': 99, 'features.denseblock3.denselayer7.conv1': 104, 'features.denseblock1.denselayer6.conv1': 91, 'features.denseblock2.denselayer2.conv1': 99, 'features.denseblock2.denselayer4.conv1': 107, 'features.denseblock4.denselayer16.conv1': 82, 'features.denseblock4.denselayer8.conv1': 96, 'features.denseblock1.denselayer3.conv1': 85, 'features.denseblock3.denselayer16.conv1': 103, 'features.denseblock3.denselayer1.conv1': 98, 'features.denseblock4.denselayer10.conv1': 102, 'features.denseblock1.denselayer2.conv1': 83, 'features.denseblock3.denselayer5.conv1': 100, 'features.denseblock3.denselayer24.conv1': 104, 'features.denseblock2.denselayer12.conv1': 99}
Pruning filters.. 
Filters pruned 4.989270386266094%
Test:
	post prune Score: 10.0
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 98.30 - Val acc: 96.69 - Train loss: 0.0595 - Val loss: 0.1083 - Training time: 387.28s
Test pruning iteration :4
	Score: 94.35
Perform pruning iteration: 5
Layers that will be pruned {'features.denseblock3.denselayer15.conv1': 9, 'features.denseblock4.denselayer3.conv1': 6, 'features.denseblock3.denselayer12.conv1': 1, 'features.denseblock2.denselayer4.conv1': 7, 'features.denseblock2.denselayer12.conv1': 8, 'features.denseblock4.denselayer1.conv1': 11, 'features.denseblock4.denselayer16.conv1': 8, 'features.denseblock1.denselayer3.conv1': 3, 'features.denseblock4.denselayer6.conv1': 7, 'features.denseblock4.denselayer15.conv1': 8, 'features.denseblock2.denselayer6.conv1': 10, 'features.denseblock1.denselayer6.conv1': 6, 'features.denseblock4.denselayer11.conv1': 7, 'features.denseblock1.denselayer1.conv1': 6, 'features.denseblock3.denselayer8.conv1': 6, 'features.denseblock3.denselayer5.conv1': 9, 'features.denseblock3.denselayer6.conv1': 7, 'features.denseblock3.denselayer19.conv1': 7, 'features.denseblock4.denselayer13.conv1': 9, 'features.denseblock1.denselayer2.conv1': 9, 'features.denseblock3.denselayer3.conv1': 4, 'features.denseblock3.denselayer9.conv1': 9, 'features.denseblock3.denselayer23.conv1': 5, 'features.denseblock3.denselayer10.conv1': 8, 'features.denseblock4.denselayer2.conv1': 11, 'features.denseblock2.denselayer2.conv1': 10, 'features.denseblock4.denselayer8.conv1': 5, 'features.denseblock3.denselayer24.conv1': 5, 'features.denseblock4.denselayer12.conv1': 3, 'features.denseblock3.denselayer7.conv1': 9, 'features.denseblock3.denselayer14.conv1': 7, 'features.denseblock3.denselayer4.conv1': 6, 'features.denseblock3.denselayer13.conv1': 6, 'features.denseblock3.denselayer1.conv1': 5, 'features.denseblock3.denselayer22.conv1': 6, 'features.denseblock2.denselayer11.conv1': 8, 'features.denseblock3.denselayer18.conv1': 6, 'features.denseblock2.denselayer9.conv1': 6, 'features.denseblock4.denselayer14.conv1': 5, 'features.denseblock3.denselayer2.conv1': 8, 'features.denseblock2.denselayer7.conv1': 6, 'features.denseblock4.denselayer4.conv1': 3, 'features.denseblock4.denselayer10.conv1': 5, 'features.denseblock2.denselayer8.conv1': 3, 'features.denseblock3.denselayer16.conv1': 5, 'features.denseblock2.denselayer10.conv1': 5, 'features.denseblock2.denselayer5.conv1': 6, 'features.denseblock3.denselayer11.conv1': 8, 'features.denseblock2.denselayer1.conv1': 7, 'features.denseblock1.denselayer5.conv1': 2, 'features.denseblock3.denselayer17.conv1': 7, 'features.denseblock3.denselayer21.conv1': 6, 'features.denseblock2.denselayer3.conv1': 9, 'features.denseblock4.denselayer7.conv1': 7, 'features.denseblock3.denselayer20.conv1': 8, 'features.denseblock4.denselayer5.conv1': 4, 'features.denseblock1.denselayer4.conv1': 4, 'features.denseblock4.denselayer9.conv1': 1}
convolution remaining after pruning {'features.denseblock3.denselayer15.conv1': 90, 'features.denseblock4.denselayer3.conv1': 96, 'features.denseblock3.denselayer12.conv1': 98, 'features.denseblock2.denselayer4.conv1': 100, 'features.denseblock2.denselayer12.conv1': 91, 'features.denseblock4.denselayer1.conv1': 93, 'features.denseblock4.denselayer16.conv1': 74, 'features.denseblock1.denselayer3.conv1': 82, 'features.denseblock4.denselayer6.conv1': 90, 'features.denseblock4.denselayer15.conv1': 71, 'features.denseblock2.denselayer6.conv1': 87, 'features.denseblock1.denselayer6.conv1': 85, 'features.denseblock4.denselayer11.conv1': 83, 'features.denseblock1.denselayer1.conv1': 72, 'features.denseblock3.denselayer8.conv1': 94, 'features.denseblock3.denselayer5.conv1': 91, 'features.denseblock3.denselayer6.conv1': 91, 'features.denseblock3.denselayer19.conv1': 93, 'features.denseblock4.denselayer13.conv1': 68, 'features.denseblock1.denselayer2.conv1': 74, 'features.denseblock3.denselayer3.conv1': 92, 'features.denseblock3.denselayer9.conv1': 91, 'features.denseblock3.denselayer23.conv1': 94, 'features.denseblock3.denselayer10.conv1': 98, 'features.denseblock4.denselayer2.conv1': 91, 'features.denseblock2.denselayer2.conv1': 89, 'features.denseblock4.denselayer8.conv1': 91, 'features.denseblock3.denselayer24.conv1': 99, 'features.denseblock4.denselayer12.conv1': 91, 'features.denseblock3.denselayer7.conv1': 95, 'features.denseblock3.denselayer14.conv1': 98, 'features.denseblock3.denselayer4.conv1': 91, 'features.denseblock3.denselayer13.conv1': 92, 'features.denseblock3.denselayer1.conv1': 93, 'features.denseblock3.denselayer22.conv1': 93, 'features.denseblock2.denselayer11.conv1': 91, 'features.denseblock3.denselayer18.conv1': 99, 'features.denseblock2.denselayer9.conv1': 90, 'features.denseblock4.denselayer14.conv1': 85, 'features.denseblock3.denselayer2.conv1': 79, 'features.denseblock2.denselayer7.conv1': 91, 'features.denseblock4.denselayer4.conv1': 90, 'features.denseblock4.denselayer10.conv1': 97, 'features.denseblock2.denselayer8.conv1': 95, 'features.denseblock3.denselayer16.conv1': 98, 'features.denseblock2.denselayer10.conv1': 93, 'features.denseblock2.denselayer5.conv1': 90, 'features.denseblock3.denselayer11.conv1': 94, 'features.denseblock2.denselayer1.conv1': 79, 'features.denseblock1.denselayer5.conv1': 86, 'features.denseblock3.denselayer17.conv1': 94, 'features.denseblock3.denselayer21.conv1': 92, 'features.denseblock2.denselayer3.conv1': 87, 'features.denseblock4.denselayer7.conv1': 90, 'features.denseblock3.denselayer20.conv1': 96, 'features.denseblock4.denselayer5.conv1': 95, 'features.denseblock1.denselayer4.conv1': 87, 'features.denseblock4.denselayer9.conv1': 100}
Pruning filters.. 
Filters pruned 4.989270386266094%
Test:
	post prune Score: 10.0
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 98.22 - Val acc: 96.67 - Train loss: 0.0633 - Val loss: 0.1070 - Training time: 387.03s
Test pruning iteration :5
	Score: 94.16
Epoch 0 - Train acc: 99.04 - Val acc: 97.77 - Train loss: 0.0365 - Val loss: 0.0682 - Training time: 387.57s
Epoch 1 - Train acc: 99.09 - Val acc: 97.09 - Train loss: 0.0323 - Val loss: 0.0847 - Training time: 387.45s
Epoch 2 - Train acc: 99.41 - Val acc: 97.29 - Train loss: 0.0217 - Val loss: 0.0793 - Training time: 387.73s
Epoch 3 - Train acc: 99.78 - Val acc: 97.56 - Train loss: 0.0116 - Val loss: 0.0681 - Training time: 387.47s
end number of flops: 2142279040.0 	number of params: 6116770.0
Final Test:
	Score: 95.30999999999999
***  Resnet 50-0
number of flops: 4138573824.0 	number of params: 23528522.0
Test:
	Score: 9.55
Epoch 0 - Train acc: 97.25 - Val acc: 94.52 - Train loss: 0.0908 - Val loss: 0.1615 - Training time: 196.70s
Epoch 1 - Train acc: 99.24 - Val acc: 95.60 - Train loss: 0.0290 - Val loss: 0.1241 - Training time: 212.51s
Epoch 2 - Train acc: 99.72 - Val acc: 96.05 - Train loss: 0.0131 - Val loss: 0.1232 - Training time: 221.02s
Epoch 3 - Train acc: 99.92 - Val acc: 96.37 - Train loss: 0.0060 - Val loss: 0.1177 - Training time: 214.48s
Epoch 4 - Train acc: 99.92 - Val acc: 96.17 - Train loss: 0.0040 - Val loss: 0.1269 - Training time: 200.63s
Epoch 5 - Train acc: 99.97 - Val acc: 96.42 - Train loss: 0.0020 - Val loss: 0.1256 - Training time: 200.21s
Epoch 6 - Train acc: 99.99 - Val acc: 96.40 - Train loss: 0.0011 - Val loss: 0.1268 - Training time: 208.20s
Epoch 7 - Train acc: 100.00 - Val acc: 96.49 - Train loss: 0.0009 - Val loss: 0.1256 - Training time: 200.77s
Epoch 8 - Train acc: 100.00 - Val acc: 96.75 - Train loss: 0.0004 - Val loss: 0.1235 - Training time: 207.92s
Epoch 9 - Train acc: 100.00 - Val acc: 96.77 - Train loss: 0.0004 - Val loss: 0.1218 - Training time: 200.21s
Epoch 10 - Train acc: 99.99 - Val acc: 96.79 - Train loss: 0.0005 - Val loss: 0.1198 - Training time: 197.17s
Epoch 11 - Train acc: 100.00 - Val acc: 96.62 - Train loss: 0.0003 - Val loss: 0.1269 - Training time: 197.14s
Epoch 12 - Train acc: 100.00 - Val acc: 96.66 - Train loss: 0.0003 - Val loss: 0.1259 - Training time: 199.07s
Epoch 13 - Train acc: 100.00 - Val acc: 96.62 - Train loss: 0.0003 - Val loss: 0.1309 - Training time: 202.13s
Epoch 14 - Train acc: 100.00 - Val acc: 96.70 - Train loss: 0.0002 - Val loss: 0.1280 - Training time: 198.83s
end number of flops: 4138573824.0 	number of params: 23528522.0
Final Test:
	Score: 96.32
***  Resnet 50-30
number of flops: 4138573824.0 	number of params: 23528522.0
Epoch 0 - Train acc: 96.41 - Val acc: 94.07 - Train loss: 0.1068 - Val loss: 0.1714 - Training time: 200.08s
Epoch 1 - Train acc: 99.32 - Val acc: 96.09 - Train loss: 0.0275 - Val loss: 0.1139 - Training time: 200.58s
Epoch 2 - Train acc: 99.74 - Val acc: 96.01 - Train loss: 0.0133 - Val loss: 0.1227 - Training time: 201.01s
Epoch 3 - Train acc: 99.92 - Val acc: 96.33 - Train loss: 0.0056 - Val loss: 0.1191 - Training time: 200.39s
Epoch 4 - Train acc: 99.96 - Val acc: 96.43 - Train loss: 0.0029 - Val loss: 0.1224 - Training time: 201.84s
Test:
	Score: 96.21
6 iterations to reduce 30.00% filters
Perform pruning iteration: 0
Layers that will be pruned {'layer1.0.conv1': 5, 'layer1.0.conv2': 4, 'layer1.1.conv1': 4, 'layer4.0.conv1': 32, 'layer4.1.conv2': 32, 'layer3.5.conv2': 18, 'layer4.2.conv1': 36, 'layer2.0.conv1': 4, 'layer4.2.conv2': 56, 'layer4.0.conv2': 22, 'layer3.4.conv1': 15, 'layer3.4.conv2': 11, 'layer3.3.conv1': 5, 'layer3.5.conv1': 15, 'layer4.1.conv1': 37, 'layer3.0.conv1': 6, 'layer3.3.conv2': 10, 'layer2.1.conv1': 7, 'layer3.2.conv1': 5, 'layer2.3.conv2': 2, 'layer3.2.conv2': 14, 'layer3.1.conv1': 9, 'layer3.0.conv2': 8, 'layer3.1.conv2': 3, 'layer1.2.conv2': 2, 'layer2.1.conv2': 2, 'layer2.2.conv2': 6, 'layer2.3.conv1': 4, 'layer2.0.conv2': 1, 'layer1.2.conv1': 2}
convolution remaining after pruning {'layer1.0.conv1': 59, 'layer1.0.conv2': 60, 'layer1.1.conv1': 60, 'layer4.0.conv1': 480, 'layer4.1.conv2': 480, 'layer3.5.conv2': 238, 'layer4.2.conv1': 476, 'layer2.0.conv1': 124, 'layer4.2.conv2': 456, 'layer4.0.conv2': 490, 'layer3.4.conv1': 241, 'layer3.4.conv2': 245, 'layer3.3.conv1': 251, 'layer3.5.conv1': 241, 'layer4.1.conv1': 475, 'layer3.0.conv1': 250, 'layer3.3.conv2': 246, 'layer2.1.conv1': 121, 'layer3.2.conv1': 251, 'layer2.3.conv2': 126, 'layer3.2.conv2': 242, 'layer3.1.conv1': 247, 'layer3.0.conv2': 248, 'layer3.1.conv2': 253, 'layer1.2.conv2': 62, 'layer2.1.conv2': 126, 'layer2.2.conv2': 122, 'layer2.3.conv1': 124, 'layer2.0.conv2': 127, 'layer1.2.conv1': 62}
Pruning filters.. 
Filters pruned 4.992055084745763%
Test:
	post prune Score: 12.3
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 96.35 - Val acc: 93.40 - Train loss: 0.1117 - Val loss: 0.1903 - Training time: 201.54s
Test pruning iteration :0
	Score: 92.17
Perform pruning iteration: 1
Layers that will be pruned {'layer2.0.conv2': 5, 'layer3.3.conv2': 15, 'layer3.4.conv2': 16, 'layer3.5.conv2': 21, 'layer4.1.conv1': 30, 'layer4.2.conv2': 25, 'layer4.2.conv1': 27, 'layer4.0.conv1': 36, 'layer3.0.conv1': 12, 'layer4.1.conv2': 40, 'layer4.0.conv2': 37, 'layer3.3.conv1': 9, 'layer3.0.conv2': 9, 'layer1.1.conv1': 3, 'layer3.2.conv1': 11, 'layer3.5.conv1': 10, 'layer3.2.conv2': 13, 'layer2.1.conv1': 3, 'layer3.1.conv1': 13, 'layer1.0.conv1': 3, 'layer2.2.conv2': 1, 'layer3.1.conv2': 14, 'layer3.4.conv1': 9, 'layer2.3.conv1': 3, 'layer2.3.conv2': 3, 'layer2.1.conv2': 2, 'layer1.0.conv2': 2, 'layer2.2.conv1': 2, 'layer2.0.conv1': 2, 'layer1.2.conv1': 1}
convolution remaining after pruning {'layer2.0.conv2': 122, 'layer3.3.conv2': 231, 'layer3.4.conv2': 229, 'layer3.5.conv2': 217, 'layer4.1.conv1': 445, 'layer4.2.conv2': 431, 'layer4.2.conv1': 449, 'layer4.0.conv1': 444, 'layer3.0.conv1': 238, 'layer4.1.conv2': 440, 'layer4.0.conv2': 453, 'layer3.3.conv1': 242, 'layer3.0.conv2': 239, 'layer1.1.conv1': 57, 'layer3.2.conv1': 240, 'layer3.5.conv1': 231, 'layer3.2.conv2': 229, 'layer2.1.conv1': 118, 'layer3.1.conv1': 234, 'layer1.0.conv1': 56, 'layer2.2.conv2': 121, 'layer3.1.conv2': 239, 'layer3.4.conv1': 232, 'layer2.3.conv1': 121, 'layer2.3.conv2': 123, 'layer2.1.conv2': 124, 'layer1.0.conv2': 58, 'layer2.2.conv1': 126, 'layer2.0.conv1': 122, 'layer1.2.conv1': 61}
Pruning filters.. 
Filters pruned 4.992055084745763%
Test:
	post prune Score: 10.0
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 97.65 - Val acc: 94.82 - Train loss: 0.0740 - Val loss: 0.1485 - Training time: 190.73s
Test pruning iteration :1
	Score: 93.27
Perform pruning iteration: 2
Layers that will be pruned {'layer2.3.conv2': 1, 'layer4.0.conv2': 34, 'layer4.2.conv1': 32, 'layer3.0.conv2': 11, 'layer2.1.conv1': 4, 'layer3.5.conv2': 6, 'layer3.3.conv2': 15, 'layer4.1.conv2': 28, 'layer3.5.conv1': 14, 'layer2.0.conv2': 6, 'layer4.2.conv2': 37, 'layer3.1.conv1': 13, 'layer3.1.conv2': 8, 'layer3.2.conv2': 10, 'layer3.3.conv1': 13, 'layer4.0.conv1': 30, 'layer3.4.conv1': 15, 'layer3.2.conv1': 11, 'layer3.4.conv2': 11, 'layer2.2.conv1': 8, 'layer4.1.conv1': 38, 'layer1.0.conv1': 1, 'layer2.1.conv2': 4, 'layer3.0.conv1': 10, 'layer2.0.conv1': 6, 'layer1.2.conv1': 1, 'layer1.0.conv2': 3, 'layer1.1.conv1': 2, 'layer2.3.conv1': 3, 'layer1.2.conv2': 1, 'layer2.2.conv2': 1}
convolution remaining after pruning {'layer2.3.conv2': 122, 'layer4.0.conv2': 419, 'layer4.2.conv1': 417, 'layer3.0.conv2': 228, 'layer2.1.conv1': 114, 'layer3.5.conv2': 211, 'layer3.3.conv2': 216, 'layer4.1.conv2': 412, 'layer3.5.conv1': 217, 'layer2.0.conv2': 116, 'layer4.2.conv2': 394, 'layer3.1.conv1': 221, 'layer3.1.conv2': 231, 'layer3.2.conv2': 219, 'layer3.3.conv1': 229, 'layer4.0.conv1': 414, 'layer3.4.conv1': 217, 'layer3.2.conv1': 229, 'layer3.4.conv2': 218, 'layer2.2.conv1': 118, 'layer4.1.conv1': 407, 'layer1.0.conv1': 55, 'layer2.1.conv2': 120, 'layer3.0.conv1': 228, 'layer2.0.conv1': 116, 'layer1.2.conv1': 60, 'layer1.0.conv2': 55, 'layer1.1.conv1': 55, 'layer2.3.conv1': 118, 'layer1.2.conv2': 61, 'layer2.2.conv2': 120}
Pruning filters.. 
Filters pruned 4.992055084745763%
Test:
	post prune Score: 10.0
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 97.67 - Val acc: 94.89 - Train loss: 0.0744 - Val loss: 0.1549 - Training time: 193.29s
Test pruning iteration :2
	Score: 93.25
Perform pruning iteration: 3
Layers that will be pruned {'layer3.0.conv2': 8, 'layer3.2.conv2': 11, 'layer3.5.conv2': 10, 'layer3.3.conv1': 10, 'layer4.2.conv2': 31, 'layer4.0.conv1': 32, 'layer4.1.conv2': 39, 'layer3.2.conv1': 10, 'layer3.4.conv2': 9, 'layer1.2.conv1': 3, 'layer3.1.conv1': 11, 'layer2.1.conv1': 7, 'layer4.0.conv2': 31, 'layer4.1.conv1': 32, 'layer2.2.conv2': 7, 'layer3.4.conv1': 16, 'layer1.0.conv1': 2, 'layer1.0.conv2': 3, 'layer4.2.conv1': 24, 'layer2.3.conv1': 7, 'layer3.5.conv1': 12, 'layer3.0.conv1': 9, 'layer3.3.conv2': 13, 'layer3.1.conv2': 11, 'layer2.0.conv1': 3, 'layer1.1.conv2': 1, 'layer2.2.conv1': 6, 'layer2.0.conv2': 3, 'layer2.3.conv2': 3, 'layer2.1.conv2': 10, 'layer1.1.conv1': 1, 'layer1.2.conv2': 2}
convolution remaining after pruning {'layer3.0.conv2': 220, 'layer3.2.conv2': 208, 'layer3.5.conv2': 201, 'layer3.3.conv1': 219, 'layer4.2.conv2': 363, 'layer4.0.conv1': 382, 'layer4.1.conv2': 373, 'layer3.2.conv1': 219, 'layer3.4.conv2': 209, 'layer1.2.conv1': 57, 'layer3.1.conv1': 210, 'layer2.1.conv1': 107, 'layer4.0.conv2': 388, 'layer4.1.conv1': 375, 'layer2.2.conv2': 113, 'layer3.4.conv1': 201, 'layer1.0.conv1': 53, 'layer1.0.conv2': 52, 'layer4.2.conv1': 393, 'layer2.3.conv1': 111, 'layer3.5.conv1': 205, 'layer3.0.conv1': 219, 'layer3.3.conv2': 203, 'layer3.1.conv2': 220, 'layer2.0.conv1': 113, 'layer1.1.conv2': 63, 'layer2.2.conv1': 112, 'layer2.0.conv2': 113, 'layer2.3.conv2': 119, 'layer2.1.conv2': 110, 'layer1.1.conv1': 54, 'layer1.2.conv2': 59}
Pruning filters.. 
Filters pruned 4.992055084745763%
Test:
	post prune Score: 10.0
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 97.25 - Val acc: 95.02 - Train loss: 0.0835 - Val loss: 0.1498 - Training time: 190.00s
Test pruning iteration :3
	Score: 92.36
Perform pruning iteration: 4
Layers that will be pruned {'layer2.0.conv2': 4, 'layer2.1.conv2': 3, 'layer2.1.conv1': 8, 'layer2.2.conv2': 4, 'layer4.1.conv2': 41, 'layer3.3.conv1': 15, 'layer1.2.conv2': 1, 'layer4.0.conv2': 28, 'layer3.0.conv1': 10, 'layer4.1.conv1': 30, 'layer4.2.conv1': 33, 'layer4.0.conv1': 33, 'layer3.4.conv2': 15, 'layer3.1.conv2': 10, 'layer3.2.conv1': 13, 'layer2.0.conv1': 6, 'layer3.2.conv2': 8, 'layer3.0.conv2': 12, 'layer3.4.conv1': 10, 'layer2.3.conv1': 3, 'layer3.5.conv2': 10, 'layer3.3.conv2': 18, 'layer2.2.conv1': 6, 'layer1.0.conv2': 1, 'layer4.2.conv2': 23, 'layer1.1.conv2': 3, 'layer3.1.conv1': 11, 'layer3.5.conv1': 9, 'layer1.2.conv1': 1, 'layer1.1.conv1': 1, 'layer1.0.conv1': 3, 'layer2.3.conv2': 4}
convolution remaining after pruning {'layer2.0.conv2': 109, 'layer2.1.conv2': 107, 'layer2.1.conv1': 99, 'layer2.2.conv2': 109, 'layer4.1.conv2': 332, 'layer3.3.conv1': 204, 'layer1.2.conv2': 58, 'layer4.0.conv2': 360, 'layer3.0.conv1': 209, 'layer4.1.conv1': 345, 'layer4.2.conv1': 360, 'layer4.0.conv1': 349, 'layer3.4.conv2': 194, 'layer3.1.conv2': 210, 'layer3.2.conv1': 206, 'layer2.0.conv1': 107, 'layer3.2.conv2': 200, 'layer3.0.conv2': 208, 'layer3.4.conv1': 191, 'layer2.3.conv1': 108, 'layer3.5.conv2': 191, 'layer3.3.conv2': 185, 'layer2.2.conv1': 106, 'layer1.0.conv2': 51, 'layer4.2.conv2': 340, 'layer1.1.conv2': 60, 'layer3.1.conv1': 199, 'layer3.5.conv1': 196, 'layer1.2.conv1': 56, 'layer1.1.conv1': 53, 'layer1.0.conv1': 50, 'layer2.3.conv2': 115}
Pruning filters.. 
Filters pruned 4.992055084745763%
Test:
	post prune Score: 10.0
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 97.88 - Val acc: 95.86 - Train loss: 0.0691 - Val loss: 0.1261 - Training time: 184.12s
Test pruning iteration :4
	Score: 93.06
Perform pruning iteration: 5
Layers that will be pruned {'layer4.0.conv1': 28, 'layer4.2.conv2': 31, 'layer3.4.conv1': 16, 'layer3.3.conv2': 13, 'layer3.3.conv1': 16, 'layer3.4.conv2': 15, 'layer4.1.conv1': 36, 'layer2.0.conv2': 8, 'layer3.2.conv1': 12, 'layer4.0.conv2': 27, 'layer3.5.conv1': 9, 'layer4.2.conv1': 28, 'layer3.2.conv2': 8, 'layer2.1.conv1': 4, 'layer1.2.conv1': 4, 'layer4.1.conv2': 24, 'layer2.2.conv2': 6, 'layer2.1.conv2': 6, 'layer3.1.conv2': 13, 'layer3.1.conv1': 12, 'layer2.0.conv1': 6, 'layer2.3.conv1': 2, 'layer3.0.conv1': 13, 'layer3.0.conv2': 12, 'layer3.5.conv2': 10, 'layer1.0.conv1': 3, 'layer2.3.conv2': 2, 'layer1.0.conv2': 5, 'layer1.1.conv2': 4, 'layer1.2.conv2': 1, 'layer2.2.conv1': 2, 'layer1.1.conv1': 1}
convolution remaining after pruning {'layer4.0.conv1': 321, 'layer4.2.conv2': 309, 'layer3.4.conv1': 175, 'layer3.3.conv2': 172, 'layer3.3.conv1': 188, 'layer3.4.conv2': 179, 'layer4.1.conv1': 309, 'layer2.0.conv2': 101, 'layer3.2.conv1': 194, 'layer4.0.conv2': 333, 'layer3.5.conv1': 187, 'layer4.2.conv1': 332, 'layer3.2.conv2': 192, 'layer2.1.conv1': 95, 'layer1.2.conv1': 52, 'layer4.1.conv2': 308, 'layer2.2.conv2': 103, 'layer2.1.conv2': 101, 'layer3.1.conv2': 197, 'layer3.1.conv1': 187, 'layer2.0.conv1': 101, 'layer2.3.conv1': 106, 'layer3.0.conv1': 196, 'layer3.0.conv2': 196, 'layer3.5.conv2': 181, 'layer1.0.conv1': 47, 'layer2.3.conv2': 113, 'layer1.0.conv2': 46, 'layer1.1.conv2': 56, 'layer1.2.conv2': 57, 'layer2.2.conv1': 104, 'layer1.1.conv1': 52}
Pruning filters.. 
Filters pruned 4.992055084745763%
Test:
	post prune Score: 10.0
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 98.10 - Val acc: 95.19 - Train loss: 0.0648 - Val loss: 0.1362 - Training time: 184.36s
Test pruning iteration :5
	Score: 93.17999999999999
Epoch 0 - Train acc: 99.01 - Val acc: 97.30 - Train loss: 0.0363 - Val loss: 0.0789 - Training time: 179.96s
Epoch 1 - Train acc: 99.50 - Val acc: 97.45 - Train loss: 0.0181 - Val loss: 0.0733 - Training time: 183.89s
Epoch 2 - Train acc: 99.67 - Val acc: 97.38 - Train loss: 0.0128 - Val loss: 0.0805 - Training time: 183.94s
Epoch 3 - Train acc: 99.93 - Val acc: 97.95 - Train loss: 0.0045 - Val loss: 0.0646 - Training time: 179.15s
end number of flops: 2863310592.0 	number of params: 14185026.0
Final Test:
	Score: 94.76
***  Resnet 18-0
number of flops: 1825668096.0 	number of params: 11181642.0
Test:
	Score: 9.29
Epoch 0 - Train acc: 94.38 - Val acc: 92.38 - Train loss: 0.1834 - Val loss: 0.2338 - Training time: 100.97s
Epoch 1 - Train acc: 96.83 - Val acc: 93.39 - Train loss: 0.1069 - Val loss: 0.1930 - Training time: 101.36s
Epoch 2 - Train acc: 98.85 - Val acc: 94.12 - Train loss: 0.0536 - Val loss: 0.1699 - Training time: 101.51s
Epoch 3 - Train acc: 99.56 - Val acc: 94.48 - Train loss: 0.0300 - Val loss: 0.1644 - Training time: 100.59s
Epoch 4 - Train acc: 99.89 - Val acc: 94.58 - Train loss: 0.0141 - Val loss: 0.1641 - Training time: 100.70s
Epoch 5 - Train acc: 99.97 - Val acc: 94.84 - Train loss: 0.0086 - Val loss: 0.1650 - Training time: 100.84s
Epoch 6 - Train acc: 100.00 - Val acc: 94.83 - Train loss: 0.0051 - Val loss: 0.1694 - Training time: 101.31s
Epoch 7 - Train acc: 100.00 - Val acc: 94.73 - Train loss: 0.0035 - Val loss: 0.1744 - Training time: 100.64s
Epoch 8 - Train acc: 100.00 - Val acc: 94.63 - Train loss: 0.0024 - Val loss: 0.1758 - Training time: 99.69s
Epoch 9 - Train acc: 100.00 - Val acc: 94.87 - Train loss: 0.0019 - Val loss: 0.1775 - Training time: 99.82s
Epoch 10 - Train acc: 100.00 - Val acc: 94.93 - Train loss: 0.0016 - Val loss: 0.1794 - Training time: 99.63s
Epoch 11 - Train acc: 100.00 - Val acc: 94.99 - Train loss: 0.0012 - Val loss: 0.1807 - Training time: 99.68s
Epoch 12 - Train acc: 100.00 - Val acc: 94.94 - Train loss: 0.0011 - Val loss: 0.1859 - Training time: 99.53s
Epoch 13 - Train acc: 100.00 - Val acc: 94.95 - Train loss: 0.0009 - Val loss: 0.1870 - Training time: 99.59s
Epoch 14 - Train acc: 100.00 - Val acc: 94.92 - Train loss: 0.0007 - Val loss: 0.1900 - Training time: 99.44s
end number of flops: 1825668096.0 	number of params: 11181642.0
Final Test:
	Score: 94.8
***  Resnet 18-30
number of flops: 1825668096.0 	number of params: 11181642.0
Epoch 0 - Train acc: 94.58 - Val acc: 91.82 - Train loss: 0.1778 - Val loss: 0.2479 - Training time: 99.52s
Epoch 1 - Train acc: 97.30 - Val acc: 93.51 - Train loss: 0.0964 - Val loss: 0.1946 - Training time: 99.61s
Epoch 2 - Train acc: 98.90 - Val acc: 93.90 - Train loss: 0.0514 - Val loss: 0.1794 - Training time: 99.50s
Epoch 3 - Train acc: 99.69 - Val acc: 94.17 - Train loss: 0.0263 - Val loss: 0.1720 - Training time: 99.64s
Epoch 4 - Train acc: 99.90 - Val acc: 94.35 - Train loss: 0.0141 - Val loss: 0.1788 - Training time: 99.49s
Test:
	Score: 94.19999999999999
6 iterations to reduce 30.00% filters
Perform pruning iteration: 0
Layers that will be pruned {'layer3.0.conv1': 8, 'layer4.1.conv1': 38, 'layer3.1.conv1': 18, 'layer4.0.conv1': 26, 'layer1.0.conv1': 1, 'layer2.0.conv1': 2, 'layer1.1.conv1': 2, 'layer2.1.conv1': 1}
convolution remaining after pruning {'layer3.0.conv1': 248, 'layer4.1.conv1': 474, 'layer3.1.conv1': 238, 'layer4.0.conv1': 486, 'layer1.0.conv1': 63, 'layer2.0.conv1': 126, 'layer1.1.conv1': 62, 'layer2.1.conv1': 127}
Pruning filters.. 
Filters pruned 5.0%
Test:
	post prune Score: 10.0
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 95.82 - Val acc: 92.43 - Train loss: 0.1380 - Val loss: 0.2236 - Training time: 99.48s
Test pruning iteration :0
	Score: 91.4
Perform pruning iteration: 1
Layers that will be pruned {'layer2.1.conv1': 6, 'layer3.0.conv1': 10, 'layer4.0.conv1': 38, 'layer3.1.conv1': 5, 'layer4.1.conv1': 30, 'layer1.0.conv1': 1, 'layer2.0.conv1': 5, 'layer1.1.conv1': 1}
convolution remaining after pruning {'layer2.1.conv1': 121, 'layer3.0.conv1': 238, 'layer4.0.conv1': 448, 'layer3.1.conv1': 233, 'layer4.1.conv1': 444, 'layer1.0.conv1': 62, 'layer2.0.conv1': 121, 'layer1.1.conv1': 61}
Pruning filters.. 
Filters pruned 5.0%
Test:
	post prune Score: 10.0
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 95.92 - Val acc: 92.92 - Train loss: 0.1334 - Val loss: 0.2032 - Training time: 99.48s
Test pruning iteration :1
	Score: 90.9
Perform pruning iteration: 2
Layers that will be pruned {'layer4.0.conv1': 33, 'layer4.1.conv1': 31, 'layer3.1.conv1': 13, 'layer2.0.conv1': 2, 'layer3.0.conv1': 10, 'layer2.1.conv1': 3, 'layer1.0.conv1': 1, 'layer1.1.conv1': 3}
convolution remaining after pruning {'layer4.0.conv1': 415, 'layer4.1.conv1': 413, 'layer3.1.conv1': 220, 'layer2.0.conv1': 119, 'layer3.0.conv1': 228, 'layer2.1.conv1': 118, 'layer1.0.conv1': 61, 'layer1.1.conv1': 58}
Pruning filters.. 
Filters pruned 5.0%
Test:
	post prune Score: 10.0
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 96.86 - Val acc: 93.84 - Train loss: 0.1054 - Val loss: 0.1782 - Training time: 99.66s
Test pruning iteration :2
	Score: 92.23
Perform pruning iteration: 3
Layers that will be pruned {'layer4.0.conv1': 24, 'layer4.1.conv1': 33, 'layer1.0.conv1': 5, 'layer3.0.conv1': 12, 'layer3.1.conv1': 14, 'layer2.0.conv1': 4, 'layer2.1.conv1': 4}
convolution remaining after pruning {'layer4.0.conv1': 391, 'layer4.1.conv1': 380, 'layer1.0.conv1': 56, 'layer3.0.conv1': 216, 'layer3.1.conv1': 206, 'layer2.0.conv1': 115, 'layer2.1.conv1': 114}
Pruning filters.. 
Filters pruned 5.0%
Test:
	post prune Score: 9.99
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 96.67 - Val acc: 94.02 - Train loss: 0.1102 - Val loss: 0.1766 - Training time: 99.47s
Test pruning iteration :3
	Score: 91.62
Perform pruning iteration: 4
Layers that will be pruned {'layer2.0.conv1': 5, 'layer4.0.conv1': 24, 'layer4.1.conv1': 32, 'layer3.1.conv1': 11, 'layer2.1.conv1': 7, 'layer3.0.conv1': 11, 'layer1.1.conv1': 3, 'layer1.0.conv1': 3}
convolution remaining after pruning {'layer2.0.conv1': 110, 'layer4.0.conv1': 367, 'layer4.1.conv1': 348, 'layer3.1.conv1': 195, 'layer2.1.conv1': 107, 'layer3.0.conv1': 205, 'layer1.1.conv1': 55, 'layer1.0.conv1': 53}
Pruning filters.. 
Filters pruned 5.0%
Test:
	post prune Score: 10.0
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 95.57 - Val acc: 93.08 - Train loss: 0.1291 - Val loss: 0.2048 - Training time: 99.09s
Test pruning iteration :4
	Score: 91.05
Perform pruning iteration: 5
Layers that will be pruned {'layer3.0.conv1': 10, 'layer3.1.conv1': 13, 'layer4.0.conv1': 24, 'layer2.1.conv1': 4, 'layer4.1.conv1': 31, 'layer2.0.conv1': 9, 'layer1.1.conv1': 4, 'layer1.0.conv1': 1}
convolution remaining after pruning {'layer3.0.conv1': 195, 'layer3.1.conv1': 182, 'layer4.0.conv1': 343, 'layer2.1.conv1': 103, 'layer4.1.conv1': 317, 'layer2.0.conv1': 101, 'layer1.1.conv1': 51, 'layer1.0.conv1': 52}
Pruning filters.. 
Filters pruned 5.0%
Test:
	post prune Score: 10.0
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 95.86 - Val acc: 93.21 - Train loss: 0.1286 - Val loss: 0.1981 - Training time: 99.50s
Test pruning iteration :5
	Score: 89.96
Epoch 0 - Train acc: 98.29 - Val acc: 95.90 - Train loss: 0.0646 - Val loss: 0.1219 - Training time: 99.17s
Epoch 1 - Train acc: 99.59 - Val acc: 96.94 - Train loss: 0.0264 - Val loss: 0.0916 - Training time: 99.04s
Epoch 2 - Train acc: 99.89 - Val acc: 96.97 - Train loss: 0.0143 - Val loss: 0.0923 - Training time: 99.21s
Epoch 3 - Train acc: 99.96 - Val acc: 96.81 - Train loss: 0.0087 - Val loss: 0.0930 - Training time: 98.94s
end number of flops: 1399509120.0 	number of params: 7530378.0
Final Test:
	Score: 93.11
***  Alexnet 0
number of flops: 823247104.0 	number of params: 61100840.0
Test:
	Score: 0.01
Epoch 0 - Train acc: 84.03 - Val acc: 81.48 - Train loss: 0.4695 - Val loss: 0.5336 - Training time: 87.80s
Epoch 1 - Train acc: 86.42 - Val acc: 83.39 - Train loss: 0.3873 - Val loss: 0.4751 - Training time: 88.13s
Epoch 2 - Train acc: 91.07 - Val acc: 86.84 - Train loss: 0.2631 - Val loss: 0.3810 - Training time: 88.05s
Epoch 3 - Train acc: 91.50 - Val acc: 86.74 - Train loss: 0.2460 - Val loss: 0.3873 - Training time: 87.52s
Epoch 4 - Train acc: 93.69 - Val acc: 87.82 - Train loss: 0.1841 - Val loss: 0.3594 - Training time: 88.24s
Epoch 5 - Train acc: 95.38 - Val acc: 88.89 - Train loss: 0.1418 - Val loss: 0.3390 - Training time: 87.33s
Epoch 6 - Train acc: 96.40 - Val acc: 89.26 - Train loss: 0.1119 - Val loss: 0.3185 - Training time: 87.63s
Epoch 7 - Train acc: 97.51 - Val acc: 89.94 - Train loss: 0.0838 - Val loss: 0.3073 - Training time: 88.49s
Epoch 8 - Train acc: 96.75 - Val acc: 89.00 - Train loss: 0.0943 - Val loss: 0.3505 - Training time: 87.86s
Epoch 9 - Train acc: 98.67 - Val acc: 90.50 - Train loss: 0.0493 - Val loss: 0.3054 - Training time: 88.20s
Epoch 10 - Train acc: 97.05 - Val acc: 88.79 - Train loss: 0.0830 - Val loss: 0.3711 - Training time: 87.31s
Epoch 11 - Train acc: 98.74 - Val acc: 89.82 - Train loss: 0.0470 - Val loss: 0.3368 - Training time: 87.52s
Epoch 12 - Train acc: 99.28 - Val acc: 90.56 - Train loss: 0.0286 - Val loss: 0.3345 - Training time: 88.16s
Epoch 13 - Train acc: 99.23 - Val acc: 90.28 - Train loss: 0.0308 - Val loss: 0.3237 - Training time: 87.25s
Epoch 14 - Train acc: 99.78 - Val acc: 90.51 - Train loss: 0.0147 - Val loss: 0.3346 - Training time: 88.30s
end number of flops: 823247104.0 	number of params: 61100840.0
Final Test:
	Score: 90.98
***  Alexnet 30
number of flops: 823247104.0 	number of params: 61100840.0
Epoch 0 - Train acc: 82.39 - Val acc: 81.51 - Train loss: 0.5125 - Val loss: 0.5458 - Training time: 88.29s
Epoch 1 - Train acc: 86.75 - Val acc: 84.40 - Train loss: 0.3848 - Val loss: 0.4503 - Training time: 87.23s
Epoch 2 - Train acc: 91.42 - Val acc: 87.89 - Train loss: 0.2541 - Val loss: 0.3507 - Training time: 88.06s
Epoch 3 - Train acc: 92.47 - Val acc: 88.22 - Train loss: 0.2234 - Val loss: 0.3438 - Training time: 88.31s
Epoch 4 - Train acc: 94.08 - Val acc: 88.97 - Train loss: 0.1789 - Val loss: 0.3175 - Training time: 87.46s
Test:
	Score: 88.92999999999999
6 iterations to reduce 30.00% filters
Perform pruning iteration: 0
Layers that will be pruned {'features.6': 27, 'features.10': 13, 'features.3': 5, 'features.0': 1, 'features.8': 11}
convolution remaining after pruning {'features.6': 357, 'features.10': 243, 'features.3': 187, 'features.0': 63, 'features.8': 245}
Pruning filters.. 
Filters pruned 4.947916666666667%
Test:
	post prune Score: 10.33
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 92.47 - Val acc: 90.70 - Train loss: 0.2128 - Val loss: 0.2607 - Training time: 87.99s
Test pruning iteration :0
	Score: 88.22
Perform pruning iteration: 1
Layers that will be pruned {'features.10': 13, 'features.6': 24, 'features.3': 6, 'features.8': 13, 'features.0': 1}
convolution remaining after pruning {'features.10': 230, 'features.6': 333, 'features.3': 181, 'features.8': 232, 'features.0': 62}
Pruning filters.. 
Filters pruned 4.947916666666667%
Test:
	post prune Score: 27.76
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 93.74 - Val acc: 91.86 - Train loss: 0.1858 - Val loss: 0.2342 - Training time: 87.00s
Test pruning iteration :1
	Score: 88.82
Perform pruning iteration: 2
Layers that will be pruned {'features.6': 19, 'features.8': 17, 'features.10': 13, 'features.3': 5, 'features.0': 2}
convolution remaining after pruning {'features.6': 314, 'features.8': 215, 'features.10': 217, 'features.3': 176, 'features.0': 60}
Pruning filters.. 
Filters pruned 4.947916666666667%
Test:
	post prune Score: 56.2
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 94.34 - Val acc: 92.88 - Train loss: 0.1686 - Val loss: 0.2107 - Training time: 87.47s
Test pruning iteration :2
	Score: 89.41
Perform pruning iteration: 3
Layers that will be pruned {'features.10': 11, 'features.8': 15, 'features.6': 22, 'features.3': 7, 'features.0': 2}
convolution remaining after pruning {'features.10': 206, 'features.8': 200, 'features.6': 292, 'features.3': 169, 'features.0': 58}
Pruning filters.. 
Filters pruned 4.947916666666667%
Test:
	post prune Score: 52.14
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 95.00 - Val acc: 93.12 - Train loss: 0.1562 - Val loss: 0.1962 - Training time: 87.41s
Test pruning iteration :3
	Score: 89.3
Perform pruning iteration: 4
Layers that will be pruned {'features.0': 5, 'features.10': 20, 'features.8': 13, 'features.3': 5, 'features.6': 14}
convolution remaining after pruning {'features.0': 53, 'features.10': 186, 'features.8': 187, 'features.3': 164, 'features.6': 278}
Pruning filters.. 
Filters pruned 4.947916666666667%
Test:
	post prune Score: 74.76
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 93.16 - Val acc: 91.64 - Train loss: 0.1976 - Val loss: 0.2385 - Training time: 87.22s
Test pruning iteration :4
	Score: 87.68
Perform pruning iteration: 5
Layers that will be pruned {'features.3': 10, 'features.8': 11, 'features.10': 11, 'features.6': 25}
convolution remaining after pruning {'features.3': 154, 'features.8': 176, 'features.10': 175, 'features.6': 253}
Pruning filters.. 
Filters pruned 4.947916666666667%
Test:
	post prune Score: 71.19
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 94.14 - Val acc: 92.76 - Train loss: 0.1748 - Val loss: 0.2112 - Training time: 88.28s
Test pruning iteration :5
	Score: 88.57000000000001
end number of flops: 500474208.0 	number of params: 47939920.0
Final Test:
	Score: 88.57000000000001
***  vgg16 0
number of flops: 17315688448.0 	number of params: 138357536.0
Traceback (most recent call last):
  File "C:/dev/cnnpruner/POC.py", line 540, in <module>
    run_compare_model_and_prune_alexnet()
  File "C:/dev/cnnpruner/POC.py", line 497, in run_compare_model_and_prune_alexnet
    run_strategy_prune_compare(dataset_params)
  File "C:/dev/cnnpruner/POC.py", line 399, in run_strategy_prune_compare
    dataset_params=dataset_params)
  File "C:/dev/cnnpruner/POC.py", line 265, in exec_vgg16
    dataset_params=dataset_params)
  File "C:/dev/cnnpruner/POC.py", line 104, in common_training_code
    test_score = test(model, dataset_params.test_dataset, exec_params.batch_size, use_gpu=use_gpu)
  File "C:\dev\cnnpruner\deeplib_ext\CustomDeepLib.py", line 109, in test
    score, loss = validate(model, test_loader, use_gpu=use_gpu)
  File "C:\dev\cnnpruner\deeplib_ext\CustomDeepLib.py", line 134, in validate
    output = model(inputs)
  File "C:\Users\naked\AppData\Local\conda\conda\envs\7030\lib\site-packages\torch\nn\modules\module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "C:\Users\naked\AppData\Local\conda\conda\envs\7030\lib\site-packages\torchvision\models\vgg.py", line 42, in forward
    x = self.features(x)
  File "C:\Users\naked\AppData\Local\conda\conda\envs\7030\lib\site-packages\torch\nn\modules\module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "C:\Users\naked\AppData\Local\conda\conda\envs\7030\lib\site-packages\torch\nn\modules\container.py", line 92, in forward
    input = module(input)
  File "C:\Users\naked\AppData\Local\conda\conda\envs\7030\lib\site-packages\torch\nn\modules\module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "C:\Users\naked\AppData\Local\conda\conda\envs\7030\lib\site-packages\torch\nn\modules\conv.py", line 320, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 11.00 GiB total capacity; 7.41 GiB already allocated; 153.74 MiB free; 903.64 MiB cached)

Process finished with exit code 1
