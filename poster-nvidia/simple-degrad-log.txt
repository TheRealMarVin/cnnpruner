C:\Users\naked\AppData\Local\conda\conda\envs\7030\python.exe "C:\Program Files\JetBrains\PyCharm 2018.3.3\helpers\pydev\pydevconsole.py" --mode=client --port=58367
import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['C:\\dev\\cnnpruner', 'C:/dev/cnnpruner'])
Python 3.7.1 | packaged by conda-forge | (default, Nov 13 2018, 19:01:41) [MSC v.1900 64 bit (AMD64)]
Type 'copyright', 'credits' or 'license' for more information
IPython 7.2.0 -- An enhanced Interactive Python. Type '?' for help.
PyDev console: using IPython 7.2.0
Python 3.7.1 | packaged by conda-forge | (default, Nov 13 2018, 19:01:41) [MSC v.1900 64 bit (AMD64)] on win32
runfile('C:/dev/cnnpruner/benchmark/degradation_test.py', wdir='C:/dev/cnnpruner/benchmark')
Files already downloaded and verified
Files already downloaded and verified
***  AlexNet-degrad
number of flops: 823247104.0 	number of params: 61100840.0
C:\dev\cnnpruner\deeplib_ext\CustomDeepLib.py:129: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs = Variable(inputs, volatile=True)
C:\dev\cnnpruner\deeplib_ext\CustomDeepLib.py:130: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  targets = Variable(targets, volatile=True)
Epoch 0 - Train acc: 98.48 - Val acc: 97.40 - Train loss: 0.0520 - Val loss: 0.0919 - Training time: 98.08s
end number of flops: 823247104.0 	number of params: 61100840.0
diff number of flops: 0.0 	diff number of params: 0.0
Final Test:
	Score: 90.61
***  AlexNet-degrad
number of flops: 823247104.0 	number of params: 61100840.0
1 iterations to reduce 5.00% filters
Perform pruning iteration: 0
junction pruning size: {}
Layers that will be pruned {'features.3': 14, 'features.6': 26, 'features.10': 7, 'features.8': 8, 'features.0': 2}
convolution remaining after pruning {'features.3': 178, 'features.6': 358, 'features.10': 249, 'features.8': 248, 'features.0': 62}
Pruning filters.. 
Filters pruned 4.947916666666667%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 98.19 - Val acc: 97.28 - Train loss: 0.0676 - Val loss: 0.0956 - Training time: 96.50s
end number of flops: 755812096.0 	number of params: 59826792.0
diff number of flops: 0.08191344697399627 	diff number of params: 0.020851562760839295
Final Test:
	Score: 90.61
***  AlexNet-degrad
number of flops: 823247104.0 	number of params: 61100840.0
1 iterations to reduce 10.00% filters
Perform pruning iteration: 0
junction pruning size: {}
Layers that will be pruned {'features.6': 56, 'features.8': 23, 'features.10': 14, 'features.0': 5, 'features.3': 17}
convolution remaining after pruning {'features.6': 328, 'features.8': 233, 'features.10': 242, 'features.0': 59, 'features.3': 175}
Pruning filters.. 
Filters pruned 9.98263888888889%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 98.05 - Val acc: 96.28 - Train loss: 0.0696 - Val loss: 0.1203 - Training time: 96.39s
end number of flops: 696276480.0 	number of params: 58559228.0
diff number of flops: 0.15423148576299153 	diff number of params: 0.0415970058676771
Final Test:
	Score: 90.22
***  AlexNet-degrad
number of flops: 823247104.0 	number of params: 61100840.0
1 iterations to reduce 15.00% filters
Perform pruning iteration: 0
junction pruning size: {}
Layers that will be pruned {'features.6': 62, 'features.8': 36, 'features.3': 32, 'features.10': 32, 'features.0': 10}
convolution remaining after pruning {'features.6': 322, 'features.8': 220, 'features.3': 160, 'features.10': 224, 'features.0': 54}
Pruning filters.. 
Filters pruned 14.930555555555555%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 97.37 - Val acc: 95.74 - Train loss: 0.0862 - Val loss: 0.1345 - Training time: 96.06s
end number of flops: 622058880.0 	number of params: 55693896.0
diff number of flops: 0.24438376159778147 	diff number of params: 0.08849213856961705
Final Test:
	Score: 89.47
***  AlexNet-degrad
number of flops: 823247104.0 	number of params: 61100840.0
1 iterations to reduce 20.00% filters
Perform pruning iteration: 0
junction pruning size: {}
Layers that will be pruned {'features.3': 40, 'features.8': 46, 'features.6': 107, 'features.10': 25, 'features.0': 12}
convolution remaining after pruning {'features.3': 152, 'features.8': 210, 'features.6': 277, 'features.10': 231, 'features.0': 52}
Pruning filters.. 
Filters pruned 19.96527777777778%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 96.64 - Val acc: 94.76 - Train loss: 0.1106 - Val loss: 0.1572 - Training time: 95.99s
end number of flops: 569304448.0 	number of params: 56501200.0
diff number of flops: 0.3084646818265637 	diff number of params: 0.07527948879262544
Final Test:
	Score: 89.34
***  AlexNet-degrad
number of flops: 823247104.0 	number of params: 61100840.0
1 iterations to reduce 25.00% filters
Perform pruning iteration: 0
junction pruning size: {}
Layers that will be pruned {'features.8': 63, 'features.3': 44, 'features.10': 55, 'features.6': 112, 'features.0': 14}
convolution remaining after pruning {'features.8': 193, 'features.3': 148, 'features.10': 201, 'features.6': 272, 'features.0': 50}
Pruning filters.. 
Filters pruned 25.0%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 96.10 - Val acc: 94.04 - Train loss: 0.1312 - Val loss: 0.1803 - Training time: 96.37s
end number of flops: 519528672.0 	number of params: 51908984.0
diff number of flops: 0.3689274223064865 	diff number of params: 0.15043747352736886
Final Test:
	Score: 88.44
***  AlexNet-degrad
number of flops: 823247104.0 	number of params: 61100840.0
1 iterations to reduce 30.00% filters
Perform pruning iteration: 0
junction pruning size: {}
Layers that will be pruned {'features.0': 18, 'features.6': 139, 'features.3': 48, 'features.8': 72, 'features.10': 68}
convolution remaining after pruning {'features.0': 46, 'features.6': 245, 'features.3': 144, 'features.8': 184, 'features.10': 188}
Pruning filters.. 
Filters pruned 29.947916666666668%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 95.25 - Val acc: 92.77 - Train loss: 0.1490 - Val loss: 0.2126 - Training time: 95.66s
end number of flops: 468480288.0 	number of params: 49821808.0
diff number of flops: 0.4309360024180541 	diff number of params: 0.18459700390371064
Final Test:
	Score: 88.2
***  AlexNet-degrad
number of flops: 823247104.0 	number of params: 61100840.0
1 iterations to reduce 35.00% filters
Perform pruning iteration: 0
junction pruning size: {}
Layers that will be pruned {'features.6': 166, 'features.10': 73, 'features.0': 23, 'features.8': 81, 'features.3': 60}
convolution remaining after pruning {'features.6': 218, 'features.10': 183, 'features.0': 41, 'features.8': 175, 'features.3': 132}
Pruning filters.. 
Filters pruned 34.982638888888886%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 92.81 - Val acc: 90.94 - Train loss: 0.2207 - Val loss: 0.2730 - Training time: 96.30s
end number of flops: 411407680.0 	number of params: 48908348.0
diff number of flops: 0.5002622201753898 	diff number of params: 0.19954704387042796
Final Test:
	Score: 86.29
***  AlexNet-degrad
number of flops: 823247104.0 	number of params: 61100840.0
1 iterations to reduce 40.00% filters
Perform pruning iteration: 0
junction pruning size: {}
Layers that will be pruned {'features.3': 66, 'features.10': 85, 'features.6': 176, 'features.8': 109, 'features.0': 24}
convolution remaining after pruning {'features.3': 126, 'features.10': 171, 'features.6': 208, 'features.8': 147, 'features.0': 40}
Pruning filters.. 
Filters pruned 39.93055555555556%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 90.84 - Val acc: 88.47 - Train loss: 0.2669 - Val loss: 0.3313 - Training time: 94.60s
end number of flops: 370907616.0 	number of params: 46975884.0
diff number of flops: 0.5494577336527138 	diff number of params: 0.23117449776467885
Final Test:
	Score: 84.91
***  AlexNet-degrad
number of flops: 823247104.0 	number of params: 61100840.0
1 iterations to reduce 45.00% filters
Perform pruning iteration: 0
junction pruning size: {}
Layers that will be pruned {'features.6': 191, 'features.0': 22, 'features.8': 120, 'features.3': 65, 'features.10': 119}
convolution remaining after pruning {'features.6': 193, 'features.0': 42, 'features.8': 136, 'features.3': 127, 'features.10': 137}
Pruning filters.. 
Filters pruned 44.87847222222222%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 90.66 - Val acc: 88.41 - Train loss: 0.2715 - Val loss: 0.3272 - Training time: 95.40s
end number of flops: 347489504.0 	number of params: 41857632.0
diff number of flops: 0.5779037638740361 	diff number of params: 0.3149417913076154
Final Test:
	Score: 85.41
***  AlexNet-degrad
number of flops: 823247104.0 	number of params: 61100840.0
1 iterations to reduce 50.00% filters
Perform pruning iteration: 0
junction pruning size: {}
Layers that will be pruned {'features.6': 209, 'features.3': 101, 'features.8': 119, 'features.0': 25, 'features.10': 122}
convolution remaining after pruning {'features.6': 175, 'features.3': 91, 'features.8': 137, 'features.0': 39, 'features.10': 134}
Pruning filters.. 
Filters pruned 50.0%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 86.34 - Val acc: 84.84 - Train loss: 0.3928 - Val loss: 0.4425 - Training time: 95.04s
end number of flops: 290571840.0 	number of params: 41269292.0
diff number of flops: 0.6470417708265634 	diff number of params: 0.3245707914981202
Final Test:
	Score: 82.1
***  ResNet18-degrad
number of flops: 1826680832.0 	number of params: 11689512.0
Epoch 0 - Train acc: 99.54 - Val acc: 98.40 - Train loss: 0.0165 - Val loss: 0.0599 - Training time: 118.18s
end number of flops: 1826680832.0 	number of params: 11689512.0
diff number of flops: 0.0 	diff number of params: 0.0
Final Test:
	Score: 93.89
***  ResNet18-degrad
number of flops: 1826680832.0 	number of params: 11689512.0
1 iterations to reduce 5.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 0}
Layers that will be pruned {'layer4.1.conv1': 61, 'layer2.0.conv1': 9, 'layer3.1.conv1': 8, 'layer3.0.conv1': 8, 'layer4.0.conv1': 21, 'layer2.1.conv1': 2}
convolution remaining after pruning {'layer4.1.conv1': 451, 'layer2.0.conv1': 119, 'layer3.1.conv1': 248, 'layer3.0.conv1': 248, 'layer4.0.conv1': 491, 'layer2.1.conv1': 126}
Pruning filters.. 
Filters pruned 3.3394607843137254%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 96.99 - Val acc: 93.99 - Train loss: 0.0895 - Val loss: 0.1683 - Training time: 117.12s
end number of flops: 1763508992.0 	number of params: 10897294.0
diff number of flops: 0.034582855906378725 	diff number of params: 0.06777169141021455
Final Test:
	Score: 91.36999999999999
***  ResNet18-degrad
number of flops: 1826680832.0 	number of params: 11689512.0
1 iterations to reduce 10.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 0}
Layers that will be pruned {'layer4.1.conv1': 99, 'layer4.0.conv1': 41, 'layer3.1.conv1': 32, 'layer2.1.conv1': 12, 'layer3.0.conv1': 24, 'layer2.0.conv1': 6, 'layer1.1.conv1': 2, 'layer1.0.conv1': 1}
convolution remaining after pruning {'layer4.1.conv1': 413, 'layer4.0.conv1': 471, 'layer3.1.conv1': 224, 'layer2.1.conv1': 116, 'layer3.0.conv1': 232, 'layer2.0.conv1': 122, 'layer1.1.conv1': 62, 'layer1.0.conv1': 63}
Pruning filters.. 
Filters pruned 6.64828431372549%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 96.33 - Val acc: 92.76 - Train loss: 0.1136 - Val loss: 0.2052 - Training time: 118.75s
end number of flops: 1682121344.0 	number of params: 10221430.0
diff number of flops: 0.07913779214605565 	diff number of params: 0.12558967388886721
Final Test:
	Score: 90.60000000000001
***  ResNet18-degrad
number of flops: 1826680832.0 	number of params: 11689512.0
1 iterations to reduce 15.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 1}
Layers that will be pruned {'layer4.0.conv1': 91, 'layer3.0.conv2': 1, 'layer4.1.conv1': 158, 'layer3.0.downsample.0': 1, 'layer3.0.conv1': 21, 'layer3.1.conv2': 1, 'layer3.1.conv1': 33, 'layer2.0.conv1': 12, 'layer1.0.conv1': 5, 'layer1.1.conv1': 4, 'layer2.1.conv1': 7}
convolution remaining after pruning {'layer4.0.conv1': 421, 'layer3.0.conv2': 255, 'layer4.1.conv1': 354, 'layer3.0.downsample.0': 255, 'layer3.0.conv1': 235, 'layer3.1.conv2': 255, 'layer3.1.conv1': 223, 'layer2.0.conv1': 116, 'layer1.0.conv1': 59, 'layer1.1.conv1': 60, 'layer2.1.conv1': 121}
Pruning filters.. 
Filters pruned 10.232843137254902%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 92.91 - Val acc: 88.92 - Train loss: 0.2067 - Val loss: 0.3268 - Training time: 119.58s
end number of flops: 1617361792.0 	number of params: 9321294.0
diff number of flops: 0.11458982671363577 	diff number of params: 0.20259340167493733
Final Test:
	Score: 87.71
***  ResNet18-degrad
number of flops: 1826680832.0 	number of params: 11689512.0
1 iterations to reduce 20.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 4}
Layers that will be pruned {'layer3.0.conv2': 4, 'layer4.1.conv1': 212, 'layer3.1.conv2': 4, 'layer3.0.downsample.0': 4, 'layer4.0.conv1': 127, 'layer3.0.conv1': 35, 'layer3.1.conv1': 41, 'layer2.0.conv1': 11, 'layer2.1.conv1': 8, 'layer1.0.conv1': 7, 'layer1.1.conv1': 2}
convolution remaining after pruning {'layer3.0.conv2': 252, 'layer4.1.conv1': 300, 'layer3.1.conv2': 252, 'layer3.0.downsample.0': 252, 'layer4.0.conv1': 385, 'layer3.0.conv1': 221, 'layer3.1.conv1': 215, 'layer2.0.conv1': 117, 'layer2.1.conv1': 120, 'layer1.0.conv1': 57, 'layer1.1.conv1': 62}
Pruning filters.. 
Filters pruned 13.939950980392156%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 93.00 - Val acc: 89.46 - Train loss: 0.2000 - Val loss: 0.3059 - Training time: 118.83s
end number of flops: 1559543680.0 	number of params: 8459434.0
diff number of flops: 0.1462418323553088 	diff number of params: 0.2763227412744005
Final Test:
	Score: 87.8
***  ResNet18-degrad
number of flops: 1826680832.0 	number of params: 11689512.0
1 iterations to reduce 25.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 1, 2: 5}
Layers that will be pruned {'layer1.0.conv1': 9, 'layer2.0.conv1': 18, 'layer4.0.conv1': 155, 'layer3.0.downsample.0': 5, 'layer3.1.conv1': 80, 'layer3.0.conv1': 50, 'layer4.1.conv1': 226, 'layer3.0.conv2': 5, 'layer3.1.conv2': 5, 'layer2.0.conv2': 1, 'layer2.1.conv1': 16, 'layer2.1.conv2': 1, 'layer1.1.conv1': 5, 'layer2.0.downsample.0': 1}
convolution remaining after pruning {'layer1.0.conv1': 55, 'layer2.0.conv1': 110, 'layer4.0.conv1': 357, 'layer3.0.downsample.0': 251, 'layer3.1.conv1': 176, 'layer3.0.conv1': 206, 'layer4.1.conv1': 286, 'layer3.0.conv2': 251, 'layer3.1.conv2': 251, 'layer2.0.conv2': 127, 'layer2.1.conv1': 112, 'layer2.1.conv2': 127, 'layer1.1.conv1': 59, 'layer2.0.downsample.0': 127}
Pruning filters.. 
Filters pruned 17.67769607843137%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 91.38 - Val acc: 87.81 - Train loss: 0.2591 - Val loss: 0.3603 - Training time: 116.87s
end number of flops: 1452877056.0 	number of params: 7859096.0
diff number of flops: 0.20463551675348177 	diff number of params: 0.32767971836634413
Final Test:
	Score: 86.28
***  ResNet18-degrad
number of flops: 1826680832.0 	number of params: 11689512.0
1 iterations to reduce 30.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 1, 1: 1, 2: 9}
Layers that will be pruned {'conv1': 1, 'layer4.0.conv1': 193, 'layer2.1.conv1': 25, 'layer4.1.conv1': 271, 'layer3.0.conv1': 64, 'layer3.0.downsample.0': 9, 'layer2.1.conv2': 1, 'layer2.0.downsample.0': 1, 'layer3.0.conv2': 9, 'layer1.1.conv2': 1, 'layer2.0.conv1': 25, 'layer1.0.conv1': 14, 'layer2.0.conv2': 1, 'layer3.1.conv2': 9, 'layer3.1.conv1': 70, 'layer1.0.conv2': 1, 'layer1.1.conv1': 9}
convolution remaining after pruning {'conv1': 63, 'layer4.0.conv1': 319, 'layer2.1.conv1': 103, 'layer4.1.conv1': 241, 'layer3.0.conv1': 192, 'layer3.0.downsample.0': 247, 'layer2.1.conv2': 127, 'layer2.0.downsample.0': 127, 'layer3.0.conv2': 247, 'layer1.1.conv2': 63, 'layer2.0.conv1': 103, 'layer1.0.conv1': 50, 'layer2.0.conv2': 127, 'layer3.1.conv2': 247, 'layer3.1.conv1': 186, 'layer1.0.conv2': 63, 'layer1.1.conv1': 55}
Pruning filters.. 
Filters pruned 21.568627450980394%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 85.42 - Val acc: 81.91 - Train loss: 0.4279 - Val loss: 0.5268 - Training time: 117.34s
end number of flops: 1347574912.0 	number of params: 7100318.0
diff number of flops: 0.2622822288420444 	diff number of params: 0.39259072577195697
Final Test:
	Score: 81.58
***  ResNet18-degrad
number of flops: 1826680832.0 	number of params: 11689512.0
1 iterations to reduce 35.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 1, 1: 2, 2: 13}
Layers that will be pruned {'conv1': 1, 'layer2.0.conv1': 25, 'layer3.0.downsample.0': 13, 'layer4.1.conv1': 292, 'layer2.1.conv1': 21, 'layer4.0.conv1': 247, 'layer3.1.conv2': 13, 'layer3.0.conv2': 13, 'layer2.1.conv2': 2, 'layer1.0.conv2': 1, 'layer2.0.conv2': 2, 'layer3.0.conv1': 76, 'layer3.1.conv1': 79, 'layer1.0.conv1': 7, 'layer1.1.conv2': 1, 'layer2.0.downsample.0': 2, 'layer1.1.conv1': 8}
convolution remaining after pruning {'conv1': 63, 'layer2.0.conv1': 103, 'layer3.0.downsample.0': 243, 'layer4.1.conv1': 220, 'layer2.1.conv1': 107, 'layer4.0.conv1': 265, 'layer3.1.conv2': 243, 'layer3.0.conv2': 243, 'layer2.1.conv2': 126, 'layer1.0.conv2': 63, 'layer2.0.conv2': 126, 'layer3.0.conv1': 180, 'layer3.1.conv1': 177, 'layer1.0.conv1': 57, 'layer1.1.conv2': 63, 'layer2.0.downsample.0': 126, 'layer1.1.conv1': 56}
Pruning filters.. 
Filters pruned 24.60171568627451%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 87.87 - Val acc: 83.91 - Train loss: 0.3524 - Val loss: 0.4598 - Training time: 117.32s
end number of flops: 1332849280.0 	number of params: 6439421.0
diff number of flops: 0.2703436437000944 	diff number of params: 0.4491283297369471
Final Test:
	Score: 83.71
***  ResNet18-degrad
number of flops: 1826680832.0 	number of params: 11689512.0
1 iterations to reduce 40.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 1, 1: 4, 2: 12}
Layers that will be pruned {'conv1': 1, 'layer3.1.conv1': 110, 'layer3.0.downsample.0': 12, 'layer4.1.conv1': 328, 'layer3.1.conv2': 12, 'layer3.0.conv1': 81, 'layer2.0.conv2': 4, 'layer3.0.conv2': 12, 'layer4.0.conv1': 270, 'layer2.1.conv2': 4, 'layer1.0.conv2': 1, 'layer2.0.conv1': 38, 'layer1.0.conv1': 14, 'layer1.1.conv1': 10, 'layer2.0.downsample.0': 4, 'layer2.1.conv1': 27, 'layer1.1.conv2': 1}
convolution remaining after pruning {'conv1': 63, 'layer3.1.conv1': 146, 'layer3.0.downsample.0': 244, 'layer4.1.conv1': 184, 'layer3.1.conv2': 244, 'layer3.0.conv1': 175, 'layer2.0.conv2': 124, 'layer3.0.conv2': 244, 'layer4.0.conv1': 242, 'layer2.1.conv2': 124, 'layer1.0.conv2': 63, 'layer2.0.conv1': 90, 'layer1.0.conv1': 50, 'layer1.1.conv1': 54, 'layer2.0.downsample.0': 124, 'layer2.1.conv1': 101, 'layer1.1.conv2': 63}
Pruning filters.. 
Filters pruned 28.462009803921568%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 84.90 - Val acc: 81.19 - Train loss: 0.4418 - Val loss: 0.5442 - Training time: 117.19s
end number of flops: 1214908416.0 	number of params: 5750981.0
diff number of flops: 0.3349093094332092 	diff number of params: 0.5080221484010624
Final Test:
	Score: 80.53
***  ResNet18-degrad
number of flops: 1826680832.0 	number of params: 11689512.0
1 iterations to reduce 45.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 2, 2: 37}
Layers that will be pruned {'layer4.1.conv1': 367, 'layer4.0.conv1': 301, 'layer3.0.downsample.0': 37, 'layer3.0.conv2': 37, 'layer2.0.downsample.0': 2, 'layer2.0.conv1': 41, 'layer1.0.conv1': 12, 'layer3.1.conv1': 105, 'layer1.1.conv1': 10, 'layer3.1.conv2': 37, 'layer3.0.conv1': 107, 'layer2.1.conv1': 42, 'layer2.1.conv2': 2, 'layer2.0.conv2': 2}
convolution remaining after pruning {'layer4.1.conv1': 145, 'layer4.0.conv1': 211, 'layer3.0.downsample.0': 219, 'layer3.0.conv2': 219, 'layer2.0.downsample.0': 126, 'layer2.0.conv1': 87, 'layer1.0.conv1': 52, 'layer3.1.conv1': 151, 'layer1.1.conv1': 54, 'layer3.1.conv2': 219, 'layer3.0.conv1': 149, 'layer2.1.conv1': 86, 'layer2.1.conv2': 126, 'layer2.0.conv2': 126}
Pruning filters.. 
Filters pruned 33.76225490196079%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 82.98 - Val acc: 80.17 - Train loss: 0.4821 - Val loss: 0.5774 - Training time: 117.48s
end number of flops: 1140743680.0 	number of params: 4925896.0
diff number of flops: 0.3755101274309534 	diff number of params: 0.578605505516398
Final Test:
	Score: 78.63
***  ResNet18-degrad
number of flops: 1826680832.0 	number of params: 11689512.0
1 iterations to reduce 50.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 2, 1: 8, 2: 39}
Layers that will be pruned {'conv1': 2, 'layer4.1.conv1': 389, 'layer1.0.conv1': 24, 'layer3.0.conv2': 39, 'layer2.0.conv1': 40, 'layer3.0.downsample.0': 39, 'layer3.0.conv1': 111, 'layer3.1.conv1': 120, 'layer4.0.conv1': 312, 'layer1.1.conv2': 2, 'layer2.0.downsample.0': 8, 'layer2.1.conv2': 8, 'layer2.1.conv1': 37, 'layer3.1.conv2': 39, 'layer2.0.conv2': 8, 'layer1.1.conv1': 20, 'layer1.0.conv2': 2}
convolution remaining after pruning {'conv1': 62, 'layer4.1.conv1': 123, 'layer1.0.conv1': 40, 'layer3.0.conv2': 217, 'layer2.0.conv1': 88, 'layer3.0.downsample.0': 217, 'layer3.0.conv1': 145, 'layer3.1.conv1': 136, 'layer4.0.conv1': 200, 'layer1.1.conv2': 62, 'layer2.0.downsample.0': 120, 'layer2.1.conv2': 120, 'layer2.1.conv1': 91, 'layer3.1.conv2': 217, 'layer2.0.conv2': 120, 'layer1.1.conv1': 44, 'layer1.0.conv2': 62}
Pruning filters.. 
Filters pruned 36.76470588235294%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 80.52 - Val acc: 77.22 - Train loss: 0.5629 - Val loss: 0.6658 - Training time: 116.76s
end number of flops: 1013564608.0 	number of params: 4525115.0
diff number of flops: 0.4451331670841116 	diff number of params: 0.6128910257331529
Final Test:
	Score: 77.47
***  ResNet34-degrad
number of flops: 3681000960.0 	number of params: 21289802.0
Epoch 0 - Train acc: 99.74 - Val acc: 99.00 - Train loss: 0.0090 - Val loss: 0.0373 - Training time: 140.22s
end number of flops: 3681000960.0 	number of params: 21289802.0
diff number of flops: 0.0 	diff number of params: 0.0
Final Test:
	Score: 95.69
***  ResNet34-degrad
number of flops: 3681000960.0 	number of params: 21289802.0
1 iterations to reduce 5.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 0}
Layers that will be pruned {'layer4.1.conv1': 43, 'layer4.0.conv1': 36, 'layer3.4.conv1': 17, 'layer3.5.conv1': 14, 'layer2.0.conv1': 3, 'layer4.2.conv1': 45, 'layer3.0.conv1': 4, 'layer3.1.conv1': 14, 'layer3.3.conv1': 20, 'layer2.2.conv1': 4, 'layer3.2.conv1': 14, 'layer1.0.conv1': 1, 'layer2.1.conv1': 2, 'layer2.3.conv1': 1}
convolution remaining after pruning {'layer4.1.conv1': 469, 'layer4.0.conv1': 476, 'layer3.4.conv1': 239, 'layer3.5.conv1': 242, 'layer2.0.conv1': 125, 'layer4.2.conv1': 467, 'layer3.0.conv1': 252, 'layer3.1.conv1': 242, 'layer3.3.conv1': 236, 'layer2.2.conv1': 124, 'layer3.2.conv1': 242, 'layer1.0.conv1': 63, 'layer2.1.conv1': 126, 'layer2.3.conv1': 127}
Pruning filters.. 
Filters pruned 3.3725247524752477%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 97.28 - Val acc: 94.79 - Train loss: 0.0849 - Val loss: 0.1544 - Training time: 141.06s
end number of flops: 3534554368.0 	number of params: 19829206.0
diff number of flops: 0.03978444819530827 	diff number of params: 0.06860542902183872
Final Test:
	Score: 92.56
***  ResNet34-degrad
number of flops: 3681000960.0 	number of params: 21289802.0
1 iterations to reduce 10.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 0}
Layers that will be pruned {'layer1.1.conv1': 4, 'layer2.1.conv1': 10, 'layer2.2.conv1': 7, 'layer4.1.conv1': 81, 'layer4.2.conv1': 94, 'layer3.0.conv1': 22, 'layer3.3.conv1': 24, 'layer3.4.conv1': 33, 'layer4.0.conv1': 61, 'layer3.5.conv1': 34, 'layer3.1.conv1': 21, 'layer2.3.conv1': 9, 'layer2.0.conv1': 7, 'layer3.2.conv1': 20, 'layer1.2.conv1': 3, 'layer1.0.conv1': 4}
convolution remaining after pruning {'layer1.1.conv1': 60, 'layer2.1.conv1': 118, 'layer2.2.conv1': 121, 'layer4.1.conv1': 431, 'layer4.2.conv1': 418, 'layer3.0.conv1': 234, 'layer3.3.conv1': 232, 'layer3.4.conv1': 223, 'layer4.0.conv1': 451, 'layer3.5.conv1': 222, 'layer3.1.conv1': 235, 'layer2.3.conv1': 119, 'layer2.0.conv1': 121, 'layer3.2.conv1': 236, 'layer1.2.conv1': 61, 'layer1.0.conv1': 60}
Pruning filters.. 
Filters pruned 6.714108910891089%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 97.52 - Val acc: 94.11 - Train loss: 0.0773 - Val loss: 0.1623 - Training time: 140.87s
end number of flops: 3350597376.0 	number of params: 18485542.0
diff number of flops: 0.08975916811496838 	diff number of params: 0.13171846314023963
Final Test:
	Score: 93.07
***  ResNet34-degrad
number of flops: 3681000960.0 	number of params: 21289802.0
1 iterations to reduce 15.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 0}
Layers that will be pruned {'layer4.2.conv1': 131, 'layer4.0.conv1': 109, 'layer4.1.conv1': 163, 'layer2.1.conv1': 11, 'layer3.0.conv1': 36, 'layer3.3.conv1': 24, 'layer3.4.conv1': 49, 'layer3.5.conv1': 34, 'layer1.1.conv1': 4, 'layer3.2.conv1': 34, 'layer2.0.conv1': 9, 'layer3.1.conv1': 29, 'layer1.0.conv1': 4, 'layer2.3.conv1': 7, 'layer2.2.conv1': 10, 'layer1.2.conv1': 3}
convolution remaining after pruning {'layer4.2.conv1': 381, 'layer4.0.conv1': 403, 'layer4.1.conv1': 349, 'layer2.1.conv1': 117, 'layer3.0.conv1': 220, 'layer3.3.conv1': 232, 'layer3.4.conv1': 207, 'layer3.5.conv1': 222, 'layer1.1.conv1': 60, 'layer3.2.conv1': 222, 'layer2.0.conv1': 119, 'layer3.1.conv1': 227, 'layer1.0.conv1': 60, 'layer2.3.conv1': 121, 'layer2.2.conv1': 118, 'layer1.2.conv1': 61}
Pruning filters.. 
Filters pruned 10.163985148514852%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 97.27 - Val acc: 94.17 - Train loss: 0.0864 - Val loss: 0.1703 - Training time: 141.12s
end number of flops: 3230390528.0 	number of params: 16825064.0
diff number of flops: 0.12241519002483499 	diff number of params: 0.20971251869791932
Final Test:
	Score: 92.67999999999999
***  ResNet34-degrad
number of flops: 3681000960.0 	number of params: 21289802.0
1 iterations to reduce 20.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 0}
Layers that will be pruned {'layer2.0.conv1': 14, 'layer3.1.conv1': 62, 'layer3.3.conv1': 50, 'layer3.5.conv1': 43, 'layer3.0.conv1': 47, 'layer4.0.conv1': 130, 'layer4.1.conv1': 152, 'layer4.2.conv1': 176, 'layer3.4.conv1': 63, 'layer2.2.conv1': 14, 'layer1.2.conv1': 3, 'layer2.3.conv1': 13, 'layer2.1.conv1': 17, 'layer3.2.conv1': 51, 'layer1.1.conv1': 6, 'layer1.0.conv1': 10}
convolution remaining after pruning {'layer2.0.conv1': 114, 'layer3.1.conv1': 194, 'layer3.3.conv1': 206, 'layer3.5.conv1': 213, 'layer3.0.conv1': 209, 'layer4.0.conv1': 382, 'layer4.1.conv1': 360, 'layer4.2.conv1': 336, 'layer3.4.conv1': 193, 'layer2.2.conv1': 114, 'layer1.2.conv1': 61, 'layer2.3.conv1': 115, 'layer2.1.conv1': 111, 'layer3.2.conv1': 205, 'layer1.1.conv1': 58, 'layer1.0.conv1': 54}
Pruning filters.. 
Filters pruned 13.165222772277227%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 96.58 - Val acc: 93.33 - Train loss: 0.1049 - Val loss: 0.1940 - Training time: 141.03s
end number of flops: 3046219008.0 	number of params: 15817252.0
diff number of flops: 0.17244818974456338 	diff number of params: 0.2570503004208306
Final Test:
	Score: 92.24
***  ResNet34-degrad
number of flops: 3681000960.0 	number of params: 21289802.0
1 iterations to reduce 25.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 1}
Layers that will be pruned {'layer3.0.conv1': 40, 'layer3.5.conv2': 1, 'layer3.4.conv2': 1, 'layer3.2.conv2': 1, 'layer4.0.conv1': 151, 'layer3.3.conv1': 56, 'layer2.3.conv1': 18, 'layer3.1.conv1': 70, 'layer4.2.conv1': 211, 'layer3.1.conv2': 1, 'layer3.5.conv1': 64, 'layer4.1.conv1': 213, 'layer3.2.conv1': 61, 'layer3.4.conv1': 75, 'layer3.0.downsample.0': 1, 'layer3.0.conv2': 1, 'layer3.3.conv2': 1, 'layer2.2.conv1': 22, 'layer1.2.conv1': 4, 'layer2.1.conv1': 26, 'layer1.0.conv1': 10, 'layer2.0.conv1': 17, 'layer1.1.conv1': 9}
convolution remaining after pruning {'layer3.0.conv1': 216, 'layer3.5.conv2': 255, 'layer3.4.conv2': 255, 'layer3.2.conv2': 255, 'layer4.0.conv1': 361, 'layer3.3.conv1': 200, 'layer2.3.conv1': 110, 'layer3.1.conv1': 186, 'layer4.2.conv1': 301, 'layer3.1.conv2': 255, 'layer3.5.conv1': 192, 'layer4.1.conv1': 299, 'layer3.2.conv1': 195, 'layer3.4.conv1': 181, 'layer3.0.downsample.0': 255, 'layer3.0.conv2': 255, 'layer3.3.conv2': 255, 'layer2.2.conv1': 106, 'layer1.2.conv1': 60, 'layer2.1.conv1': 102, 'layer1.0.conv1': 54, 'layer2.0.conv1': 111, 'layer1.1.conv1': 55}
Pruning filters.. 
Filters pruned 16.30569306930693%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 91.46 - Val acc: 88.14 - Train loss: 0.2398 - Val loss: 0.3411 - Training time: 139.45s
end number of flops: 2886607616.0 	number of params: 14465009.0
diff number of flops: 0.21580905645838244 	diff number of params: 0.3205662974225876
Final Test:
	Score: 87.17
***  ResNet34-degrad
number of flops: 3681000960.0 	number of params: 21289802.0
1 iterations to reduce 30.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 1}
Layers that will be pruned {'layer2.0.conv1': 20, 'layer3.0.conv1': 63, 'layer3.5.conv1': 58, 'layer4.1.conv1': 345, 'layer4.2.conv1': 246, 'layer4.0.conv1': 212, 'layer3.4.conv1': 85, 'layer3.0.conv2': 1, 'layer3.1.conv1': 52, 'layer3.5.conv2': 1, 'layer3.2.conv1': 69, 'layer3.0.downsample.0': 1, 'layer3.4.conv2': 1, 'layer3.3.conv1': 65, 'layer1.2.conv1': 8, 'layer2.1.conv1': 23, 'layer3.3.conv2': 1, 'layer3.1.conv2': 1, 'layer1.1.conv1': 8, 'layer2.3.conv1': 20, 'layer3.2.conv2': 1, 'layer1.0.conv1': 12, 'layer2.2.conv1': 18}
convolution remaining after pruning {'layer2.0.conv1': 108, 'layer3.0.conv1': 193, 'layer3.5.conv1': 198, 'layer4.1.conv1': 167, 'layer4.2.conv1': 266, 'layer4.0.conv1': 300, 'layer3.4.conv1': 171, 'layer3.0.conv2': 255, 'layer3.1.conv1': 204, 'layer3.5.conv2': 255, 'layer3.2.conv1': 187, 'layer3.0.downsample.0': 255, 'layer3.4.conv2': 255, 'layer3.3.conv1': 191, 'layer1.2.conv1': 56, 'layer2.1.conv1': 105, 'layer3.3.conv2': 255, 'layer3.1.conv2': 255, 'layer1.1.conv1': 56, 'layer2.3.conv1': 108, 'layer3.2.conv2': 255, 'layer1.0.conv1': 52, 'layer2.2.conv1': 110}
Pruning filters.. 
Filters pruned 20.281559405940595%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 89.12 - Val acc: 86.52 - Train loss: 0.3089 - Val loss: 0.3988 - Training time: 139.81s
end number of flops: 2759105280.0 	number of params: 12411865.0
diff number of flops: 0.25044700884837584 	diff number of params: 0.4170042069907461
Final Test:
	Score: 85.61999999999999
***  ResNet34-degrad
number of flops: 3681000960.0 	number of params: 21289802.0
1 iterations to reduce 35.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 1, 1: 1, 2: 0}
Layers that will be pruned {'conv1': 1, 'layer2.3.conv2': 1, 'layer4.2.conv1': 300, 'layer3.4.conv1': 98, 'layer3.1.conv1': 65, 'layer2.0.downsample.0': 1, 'layer4.0.conv1': 264, 'layer3.2.conv1': 93, 'layer4.1.conv1': 297, 'layer2.0.conv1': 29, 'layer2.1.conv2': 1, 'layer1.1.conv2': 1, 'layer3.3.conv1': 85, 'layer3.5.conv1': 74, 'layer2.0.conv2': 1, 'layer3.0.conv1': 78, 'layer2.2.conv2': 1, 'layer2.1.conv1': 25, 'layer2.3.conv1': 28, 'layer1.2.conv1': 10, 'layer1.0.conv1': 9, 'layer2.2.conv1': 21, 'layer1.1.conv1': 11, 'layer1.2.conv2': 1, 'layer1.0.conv2': 1}
convolution remaining after pruning {'conv1': 63, 'layer2.3.conv2': 127, 'layer4.2.conv1': 212, 'layer3.4.conv1': 158, 'layer3.1.conv1': 191, 'layer2.0.downsample.0': 127, 'layer4.0.conv1': 248, 'layer3.2.conv1': 163, 'layer4.1.conv1': 215, 'layer2.0.conv1': 99, 'layer2.1.conv2': 127, 'layer1.1.conv2': 63, 'layer3.3.conv1': 171, 'layer3.5.conv1': 182, 'layer2.0.conv2': 127, 'layer3.0.conv1': 178, 'layer2.2.conv2': 127, 'layer2.1.conv1': 103, 'layer2.3.conv1': 100, 'layer1.2.conv1': 54, 'layer1.0.conv1': 55, 'layer2.2.conv1': 107, 'layer1.1.conv1': 53, 'layer1.2.conv2': 63, 'layer1.0.conv2': 63}
Pruning filters.. 
Filters pruned 23.143564356435643%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 90.50 - Val acc: 87.84 - Train loss: 0.2770 - Val loss: 0.3526 - Training time: 140.80s
end number of flops: 2594338048.0 	number of params: 11510560.0
diff number of flops: 0.2952085380602563 	diff number of params: 0.4593392648743281
Final Test:
	Score: 87.02
***  ResNet34-degrad
number of flops: 3681000960.0 	number of params: 21289802.0
1 iterations to reduce 40.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 1, 1: 0, 2: 2}
Layers that will be pruned {'conv1': 1, 'layer4.2.conv1': 320, 'layer3.2.conv1': 96, 'layer1.0.conv1': 16, 'layer4.1.conv1': 329, 'layer4.0.conv1': 277, 'layer3.4.conv1': 115, 'layer2.3.conv1': 34, 'layer1.1.conv2': 1, 'layer1.2.conv2': 1, 'layer3.1.conv1': 96, 'layer3.5.conv1': 115, 'layer3.2.conv2': 2, 'layer3.1.conv2': 2, 'layer3.3.conv1': 96, 'layer3.0.conv2': 2, 'layer3.0.downsample.0': 2, 'layer3.4.conv2': 2, 'layer3.5.conv2': 2, 'layer3.0.conv1': 71, 'layer2.2.conv1': 31, 'layer3.3.conv2': 2, 'layer1.0.conv2': 1, 'layer2.0.conv1': 32, 'layer1.1.conv1': 17, 'layer2.1.conv1': 23, 'layer1.2.conv1': 12}
convolution remaining after pruning {'conv1': 63, 'layer4.2.conv1': 192, 'layer3.2.conv1': 160, 'layer1.0.conv1': 48, 'layer4.1.conv1': 183, 'layer4.0.conv1': 235, 'layer3.4.conv1': 141, 'layer2.3.conv1': 94, 'layer1.1.conv2': 63, 'layer1.2.conv2': 63, 'layer3.1.conv1': 160, 'layer3.5.conv1': 141, 'layer3.2.conv2': 254, 'layer3.1.conv2': 254, 'layer3.3.conv1': 160, 'layer3.0.conv2': 254, 'layer3.0.downsample.0': 254, 'layer3.4.conv2': 254, 'layer3.5.conv2': 254, 'layer3.0.conv1': 185, 'layer2.2.conv1': 97, 'layer3.3.conv2': 254, 'layer1.0.conv2': 63, 'layer2.0.conv1': 96, 'layer1.1.conv1': 47, 'layer2.1.conv1': 105, 'layer1.2.conv1': 52}
Pruning filters.. 
Filters pruned 26.268564356435643%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 89.31 - Val acc: 86.33 - Train loss: 0.3170 - Val loss: 0.3960 - Training time: 140.00s
end number of flops: 2394319104.0 	number of params: 10408333.0
diff number of flops: 0.3495467319845524 	diff number of params: 0.5111117989730483
Final Test:
	Score: 85.99
***  ResNet34-degrad
number of flops: 3681000960.0 	number of params: 21289802.0
1 iterations to reduce 45.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 2, 1: 0, 2: 1}
Layers that will be pruned {'conv1': 2, 'layer2.0.conv1': 39, 'layer4.1.conv1': 343, 'layer4.0.conv1': 276, 'layer3.4.conv2': 1, 'layer3.1.conv1': 118, 'layer3.3.conv2': 1, 'layer1.2.conv1': 16, 'layer1.0.conv1': 26, 'layer3.5.conv1': 118, 'layer3.2.conv1': 135, 'layer3.0.downsample.0': 1, 'layer3.0.conv2': 1, 'layer3.1.conv2': 1, 'layer4.2.conv1': 336, 'layer1.1.conv2': 2, 'layer3.4.conv1': 121, 'layer3.3.conv1': 139, 'layer1.1.conv1': 19, 'layer3.0.conv1': 88, 'layer3.5.conv2': 1, 'layer3.2.conv2': 1, 'layer2.2.conv1': 34, 'layer2.1.conv1': 40, 'layer1.2.conv2': 2, 'layer2.3.conv1': 31, 'layer1.0.conv2': 2}
convolution remaining after pruning {'conv1': 62, 'layer2.0.conv1': 89, 'layer4.1.conv1': 169, 'layer4.0.conv1': 236, 'layer3.4.conv2': 255, 'layer3.1.conv1': 138, 'layer3.3.conv2': 255, 'layer1.2.conv1': 48, 'layer1.0.conv1': 38, 'layer3.5.conv1': 138, 'layer3.2.conv1': 121, 'layer3.0.downsample.0': 255, 'layer3.0.conv2': 255, 'layer3.1.conv2': 255, 'layer4.2.conv1': 176, 'layer1.1.conv2': 62, 'layer3.4.conv1': 135, 'layer3.3.conv1': 117, 'layer1.1.conv1': 45, 'layer3.0.conv1': 168, 'layer3.5.conv2': 255, 'layer3.2.conv2': 255, 'layer2.2.conv1': 94, 'layer2.1.conv1': 88, 'layer1.2.conv2': 62, 'layer2.3.conv1': 97, 'layer1.0.conv2': 62}
Pruning filters.. 
Filters pruned 29.300742574257427%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 87.08 - Val acc: 83.33 - Train loss: 0.3833 - Val loss: 0.4792 - Training time: 138.69s
end number of flops: 2163628800.0 	number of params: 9506452.0
diff number of flops: 0.4122172682073954 	diff number of params: 0.5534739120636256
Final Test:
	Score: 83.21
***  ResNet34-degrad
number of flops: 3681000960.0 	number of params: 21289802.0
1 iterations to reduce 50.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 5, 1: 1, 2: 7}
Layers that will be pruned {'conv1': 5, 'layer1.1.conv1': 14, 'layer3.1.conv1': 111, 'layer3.3.conv1': 133, 'layer3.5.conv1': 135, 'layer4.1.conv1': 355, 'layer4.2.conv1': 346, 'layer3.5.conv2': 7, 'layer3.3.conv2': 7, 'layer3.0.downsample.0': 7, 'layer4.0.conv1': 334, 'layer1.2.conv2': 5, 'layer3.2.conv1': 127, 'layer2.2.conv2': 1, 'layer3.1.conv2': 7, 'layer3.0.conv1': 116, 'layer1.1.conv2': 5, 'layer2.3.conv2': 1, 'layer3.4.conv1': 143, 'layer3.2.conv2': 7, 'layer3.0.conv2': 7, 'layer2.0.conv1': 56, 'layer3.4.conv2': 7, 'layer2.0.conv2': 1, 'layer2.0.downsample.0': 1, 'layer2.2.conv1': 47, 'layer2.1.conv1': 46, 'layer1.2.conv1': 14, 'layer2.1.conv2': 1, 'layer1.0.conv2': 5, 'layer2.3.conv1': 45, 'layer1.0.conv1': 22}
convolution remaining after pruning {'conv1': 59, 'layer1.1.conv1': 50, 'layer3.1.conv1': 145, 'layer3.3.conv1': 123, 'layer3.5.conv1': 121, 'layer4.1.conv1': 157, 'layer4.2.conv1': 166, 'layer3.5.conv2': 249, 'layer3.3.conv2': 249, 'layer3.0.downsample.0': 249, 'layer4.0.conv1': 178, 'layer1.2.conv2': 59, 'layer3.2.conv1': 129, 'layer2.2.conv2': 127, 'layer3.1.conv2': 249, 'layer3.0.conv1': 140, 'layer1.1.conv2': 59, 'layer2.3.conv2': 127, 'layer3.4.conv1': 113, 'layer3.2.conv2': 249, 'layer3.0.conv2': 249, 'layer2.0.conv1': 72, 'layer3.4.conv2': 249, 'layer2.0.conv2': 127, 'layer2.0.downsample.0': 127, 'layer2.2.conv1': 81, 'layer2.1.conv1': 82, 'layer1.2.conv1': 50, 'layer2.1.conv2': 127, 'layer1.0.conv2': 59, 'layer2.3.conv1': 83, 'layer1.0.conv1': 42}
Pruning filters.. 
Filters pruned 32.76608910891089%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 77.46 - Val acc: 74.80 - Train loss: 0.6421 - Val loss: 0.7304 - Training time: 138.04s
end number of flops: 2003238912.0 	number of params: 8524675.0
diff number of flops: 0.4557896252219396 	diff number of params: 0.5995888078245162
Final Test:
	Score: 74.46000000000001
***  ResNet50-degrad
number of flops: 4138573824.0 	number of params: 23528522.0
Epoch 0 - Train acc: 99.73 - Val acc: 99.18 - Train loss: 0.0092 - Val loss: 0.0262 - Training time: 199.51s
end number of flops: 4138573824.0 	number of params: 23528522.0
diff number of flops: 0.0 	diff number of params: 0.0
Final Test:
	Score: 95.78999999999999
***  ResNet50-degrad
number of flops: 4138573824.0 	number of params: 23528522.0
1 iterations to reduce 5.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 0}
Layers that will be pruned {'conv1': 1, 'layer1.0.conv1': 6, 'layer1.0.conv2': 5, 'layer1.1.conv1': 4, 'layer4.2.conv2': 28, 'layer4.2.conv1': 16, 'layer4.1.conv2': 23, 'layer3.0.conv1': 5, 'layer3.4.conv2': 13, 'layer4.0.conv2': 12, 'layer3.4.conv1': 1, 'layer3.3.conv2': 6, 'layer4.1.conv1': 15, 'layer4.0.conv1': 15, 'layer3.5.conv1': 4, 'layer3.2.conv1': 4, 'layer2.1.conv1': 3, 'layer3.3.conv1': 5, 'layer3.0.conv2': 4, 'layer2.0.conv1': 3, 'layer3.2.conv2': 2, 'layer2.0.conv2': 1, 'layer2.3.conv2': 1, 'layer3.1.conv1': 5, 'layer1.2.conv1': 1, 'layer3.5.conv2': 3, 'layer3.1.conv2': 4, 'layer2.1.conv2': 3}
convolution remaining after pruning {'conv1': 63, 'layer1.0.conv1': 58, 'layer1.0.conv2': 59, 'layer1.1.conv1': 60, 'layer4.2.conv2': 484, 'layer4.2.conv1': 496, 'layer4.1.conv2': 489, 'layer3.0.conv1': 251, 'layer3.4.conv2': 243, 'layer4.0.conv2': 500, 'layer3.4.conv1': 255, 'layer3.3.conv2': 250, 'layer4.1.conv1': 497, 'layer4.0.conv1': 497, 'layer3.5.conv1': 252, 'layer3.2.conv1': 252, 'layer2.1.conv1': 125, 'layer3.3.conv1': 251, 'layer3.0.conv2': 252, 'layer2.0.conv1': 125, 'layer3.2.conv2': 254, 'layer2.0.conv2': 127, 'layer2.3.conv2': 127, 'layer3.1.conv1': 251, 'layer1.2.conv1': 63, 'layer3.5.conv2': 253, 'layer3.1.conv2': 252, 'layer2.1.conv2': 125}
Pruning filters.. 
Filters pruned 1.050740418118467%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 97.75 - Val acc: 95.20 - Train loss: 0.0712 - Val loss: 0.1411 - Training time: 197.75s
end number of flops: 4008592128.0 	number of params: 22614088.0
diff number of flops: 0.03140736435489522 	diff number of params: 0.0388649146767485
Final Test:
	Score: 93.12
***  ResNet50-degrad
number of flops: 4138573824.0 	number of params: 23528522.0
1 iterations to reduce 10.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 0}
Layers that will be pruned {'conv1': 1, 'layer1.0.conv1': 10, 'layer1.0.conv2': 6, 'layer1.1.conv1': 5, 'layer3.4.conv1': 18, 'layer3.5.conv1': 15, 'layer4.1.conv2': 42, 'layer4.2.conv2': 71, 'layer4.0.conv2': 42, 'layer3.2.conv1': 13, 'layer4.1.conv1': 43, 'layer3.1.conv2': 9, 'layer2.2.conv2': 5, 'layer3.0.conv2': 20, 'layer2.3.conv1': 7, 'layer4.0.conv1': 29, 'layer2.0.conv1': 4, 'layer2.0.conv2': 5, 'layer4.2.conv1': 55, 'layer3.5.conv2': 19, 'layer3.1.conv1': 14, 'layer3.3.conv2': 16, 'layer1.2.conv2': 4, 'layer2.2.conv1': 5, 'layer2.1.conv1': 11, 'layer2.1.conv2': 5, 'layer3.3.conv1': 11, 'layer3.4.conv2': 9, 'layer3.0.conv1': 11, 'layer2.3.conv2': 4, 'layer1.1.conv2': 1, 'layer1.2.conv1': 1, 'layer3.2.conv2': 6}
convolution remaining after pruning {'conv1': 63, 'layer1.0.conv1': 54, 'layer1.0.conv2': 58, 'layer1.1.conv1': 59, 'layer3.4.conv1': 238, 'layer3.5.conv1': 241, 'layer4.1.conv2': 470, 'layer4.2.conv2': 441, 'layer4.0.conv2': 470, 'layer3.2.conv1': 243, 'layer4.1.conv1': 469, 'layer3.1.conv2': 247, 'layer2.2.conv2': 123, 'layer3.0.conv2': 236, 'layer2.3.conv1': 121, 'layer4.0.conv1': 483, 'layer2.0.conv1': 124, 'layer2.0.conv2': 123, 'layer4.2.conv1': 457, 'layer3.5.conv2': 237, 'layer3.1.conv1': 242, 'layer3.3.conv2': 240, 'layer1.2.conv2': 60, 'layer2.2.conv1': 123, 'layer2.1.conv1': 117, 'layer2.1.conv2': 123, 'layer3.3.conv1': 245, 'layer3.4.conv2': 247, 'layer3.0.conv1': 245, 'layer2.3.conv2': 124, 'layer1.1.conv2': 63, 'layer1.2.conv1': 63, 'layer3.2.conv2': 250}
Pruning filters.. 
Filters pruned 2.8146777003484322%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 96.69 - Val acc: 93.56 - Train loss: 0.1078 - Val loss: 0.1833 - Training time: 194.61s
end number of flops: 3814542592.0 	number of params: 21127052.0
diff number of flops: 0.07829538526554987 	diff number of params: 0.10206633463844435
Final Test:
	Score: 92.27
***  ResNet50-degrad
number of flops: 4138573824.0 	number of params: 23528522.0
1 iterations to reduce 15.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 1, 1: 0, 2: 0}
Layers that will be pruned {'conv1': 5, 'layer1.0.conv1': 14, 'layer1.0.conv2': 8, 'layer1.0.conv3': 1, 'layer1.0.downsample.0': 1, 'layer1.1.conv1': 8, 'layer1.1.conv3': 1, 'layer1.2.conv3': 1, 'layer4.0.conv1': 65, 'layer4.1.conv1': 71, 'layer4.2.conv1': 67, 'layer3.0.conv1': 23, 'layer4.2.conv2': 70, 'layer3.5.conv2': 36, 'layer3.2.conv1': 26, 'layer3.4.conv2': 21, 'layer4.0.conv2': 66, 'layer4.1.conv2': 89, 'layer1.2.conv1': 1, 'layer3.2.conv2': 21, 'layer2.3.conv1': 7, 'layer3.4.conv1': 17, 'layer3.5.conv1': 20, 'layer2.1.conv1': 8, 'layer2.0.conv1': 7, 'layer2.1.conv2': 10, 'layer3.3.conv1': 25, 'layer3.3.conv2': 19, 'layer3.0.conv2': 17, 'layer2.0.conv2': 8, 'layer2.2.conv2': 4, 'layer3.1.conv2': 22, 'layer3.1.conv1': 18, 'layer2.3.conv2': 8, 'layer1.2.conv2': 1, 'layer1.1.conv2': 2, 'layer2.2.conv1': 5}
convolution remaining after pruning {'conv1': 59, 'layer1.0.conv1': 50, 'layer1.0.conv2': 56, 'layer1.0.conv3': 255, 'layer1.0.downsample.0': 255, 'layer1.1.conv1': 56, 'layer1.1.conv3': 255, 'layer1.2.conv3': 255, 'layer4.0.conv1': 447, 'layer4.1.conv1': 441, 'layer4.2.conv1': 445, 'layer3.0.conv1': 233, 'layer4.2.conv2': 442, 'layer3.5.conv2': 220, 'layer3.2.conv1': 230, 'layer3.4.conv2': 235, 'layer4.0.conv2': 446, 'layer4.1.conv2': 423, 'layer1.2.conv1': 63, 'layer3.2.conv2': 235, 'layer2.3.conv1': 121, 'layer3.4.conv1': 239, 'layer3.5.conv1': 236, 'layer2.1.conv1': 120, 'layer2.0.conv1': 121, 'layer2.1.conv2': 118, 'layer3.3.conv1': 231, 'layer3.3.conv2': 237, 'layer3.0.conv2': 239, 'layer2.0.conv2': 120, 'layer2.2.conv2': 124, 'layer3.1.conv2': 234, 'layer3.1.conv1': 238, 'layer2.3.conv2': 120, 'layer1.2.conv2': 63, 'layer1.1.conv2': 62, 'layer2.2.conv1': 123}
Pruning filters.. 
Filters pruned 4.317290940766551%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 95.49 - Val acc: 92.35 - Train loss: 0.1377 - Val loss: 0.2217 - Training time: 195.37s
end number of flops: 3657109248.0 	number of params: 19919396.0
diff number of flops: 0.1163358674932749 	diff number of params: 0.1533936555810858
Final Test:
	Score: 91.36
***  ResNet50-degrad
number of flops: 4138573824.0 	number of params: 23528522.0
1 iterations to reduce 20.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 3, 1: 1, 2: 1}
Layers that will be pruned {'conv1': 6, 'layer1.0.conv1': 11, 'layer1.0.conv2': 6, 'layer1.0.conv3': 3, 'layer1.0.downsample.0': 3, 'layer1.1.conv1': 8, 'layer1.1.conv3': 3, 'layer2.0.conv3': 1, 'layer2.0.downsample.0': 1, 'layer2.1.conv3': 1, 'layer3.0.conv3': 1, 'layer3.0.downsample.0': 1, 'layer3.1.conv3': 1, 'layer3.2.conv3': 1, 'layer3.3.conv3': 1, 'layer3.4.conv3': 1, 'layer3.5.conv3': 1, 'layer1.2.conv3': 3, 'layer3.2.conv1': 34, 'layer4.0.conv1': 105, 'layer4.2.conv1': 102, 'layer3.3.conv2': 35, 'layer3.4.conv1': 29, 'layer2.2.conv3': 1, 'layer2.3.conv3': 1, 'layer3.2.conv2': 31, 'layer4.1.conv1': 80, 'layer4.0.conv2': 75, 'layer4.2.conv2': 120, 'layer2.0.conv1': 3, 'layer3.1.conv2': 32, 'layer3.1.conv1': 26, 'layer2.1.conv1': 16, 'layer2.3.conv1': 8, 'layer3.5.conv2': 29, 'layer3.5.conv1': 25, 'layer3.3.conv1': 19, 'layer4.1.conv2': 88, 'layer1.2.conv1': 4, 'layer2.0.conv2': 11, 'layer3.0.conv1': 31, 'layer3.4.conv2': 24, 'layer2.3.conv2': 4, 'layer3.0.conv2': 25, 'layer2.2.conv1': 9, 'layer1.2.conv2': 3, 'layer2.1.conv2': 12, 'layer2.2.conv2': 5, 'layer1.1.conv2': 1}
convolution remaining after pruning {'conv1': 58, 'layer1.0.conv1': 53, 'layer1.0.conv2': 58, 'layer1.0.conv3': 253, 'layer1.0.downsample.0': 253, 'layer1.1.conv1': 56, 'layer1.1.conv3': 253, 'layer2.0.conv3': 511, 'layer2.0.downsample.0': 511, 'layer2.1.conv3': 511, 'layer3.0.conv3': 1023, 'layer3.0.downsample.0': 1023, 'layer3.1.conv3': 1023, 'layer3.2.conv3': 1023, 'layer3.3.conv3': 1023, 'layer3.4.conv3': 1023, 'layer3.5.conv3': 1023, 'layer1.2.conv3': 253, 'layer3.2.conv1': 222, 'layer4.0.conv1': 407, 'layer4.2.conv1': 410, 'layer3.3.conv2': 221, 'layer3.4.conv1': 227, 'layer2.2.conv3': 511, 'layer2.3.conv3': 511, 'layer3.2.conv2': 225, 'layer4.1.conv1': 432, 'layer4.0.conv2': 437, 'layer4.2.conv2': 392, 'layer2.0.conv1': 125, 'layer3.1.conv2': 224, 'layer3.1.conv1': 230, 'layer2.1.conv1': 112, 'layer2.3.conv1': 120, 'layer3.5.conv2': 227, 'layer3.5.conv1': 231, 'layer3.3.conv1': 237, 'layer4.1.conv2': 424, 'layer1.2.conv1': 60, 'layer2.0.conv2': 117, 'layer3.0.conv1': 225, 'layer3.4.conv2': 232, 'layer2.3.conv2': 124, 'layer3.0.conv2': 231, 'layer2.2.conv1': 119, 'layer1.2.conv2': 61, 'layer2.1.conv2': 116, 'layer2.2.conv2': 123, 'layer1.1.conv2': 63}
Pruning filters.. 
Filters pruned 5.667465156794425%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 83.13 - Val acc: 80.25 - Train loss: 0.4980 - Val loss: 0.5788 - Training time: 197.66s
end number of flops: 3543236608.0 	number of params: 18863754.0
diff number of flops: 0.14385081463270763 	diff number of params: 0.1982601372070885
Final Test:
	Score: 79.44
***  ResNet50-degrad
number of flops: 4138573824.0 	number of params: 23528522.0
1 iterations to reduce 25.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 6, 1: 2, 2: 5}
Layers that will be pruned {'conv1': 8, 'layer1.0.conv1': 13, 'layer1.0.conv2': 8, 'layer1.0.conv3': 6, 'layer1.0.downsample.0': 6, 'layer1.1.conv1': 10, 'layer1.1.conv3': 6, 'layer2.0.conv3': 2, 'layer2.0.downsample.0': 2, 'layer2.1.conv3': 2, 'layer3.0.conv3': 5, 'layer3.0.downsample.0': 5, 'layer3.1.conv3': 5, 'layer3.2.conv3': 5, 'layer3.3.conv3': 5, 'layer3.5.conv3': 5, 'layer3.4.conv3': 5, 'layer4.0.conv1': 121, 'layer4.1.conv2': 118, 'layer4.0.conv2': 131, 'layer3.3.conv1': 53, 'layer4.1.conv1': 125, 'layer3.2.conv2': 30, 'layer1.2.conv3': 6, 'layer2.2.conv1': 21, 'layer3.0.conv2': 42, 'layer4.2.conv1': 114, 'layer4.2.conv2': 156, 'layer3.0.conv1': 43, 'layer2.0.conv2': 16, 'layer3.1.conv2': 38, 'layer2.3.conv3': 2, 'layer2.2.conv3': 2, 'layer3.1.conv1': 25, 'layer3.5.conv2': 43, 'layer2.3.conv2': 9, 'layer3.4.conv2': 45, 'layer3.5.conv1': 35, 'layer3.2.conv1': 40, 'layer2.0.conv1': 15, 'layer2.1.conv1': 22, 'layer2.2.conv2': 17, 'layer3.3.conv2': 31, 'layer3.4.conv1': 24, 'layer1.2.conv1': 6, 'layer1.1.conv2': 5, 'layer2.1.conv2': 19, 'layer2.3.conv1': 8, 'layer1.2.conv2': 2}
convolution remaining after pruning {'conv1': 56, 'layer1.0.conv1': 51, 'layer1.0.conv2': 56, 'layer1.0.conv3': 250, 'layer1.0.downsample.0': 250, 'layer1.1.conv1': 54, 'layer1.1.conv3': 250, 'layer2.0.conv3': 510, 'layer2.0.downsample.0': 510, 'layer2.1.conv3': 510, 'layer3.0.conv3': 1019, 'layer3.0.downsample.0': 1019, 'layer3.1.conv3': 1019, 'layer3.2.conv3': 1019, 'layer3.3.conv3': 1019, 'layer3.5.conv3': 1019, 'layer3.4.conv3': 1019, 'layer4.0.conv1': 391, 'layer4.1.conv2': 394, 'layer4.0.conv2': 381, 'layer3.3.conv1': 203, 'layer4.1.conv1': 387, 'layer3.2.conv2': 226, 'layer1.2.conv3': 250, 'layer2.2.conv1': 107, 'layer3.0.conv2': 214, 'layer4.2.conv1': 398, 'layer4.2.conv2': 356, 'layer3.0.conv1': 213, 'layer2.0.conv2': 112, 'layer3.1.conv2': 218, 'layer2.3.conv3': 510, 'layer2.2.conv3': 510, 'layer3.1.conv1': 231, 'layer3.5.conv2': 213, 'layer2.3.conv2': 119, 'layer3.4.conv2': 211, 'layer3.5.conv1': 221, 'layer3.2.conv1': 216, 'layer2.0.conv1': 113, 'layer2.1.conv1': 106, 'layer2.2.conv2': 111, 'layer3.3.conv2': 225, 'layer3.4.conv1': 232, 'layer1.2.conv1': 58, 'layer1.1.conv2': 59, 'layer2.1.conv2': 109, 'layer2.3.conv1': 120, 'layer1.2.conv2': 62}
Pruning filters.. 
Filters pruned 7.959494773519164%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 82.92 - Val acc: 79.41 - Train loss: 0.4949 - Val loss: 0.5816 - Training time: 188.51s
end number of flops: 3302521088.0 	number of params: 17323414.0
diff number of flops: 0.202014696742063 	diff number of params: 0.26372706283888125
Final Test:
	Score: 79.29
***  ResNet50-degrad
number of flops: 4138573824.0 	number of params: 23528522.0
1 iterations to reduce 30.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 7, 1: 1, 2: 9}
Layers that will be pruned {'conv1': 13, 'layer1.0.conv1': 13, 'layer1.0.conv2': 17, 'layer1.0.conv3': 7, 'layer1.0.downsample.0': 7, 'layer1.1.conv1': 10, 'layer1.1.conv3': 7, 'layer2.0.conv3': 1, 'layer2.0.downsample.0': 1, 'layer2.1.conv3': 1, 'layer3.0.conv3': 9, 'layer3.0.downsample.0': 9, 'layer3.1.conv3': 9, 'layer3.2.conv3': 9, 'layer3.3.conv3': 9, 'layer3.5.conv3': 9, 'layer3.4.conv3': 9, 'layer4.1.conv2': 192, 'layer3.5.conv1': 53, 'layer4.1.conv1': 142, 'layer4.2.conv2': 169, 'layer2.3.conv3': 1, 'layer3.5.conv2': 43, 'layer3.0.conv2': 47, 'layer3.4.conv1': 44, 'layer3.2.conv1': 57, 'layer2.2.conv3': 1, 'layer3.3.conv2': 38, 'layer1.2.conv3': 7, 'layer4.0.conv1': 140, 'layer2.3.conv2': 18, 'layer4.2.conv1': 149, 'layer4.0.conv2': 138, 'layer2.1.conv1': 26, 'layer3.0.conv1': 46, 'layer3.1.conv1': 50, 'layer2.2.conv1': 17, 'layer3.4.conv2': 43, 'layer2.1.conv2': 15, 'layer3.2.conv2': 43, 'layer3.3.conv1': 50, 'layer2.2.conv2': 7, 'layer2.0.conv2': 17, 'layer3.1.conv2': 42, 'layer1.2.conv1': 2, 'layer1.2.conv2': 6, 'layer2.3.conv1': 11, 'layer1.1.conv2': 6, 'layer2.0.conv1': 10}
convolution remaining after pruning {'conv1': 51, 'layer1.0.conv1': 51, 'layer1.0.conv2': 47, 'layer1.0.conv3': 249, 'layer1.0.downsample.0': 249, 'layer1.1.conv1': 54, 'layer1.1.conv3': 249, 'layer2.0.conv3': 511, 'layer2.0.downsample.0': 511, 'layer2.1.conv3': 511, 'layer3.0.conv3': 1015, 'layer3.0.downsample.0': 1015, 'layer3.1.conv3': 1015, 'layer3.2.conv3': 1015, 'layer3.3.conv3': 1015, 'layer3.5.conv3': 1015, 'layer3.4.conv3': 1015, 'layer4.1.conv2': 320, 'layer3.5.conv1': 203, 'layer4.1.conv1': 370, 'layer4.2.conv2': 343, 'layer2.3.conv3': 511, 'layer3.5.conv2': 213, 'layer3.0.conv2': 209, 'layer3.4.conv1': 212, 'layer3.2.conv1': 199, 'layer2.2.conv3': 511, 'layer3.3.conv2': 218, 'layer1.2.conv3': 249, 'layer4.0.conv1': 372, 'layer2.3.conv2': 110, 'layer4.2.conv1': 363, 'layer4.0.conv2': 374, 'layer2.1.conv1': 102, 'layer3.0.conv1': 210, 'layer3.1.conv1': 206, 'layer2.2.conv1': 111, 'layer3.4.conv2': 213, 'layer2.1.conv2': 113, 'layer3.2.conv2': 213, 'layer3.3.conv1': 206, 'layer2.2.conv2': 121, 'layer2.0.conv2': 111, 'layer3.1.conv2': 214, 'layer1.2.conv1': 62, 'layer1.2.conv2': 58, 'layer2.3.conv1': 117, 'layer1.1.conv2': 58, 'layer2.0.conv1': 118}
Pruning filters.. 
Filters pruned 9.636324041811847%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 83.08 - Val acc: 79.09 - Train loss: 0.4955 - Val loss: 0.5931 - Training time: 182.98s
end number of flops: 3162799104.0 	number of params: 16119678.0
diff number of flops: 0.2357755984299194 	diff number of params: 0.3148877774813055
Final Test:
	Score: 79.65
***  ResNet50-degrad
number of flops: 4138573824.0 	number of params: 23528522.0
1 iterations to reduce 35.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 13, 1: 2, 2: 8}
Layers that will be pruned {'conv1': 18, 'layer1.0.conv1': 18, 'layer1.0.conv2': 14, 'layer1.0.conv3': 13, 'layer1.0.downsample.0': 13, 'layer1.1.conv1': 7, 'layer1.1.conv3': 13, 'layer2.0.conv3': 2, 'layer2.0.downsample.0': 2, 'layer2.1.conv3': 2, 'layer3.0.conv3': 8, 'layer3.0.downsample.0': 8, 'layer3.1.conv3': 8, 'layer3.2.conv3': 8, 'layer3.3.conv3': 8, 'layer3.4.conv3': 8, 'layer4.0.conv1': 144, 'layer2.3.conv3': 2, 'layer3.5.conv3': 8, 'layer4.2.conv1': 210, 'layer4.0.conv2': 162, 'layer4.1.conv1': 197, 'layer3.4.conv2': 58, 'layer3.1.conv2': 56, 'layer1.2.conv3': 13, 'layer3.0.conv1': 57, 'layer3.2.conv2': 52, 'layer4.2.conv2': 251, 'layer3.1.conv1': 59, 'layer3.5.conv1': 53, 'layer3.3.conv2': 55, 'layer4.1.conv2': 178, 'layer3.5.conv2': 66, 'layer2.2.conv3': 2, 'layer2.0.conv2': 21, 'layer2.1.conv2': 35, 'layer3.2.conv1': 47, 'layer2.3.conv2': 17, 'layer3.3.conv1': 50, 'layer2.3.conv1': 19, 'layer3.4.conv1': 56, 'layer1.2.conv1': 6, 'layer2.1.conv1': 27, 'layer3.0.conv2': 50, 'layer2.2.conv2': 23, 'layer2.0.conv1': 26, 'layer2.2.conv1': 21, 'layer1.2.conv2': 5, 'layer1.1.conv2': 6}
convolution remaining after pruning {'conv1': 46, 'layer1.0.conv1': 46, 'layer1.0.conv2': 50, 'layer1.0.conv3': 243, 'layer1.0.downsample.0': 243, 'layer1.1.conv1': 57, 'layer1.1.conv3': 243, 'layer2.0.conv3': 510, 'layer2.0.downsample.0': 510, 'layer2.1.conv3': 510, 'layer3.0.conv3': 1016, 'layer3.0.downsample.0': 1016, 'layer3.1.conv3': 1016, 'layer3.2.conv3': 1016, 'layer3.3.conv3': 1016, 'layer3.4.conv3': 1016, 'layer4.0.conv1': 368, 'layer2.3.conv3': 510, 'layer3.5.conv3': 1016, 'layer4.2.conv1': 302, 'layer4.0.conv2': 350, 'layer4.1.conv1': 315, 'layer3.4.conv2': 198, 'layer3.1.conv2': 200, 'layer1.2.conv3': 243, 'layer3.0.conv1': 199, 'layer3.2.conv2': 204, 'layer4.2.conv2': 261, 'layer3.1.conv1': 197, 'layer3.5.conv1': 203, 'layer3.3.conv2': 201, 'layer4.1.conv2': 334, 'layer3.5.conv2': 190, 'layer2.2.conv3': 510, 'layer2.0.conv2': 107, 'layer2.1.conv2': 93, 'layer3.2.conv1': 209, 'layer2.3.conv2': 111, 'layer3.3.conv1': 206, 'layer2.3.conv1': 109, 'layer3.4.conv1': 200, 'layer1.2.conv1': 58, 'layer2.1.conv1': 101, 'layer3.0.conv2': 206, 'layer2.2.conv2': 105, 'layer2.0.conv1': 102, 'layer2.2.conv1': 107, 'layer1.2.conv2': 59, 'layer1.1.conv2': 58}
Pruning filters.. 
Filters pruned 11.879355400696864%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 81.74 - Val acc: 77.48 - Train loss: 0.5265 - Val loss: 0.6451 - Training time: 176.16s
end number of flops: 2938815232.0 	number of params: 14676043.0
diff number of flops: 0.2898966269593842 	diff number of params: 0.3762445851889889
Final Test:
	Score: 78.43
***  ResNet50-degrad
number of flops: 4138573824.0 	number of params: 23528522.0
1 iterations to reduce 40.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 11, 1: 9, 2: 42}
Layers that will be pruned {'conv1': 15, 'layer1.0.conv1': 24, 'layer1.0.conv2': 19, 'layer1.0.conv3': 11, 'layer1.0.downsample.0': 11, 'layer1.1.conv1': 9, 'layer1.1.conv3': 11, 'layer2.0.conv3': 9, 'layer2.0.downsample.0': 9, 'layer2.1.conv3': 9, 'layer3.0.conv3': 42, 'layer3.0.downsample.0': 42, 'layer3.1.conv3': 42, 'layer3.2.conv3': 42, 'layer3.3.conv3': 42, 'layer3.5.conv3': 42, 'layer4.1.conv1': 207, 'layer4.0.conv2': 206, 'layer3.4.conv3': 42, 'layer3.1.conv2': 80, 'layer3.2.conv1': 64, 'layer3.4.conv1': 71, 'layer2.3.conv3': 9, 'layer4.0.conv1': 177, 'layer1.2.conv3': 11, 'layer3.0.conv2': 59, 'layer4.1.conv2': 248, 'layer4.2.conv2': 252, 'layer3.3.conv2': 63, 'layer3.1.conv1': 63, 'layer3.5.conv1': 85, 'layer4.2.conv1': 189, 'layer1.1.conv2': 6, 'layer2.3.conv2': 25, 'layer2.0.conv2': 17, 'layer2.2.conv3': 9, 'layer3.4.conv2': 78, 'layer2.2.conv1': 15, 'layer3.3.conv1': 70, 'layer3.5.conv2': 71, 'layer2.1.conv2': 24, 'layer3.2.conv2': 62, 'layer3.0.conv1': 59, 'layer2.1.conv1': 30, 'layer2.0.conv1': 27, 'layer2.2.conv2': 28, 'layer2.3.conv1': 15, 'layer1.2.conv2': 8, 'layer1.2.conv1': 10}
convolution remaining after pruning {'conv1': 49, 'layer1.0.conv1': 40, 'layer1.0.conv2': 45, 'layer1.0.conv3': 245, 'layer1.0.downsample.0': 245, 'layer1.1.conv1': 55, 'layer1.1.conv3': 245, 'layer2.0.conv3': 503, 'layer2.0.downsample.0': 503, 'layer2.1.conv3': 503, 'layer3.0.conv3': 982, 'layer3.0.downsample.0': 982, 'layer3.1.conv3': 982, 'layer3.2.conv3': 982, 'layer3.3.conv3': 982, 'layer3.5.conv3': 982, 'layer4.1.conv1': 305, 'layer4.0.conv2': 306, 'layer3.4.conv3': 982, 'layer3.1.conv2': 176, 'layer3.2.conv1': 192, 'layer3.4.conv1': 185, 'layer2.3.conv3': 503, 'layer4.0.conv1': 335, 'layer1.2.conv3': 245, 'layer3.0.conv2': 197, 'layer4.1.conv2': 264, 'layer4.2.conv2': 260, 'layer3.3.conv2': 193, 'layer3.1.conv1': 193, 'layer3.5.conv1': 171, 'layer4.2.conv1': 323, 'layer1.1.conv2': 58, 'layer2.3.conv2': 103, 'layer2.0.conv2': 111, 'layer2.2.conv3': 503, 'layer3.4.conv2': 178, 'layer2.2.conv1': 113, 'layer3.3.conv1': 186, 'layer3.5.conv2': 185, 'layer2.1.conv2': 104, 'layer3.2.conv2': 194, 'layer3.0.conv1': 197, 'layer2.1.conv1': 98, 'layer2.0.conv1': 101, 'layer2.2.conv2': 100, 'layer2.3.conv1': 113, 'layer1.2.conv2': 56, 'layer1.2.conv1': 54}
Pruning filters.. 
Filters pruned 15.020688153310104%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 78.09 - Val acc: 74.80 - Train loss: 0.6295 - Val loss: 0.7280 - Training time: 184.38s
end number of flops: 2759742208.0 	number of params: 13377039.0
diff number of flops: 0.3331658862780262 	diff number of params: 0.4314543429459785
Final Test:
	Score: 74.49
***  ResNet50-degrad
number of flops: 4138573824.0 	number of params: 23528522.0
1 iterations to reduce 45.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 26, 1: 19, 2: 64}
Layers that will be pruned {'conv1': 17, 'layer1.0.conv1': 20, 'layer1.0.conv2': 20, 'layer1.0.conv3': 26, 'layer1.0.downsample.0': 26, 'layer1.1.conv1': 18, 'layer1.1.conv3': 26, 'layer2.0.conv3': 19, 'layer2.0.downsample.0': 19, 'layer2.1.conv3': 19, 'layer3.0.conv3': 64, 'layer3.0.downsample.0': 64, 'layer3.1.conv3': 64, 'layer3.2.conv3': 64, 'layer3.3.conv3': 64, 'layer2.2.conv3': 19, 'layer3.4.conv3': 64, 'layer3.5.conv3': 64, 'layer1.2.conv3': 26, 'layer4.1.conv1': 218, 'layer4.2.conv1': 211, 'layer4.2.conv2': 289, 'layer3.4.conv2': 83, 'layer2.3.conv3': 19, 'layer4.1.conv2': 236, 'layer2.1.conv2': 44, 'layer3.3.conv1': 46, 'layer2.3.conv1': 20, 'layer3.0.conv2': 75, 'layer3.0.conv1': 87, 'layer3.1.conv1': 71, 'layer4.0.conv2': 178, 'layer3.5.conv2': 71, 'layer4.0.conv1': 184, 'layer2.2.conv2': 39, 'layer3.2.conv2': 72, 'layer3.5.conv1': 72, 'layer3.2.conv1': 80, 'layer3.4.conv1': 87, 'layer2.3.conv2': 25, 'layer2.1.conv1': 36, 'layer3.3.conv2': 68, 'layer2.0.conv1': 26, 'layer2.2.conv1': 28, 'layer3.1.conv2': 77, 'layer1.2.conv1': 11, 'layer1.1.conv2': 12, 'layer2.0.conv2': 28, 'layer1.2.conv2': 11}
convolution remaining after pruning {'conv1': 47, 'layer1.0.conv1': 44, 'layer1.0.conv2': 44, 'layer1.0.conv3': 230, 'layer1.0.downsample.0': 230, 'layer1.1.conv1': 46, 'layer1.1.conv3': 230, 'layer2.0.conv3': 493, 'layer2.0.downsample.0': 493, 'layer2.1.conv3': 493, 'layer3.0.conv3': 960, 'layer3.0.downsample.0': 960, 'layer3.1.conv3': 960, 'layer3.2.conv3': 960, 'layer3.3.conv3': 960, 'layer2.2.conv3': 493, 'layer3.4.conv3': 960, 'layer3.5.conv3': 960, 'layer1.2.conv3': 230, 'layer4.1.conv1': 294, 'layer4.2.conv1': 301, 'layer4.2.conv2': 223, 'layer3.4.conv2': 173, 'layer2.3.conv3': 493, 'layer4.1.conv2': 276, 'layer2.1.conv2': 84, 'layer3.3.conv1': 210, 'layer2.3.conv1': 108, 'layer3.0.conv2': 181, 'layer3.0.conv1': 169, 'layer3.1.conv1': 185, 'layer4.0.conv2': 334, 'layer3.5.conv2': 185, 'layer4.0.conv1': 328, 'layer2.2.conv2': 89, 'layer3.2.conv2': 184, 'layer3.5.conv1': 184, 'layer3.2.conv1': 176, 'layer3.4.conv1': 169, 'layer2.3.conv2': 103, 'layer2.1.conv1': 92, 'layer3.3.conv2': 188, 'layer2.0.conv1': 102, 'layer2.2.conv1': 100, 'layer3.1.conv2': 179, 'layer1.2.conv1': 53, 'layer1.1.conv2': 52, 'layer2.0.conv2': 100, 'layer1.2.conv2': 53}
Pruning filters.. 
Filters pruned 17.45971254355401%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 80.51 - Val acc: 78.22 - Train loss: 0.5686 - Val loss: 0.6306 - Training time: 174.09s
end number of flops: 2546199552.0 	number of params: 12825459.0
diff number of flops: 0.3847640128504326 	diff number of params: 0.45489737944440367
Final Test:
	Score: 77.53999999999999
***  ResNet50-degrad
number of flops: 4138573824.0 	number of params: 23528522.0
1 iterations to reduce 50.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 28, 1: 28, 2: 96}
Layers that will be pruned {'conv1': 16, 'layer1.0.conv1': 16, 'layer1.0.conv2': 18, 'layer1.0.conv3': 28, 'layer1.0.downsample.0': 28, 'layer1.1.conv1': 14, 'layer1.1.conv3': 28, 'layer2.0.conv3': 28, 'layer2.0.downsample.0': 28, 'layer2.1.conv3': 28, 'layer3.0.conv3': 96, 'layer3.0.downsample.0': 96, 'layer3.1.conv3': 96, 'layer3.2.conv3': 96, 'layer3.3.conv3': 96, 'layer4.1.conv1': 259, 'layer3.5.conv3': 96, 'layer3.4.conv3': 96, 'layer4.0.conv1': 293, 'layer3.3.conv1': 78, 'layer2.0.conv1': 31, 'layer4.2.conv1': 272, 'layer2.2.conv3': 28, 'layer1.2.conv3': 28, 'layer3.4.conv1': 79, 'layer3.2.conv1': 85, 'layer4.2.conv2': 324, 'layer2.3.conv3': 28, 'layer2.2.conv1': 22, 'layer4.1.conv2': 254, 'layer2.1.conv1': 37, 'layer3.0.conv1': 94, 'layer4.0.conv2': 233, 'layer3.4.conv2': 94, 'layer3.1.conv1': 82, 'layer3.5.conv2': 101, 'layer3.0.conv2': 98, 'layer2.3.conv2': 19, 'layer1.2.conv2': 8, 'layer3.1.conv2': 85, 'layer2.1.conv2': 38, 'layer3.2.conv2': 73, 'layer3.5.conv1': 79, 'layer2.3.conv1': 25, 'layer3.3.conv2': 62, 'layer2.2.conv2': 27, 'layer1.1.conv2': 10, 'layer2.0.conv2': 29, 'layer1.2.conv1': 8}
convolution remaining after pruning {'conv1': 48, 'layer1.0.conv1': 48, 'layer1.0.conv2': 46, 'layer1.0.conv3': 228, 'layer1.0.downsample.0': 228, 'layer1.1.conv1': 50, 'layer1.1.conv3': 228, 'layer2.0.conv3': 484, 'layer2.0.downsample.0': 484, 'layer2.1.conv3': 484, 'layer3.0.conv3': 928, 'layer3.0.downsample.0': 928, 'layer3.1.conv3': 928, 'layer3.2.conv3': 928, 'layer3.3.conv3': 928, 'layer4.1.conv1': 253, 'layer3.5.conv3': 928, 'layer3.4.conv3': 928, 'layer4.0.conv1': 219, 'layer3.3.conv1': 178, 'layer2.0.conv1': 97, 'layer4.2.conv1': 240, 'layer2.2.conv3': 484, 'layer1.2.conv3': 228, 'layer3.4.conv1': 177, 'layer3.2.conv1': 171, 'layer4.2.conv2': 188, 'layer2.3.conv3': 484, 'layer2.2.conv1': 106, 'layer4.1.conv2': 258, 'layer2.1.conv1': 91, 'layer3.0.conv1': 162, 'layer4.0.conv2': 279, 'layer3.4.conv2': 162, 'layer3.1.conv1': 174, 'layer3.5.conv2': 155, 'layer3.0.conv2': 158, 'layer2.3.conv2': 109, 'layer1.2.conv2': 56, 'layer3.1.conv2': 171, 'layer2.1.conv2': 90, 'layer3.2.conv2': 183, 'layer3.5.conv1': 177, 'layer2.3.conv1': 103, 'layer3.3.conv2': 194, 'layer2.2.conv2': 101, 'layer1.1.conv2': 54, 'layer2.0.conv2': 99, 'layer1.2.conv1': 56}
Pruning filters.. 
Filters pruned 21.1618031358885%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 77.61 - Val acc: 74.29 - Train loss: 0.6561 - Val loss: 0.7389 - Training time: 168.58s
end number of flops: 2428488448.0 	number of params: 11074601.0
diff number of flops: 0.41320644471364637 	diff number of params: 0.5293116584203632
Final Test:
	Score: 74.39
***  VGG16-degrad
number of flops: 17315688448.0 	number of params: 138357536.0
Epoch 0 - Train acc: 99.28 - Val acc: 98.48 - Train loss: 0.0252 - Val loss: 0.0539 - Training time: 327.65s
end number of flops: 17315688448.0 	number of params: 138357536.0
diff number of flops: 0.0 	diff number of params: 0.0
Final Test:
	Score: 92.73
***  VGG16-degrad
number of flops: 17315688448.0 	number of params: 138357536.0
1 iterations to reduce 5.00% filters
Perform pruning iteration: 0
junction pruning size: {}
Layers that will be pruned {'features.21': 26, 'features.24': 40, 'features.19': 19, 'features.10': 11, 'features.17': 28, 'features.7': 2, 'features.5': 5, 'features.14': 7, 'features.26': 34, 'features.12': 12, 'features.0': 1}
convolution remaining after pruning {'features.21': 486, 'features.24': 472, 'features.19': 493, 'features.10': 245, 'features.17': 484, 'features.7': 126, 'features.5': 123, 'features.14': 249, 'features.26': 478, 'features.12': 244, 'features.0': 63}
Pruning filters.. 
Filters pruned 4.983836206896552%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 99.14 - Val acc: 98.24 - Train loss: 0.0297 - Val loss: 0.0618 - Training time: 331.29s
end number of flops: 16126600192.0 	number of params: 136944096.0
diff number of flops: 0.06867115099528961 	diff number of params: 0.01021585119873774
Final Test:
	Score: 92.54
***  VGG16-degrad
number of flops: 17315688448.0 	number of params: 138357536.0
1 iterations to reduce 10.00% filters
Perform pruning iteration: 0
junction pruning size: {}
Layers that will be pruned {'features.7': 11, 'features.26': 76, 'features.19': 51, 'features.24': 72, 'features.17': 46, 'features.21': 52, 'features.14': 12, 'features.10': 21, 'features.12': 16, 'features.5': 7, 'features.2': 5, 'features.0': 2}
convolution remaining after pruning {'features.7': 117, 'features.26': 436, 'features.19': 461, 'features.24': 440, 'features.17': 466, 'features.21': 460, 'features.14': 244, 'features.10': 235, 'features.12': 240, 'features.5': 121, 'features.2': 59, 'features.0': 62}
Pruning filters.. 
Filters pruned 9.994612068965518%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 97.33 - Val acc: 95.51 - Train loss: 0.0789 - Val loss: 0.1362 - Training time: 308.26s
end number of flops: 14778759168.0 	number of params: 135577632.0
diff number of flops: 0.1465104484651906 	diff number of params: 0.02009217625847283
Final Test:
	Score: 90.38000000000001
***  VGG16-degrad
number of flops: 17315688448.0 	number of params: 138357536.0
1 iterations to reduce 15.00% filters
Perform pruning iteration: 0
junction pruning size: {}
Layers that will be pruned {'features.24': 79, 'features.12': 37, 'features.0': 8, 'features.26': 100, 'features.10': 34, 'features.19': 84, 'features.21': 70, 'features.17': 89, 'features.14': 27, 'features.5': 9, 'features.7': 12, 'features.2': 7}
convolution remaining after pruning {'features.24': 433, 'features.12': 219, 'features.0': 56, 'features.26': 412, 'features.10': 222, 'features.19': 428, 'features.21': 442, 'features.17': 423, 'features.14': 229, 'features.5': 119, 'features.7': 116, 'features.2': 57}
Pruning filters.. 
Filters pruned 14.97844827586207%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 98.56 - Val acc: 97.21 - Train loss: 0.0475 - Val loss: 0.0927 - Training time: 294.15s
end number of flops: 13235515392.0 	number of params: 134413056.0
diff number of flops: 0.23563446918399997 	diff number of params: 0.028509325288938365
Final Test:
	Score: 92.13
***  VGG16-degrad
number of flops: 17315688448.0 	number of params: 138357536.0
1 iterations to reduce 20.00% filters
Perform pruning iteration: 0
junction pruning size: {}
Layers that will be pruned {'features.21': 117, 'features.17': 94, 'features.26': 153, 'features.19': 98, 'features.10': 41, 'features.24': 140, 'features.12': 34, 'features.14': 33, 'features.0': 4, 'features.5': 11, 'features.7': 9, 'features.2': 8}
convolution remaining after pruning {'features.21': 395, 'features.17': 418, 'features.26': 359, 'features.19': 414, 'features.10': 215, 'features.24': 372, 'features.12': 222, 'features.14': 223, 'features.0': 60, 'features.5': 117, 'features.7': 119, 'features.2': 56}
Pruning filters.. 
Filters pruned 19.989224137931036%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 98.41 - Val acc: 96.67 - Train loss: 0.0524 - Val loss: 0.0992 - Training time: 284.79s
end number of flops: 12737929216.0 	number of params: 133014688.0
diff number of flops: 0.26437061660859007 	diff number of params: 0.03861624132999882
Final Test:
	Score: 92.12
***  VGG16-degrad
number of flops: 17315688448.0 	number of params: 138357536.0
1 iterations to reduce 25.00% filters
Perform pruning iteration: 0
junction pruning size: {}
Layers that will be pruned {'features.17': 136, 'features.19': 122, 'features.24': 165, 'features.26': 169, 'features.7': 22, 'features.21': 152, 'features.12': 47, 'features.0': 6, 'features.14': 50, 'features.10': 39, 'features.2': 7, 'features.5': 13}
convolution remaining after pruning {'features.17': 376, 'features.19': 390, 'features.24': 347, 'features.26': 343, 'features.7': 106, 'features.21': 360, 'features.12': 209, 'features.0': 58, 'features.14': 206, 'features.10': 217, 'features.2': 57, 'features.5': 115}
Pruning filters.. 
Filters pruned 25.0%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 97.19 - Val acc: 95.14 - Train loss: 0.0873 - Val loss: 0.1514 - Training time: 269.16s
end number of flops: 11532784640.0 	number of params: 131905336.0
diff number of flops: 0.3339690376947119 	diff number of params: 0.046634250555025784
Final Test:
	Score: 90.99000000000001
***  VGG16-degrad
number of flops: 17315688448.0 	number of params: 138357536.0
1 iterations to reduce 30.00% filters
Perform pruning iteration: 0
junction pruning size: {}
Layers that will be pruned {'features.24': 197, 'features.17': 156, 'features.26': 223, 'features.10': 53, 'features.21': 173, 'features.7': 25, 'features.14': 48, 'features.19': 160, 'features.2': 9, 'features.0': 11, 'features.12': 39, 'features.5': 19}
convolution remaining after pruning {'features.24': 315, 'features.17': 356, 'features.26': 289, 'features.10': 203, 'features.21': 339, 'features.7': 103, 'features.14': 208, 'features.19': 352, 'features.2': 55, 'features.0': 53, 'features.12': 217, 'features.5': 109}
Pruning filters.. 
Filters pruned 29.98383620689655%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 95.27 - Val acc: 92.28 - Train loss: 0.1359 - Val loss: 0.2255 - Training time: 248.46s
end number of flops: 10592495616.0 	number of params: 130799776.0
diff number of flops: 0.38827176015496767 	diff number of params: 0.05462485252700655
Final Test:
	Score: 89.4
***  VGG16-degrad
number of flops: 17315688448.0 	number of params: 138357536.0
1 iterations to reduce 35.00% filters
Perform pruning iteration: 0
junction pruning size: {}
Layers that will be pruned {'features.24': 196, 'features.21': 198, 'features.17': 192, 'features.19': 200, 'features.26': 248, 'features.10': 61, 'features.12': 63, 'features.14': 69, 'features.2': 11, 'features.5': 23, 'features.7': 23, 'features.0': 15}
convolution remaining after pruning {'features.24': 316, 'features.21': 314, 'features.17': 320, 'features.19': 312, 'features.26': 264, 'features.10': 195, 'features.12': 193, 'features.14': 187, 'features.2': 53, 'features.5': 105, 'features.7': 105, 'features.0': 49}
Pruning filters.. 
Filters pruned 34.994612068965516%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 95.57 - Val acc: 93.00 - Train loss: 0.1346 - Val loss: 0.2078 - Training time: 235.15s
end number of flops: 9315134464.0 	number of params: 129846776.0
diff number of flops: 0.46204076771340163 	diff number of params: 0.06151280404415412
Final Test:
	Score: 89.89
***  VGG16-degrad
number of flops: 17315688448.0 	number of params: 138357536.0
1 iterations to reduce 40.00% filters
Perform pruning iteration: 0
junction pruning size: {}
Layers that will be pruned {'features.21': 240, 'features.26': 288, 'features.24': 234, 'features.17': 200, 'features.19': 205, 'features.10': 57, 'features.12': 80, 'features.7': 32, 'features.5': 34, 'features.14': 83, 'features.0': 14, 'features.2': 17}
convolution remaining after pruning {'features.21': 272, 'features.26': 224, 'features.24': 278, 'features.17': 312, 'features.19': 307, 'features.10': 199, 'features.12': 176, 'features.7': 96, 'features.5': 94, 'features.14': 173, 'features.0': 50, 'features.2': 47}
Pruning filters.. 
Filters pruned 39.97844827586207%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 92.75 - Val acc: 90.36 - Train loss: 0.2073 - Val loss: 0.2878 - Training time: 219.55s
end number of flops: 8176048640.0 	number of params: 128922816.0
diff number of flops: 0.5278242234172127 	diff number of params: 0.06819086457278337
Final Test:
	Score: 87.66000000000001
***  VGG16-degrad
number of flops: 17315688448.0 	number of params: 138357536.0
1 iterations to reduce 45.00% filters
Perform pruning iteration: 0
junction pruning size: {}
Layers that will be pruned {'features.24': 277, 'features.21': 266, 'features.2': 26, 'features.17': 237, 'features.26': 318, 'features.12': 82, 'features.10': 85, 'features.7': 33, 'features.19': 217, 'features.14': 74, 'features.0': 18, 'features.5': 37}
convolution remaining after pruning {'features.24': 235, 'features.21': 246, 'features.2': 38, 'features.17': 275, 'features.26': 194, 'features.12': 174, 'features.10': 171, 'features.7': 95, 'features.19': 295, 'features.14': 182, 'features.0': 46, 'features.5': 91}
Pruning filters.. 
Filters pruned 44.98922413793103%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 89.52 - Val acc: 87.69 - Train loss: 0.3028 - Val loss: 0.3522 - Training time: 207.11s
end number of flops: 7153137152.0 	number of params: 128128568.0
diff number of flops: 0.5868984837951272 	diff number of params: 0.0739314120193641
Final Test:
	Score: 85.67
***  VGG16-degrad
number of flops: 17315688448.0 	number of params: 138357536.0
1 iterations to reduce 50.00% filters
Perform pruning iteration: 0
junction pruning size: {}
Layers that will be pruned {'features.21': 295, 'features.24': 316, 'features.26': 306, 'features.19': 271, 'features.14': 91, 'features.10': 90, 'features.5': 26, 'features.7': 36, 'features.0': 15, 'features.17': 292, 'features.12': 105, 'features.2': 13}
convolution remaining after pruning {'features.21': 217, 'features.24': 196, 'features.26': 206, 'features.19': 241, 'features.14': 165, 'features.10': 166, 'features.5': 102, 'features.7': 92, 'features.0': 49, 'features.17': 220, 'features.12': 151, 'features.2': 51}
Pruning filters.. 
Filters pruned 50.0%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 90.05 - Val acc: 87.96 - Train loss: 0.2950 - Val loss: 0.3517 - Training time: 201.86s
end number of flops: 6942351360.0 	number of params: 127357560.0
diff number of flops: 0.5990715944764035 	diff number of params: 0.0795039888539212
Final Test:
	Score: 85.94000000000001
***  DenseNet121-degrad
number of flops: 2914598912.0 	number of params: 7978856.0
Epoch 0 - Train acc: 99.68 - Val acc: 99.04 - Train loss: 0.0109 - Val loss: 0.0318 - Training time: 265.17s
end number of flops: 2914598912.0 	number of params: 7978856.0
diff number of flops: 0.0 	diff number of params: 0.0
Final Test:
	Score: 95.81
***  DenseNet121-degrad
number of flops: 2914598912.0 	number of params: 7978856.0
1 iterations to reduce 5.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 0}
Layers that will be pruned {'features.denseblock4.denselayer7.conv1': 8, 'features.denseblock4.denselayer9.conv1': 9, 'features.denseblock4.denselayer12.conv1': 11, 'features.denseblock4.denselayer13.conv1': 19, 'features.denseblock4.denselayer14.conv1': 16, 'features.denseblock4.denselayer15.conv1': 25, 'features.denseblock1.denselayer1.conv1': 13, 'features.denseblock1.denselayer2.conv1': 5, 'features.denseblock4.denselayer8.conv1': 8, 'features.denseblock3.denselayer12.conv1': 6, 'features.denseblock4.denselayer1.conv1': 6, 'features.denseblock4.denselayer10.conv1': 11, 'features.denseblock3.denselayer5.conv1': 4, 'features.denseblock2.denselayer11.conv1': 5, 'features.denseblock1.denselayer6.conv1': 13, 'features.denseblock4.denselayer16.conv1': 17, 'features.denseblock2.denselayer2.conv1': 6, 'features.denseblock2.denselayer4.conv1': 6, 'features.denseblock3.denselayer14.conv1': 6, 'features.denseblock3.denselayer6.conv1': 2, 'features.denseblock4.denselayer6.conv1': 9, 'features.denseblock3.denselayer16.conv1': 5, 'features.denseblock3.denselayer15.conv1': 6, 'features.denseblock3.denselayer2.conv1': 8, 'features.denseblock3.denselayer22.conv1': 5, 'features.denseblock4.denselayer11.conv1': 10, 'features.denseblock3.denselayer17.conv1': 7, 'features.denseblock3.denselayer19.conv1': 6, 'features.denseblock3.denselayer9.conv1': 5, 'features.denseblock2.denselayer8.conv1': 5, 'features.denseblock3.denselayer4.conv1': 6, 'features.denseblock4.denselayer4.conv1': 5, 'features.denseblock1.denselayer5.conv1': 6, 'features.denseblock2.denselayer1.conv1': 10, 'features.denseblock3.denselayer8.conv1': 8, 'features.denseblock4.denselayer5.conv1': 3, 'features.denseblock3.denselayer20.conv1': 3, 'features.denseblock3.denselayer3.conv1': 3, 'features.denseblock3.denselayer10.conv1': 6, 'features.denseblock3.denselayer18.conv1': 3, 'features.denseblock2.denselayer5.conv1': 8, 'features.denseblock3.denselayer1.conv1': 7, 'features.denseblock3.denselayer24.conv1': 5, 'features.denseblock2.denselayer12.conv1': 4, 'features.denseblock2.denselayer6.conv1': 5, 'features.denseblock1.denselayer4.conv1': 5, 'features.denseblock2.denselayer7.conv1': 4, 'features.denseblock4.denselayer3.conv1': 3, 'features.denseblock4.denselayer2.conv1': 5, 'features.denseblock1.denselayer3.conv1': 6, 'features.denseblock3.denselayer23.conv1': 5, 'features.denseblock2.denselayer10.conv1': 3, 'features.denseblock3.denselayer7.conv1': 4, 'features.denseblock2.denselayer3.conv1': 6, 'features.denseblock3.denselayer21.conv1': 2, 'features.denseblock2.denselayer9.conv1': 3, 'features.denseblock3.denselayer13.conv1': 2, 'features.denseblock3.denselayer11.conv1': 1}
convolution remaining after pruning {'features.denseblock4.denselayer7.conv1': 120, 'features.denseblock4.denselayer9.conv1': 119, 'features.denseblock4.denselayer12.conv1': 117, 'features.denseblock4.denselayer13.conv1': 109, 'features.denseblock4.denselayer14.conv1': 112, 'features.denseblock4.denselayer15.conv1': 103, 'features.denseblock1.denselayer1.conv1': 115, 'features.denseblock1.denselayer2.conv1': 123, 'features.denseblock4.denselayer8.conv1': 120, 'features.denseblock3.denselayer12.conv1': 122, 'features.denseblock4.denselayer1.conv1': 122, 'features.denseblock4.denselayer10.conv1': 117, 'features.denseblock3.denselayer5.conv1': 124, 'features.denseblock2.denselayer11.conv1': 123, 'features.denseblock1.denselayer6.conv1': 115, 'features.denseblock4.denselayer16.conv1': 111, 'features.denseblock2.denselayer2.conv1': 122, 'features.denseblock2.denselayer4.conv1': 122, 'features.denseblock3.denselayer14.conv1': 122, 'features.denseblock3.denselayer6.conv1': 126, 'features.denseblock4.denselayer6.conv1': 119, 'features.denseblock3.denselayer16.conv1': 123, 'features.denseblock3.denselayer15.conv1': 122, 'features.denseblock3.denselayer2.conv1': 120, 'features.denseblock3.denselayer22.conv1': 123, 'features.denseblock4.denselayer11.conv1': 118, 'features.denseblock3.denselayer17.conv1': 121, 'features.denseblock3.denselayer19.conv1': 122, 'features.denseblock3.denselayer9.conv1': 123, 'features.denseblock2.denselayer8.conv1': 123, 'features.denseblock3.denselayer4.conv1': 122, 'features.denseblock4.denselayer4.conv1': 123, 'features.denseblock1.denselayer5.conv1': 122, 'features.denseblock2.denselayer1.conv1': 118, 'features.denseblock3.denselayer8.conv1': 120, 'features.denseblock4.denselayer5.conv1': 125, 'features.denseblock3.denselayer20.conv1': 125, 'features.denseblock3.denselayer3.conv1': 125, 'features.denseblock3.denselayer10.conv1': 122, 'features.denseblock3.denselayer18.conv1': 125, 'features.denseblock2.denselayer5.conv1': 120, 'features.denseblock3.denselayer1.conv1': 121, 'features.denseblock3.denselayer24.conv1': 123, 'features.denseblock2.denselayer12.conv1': 124, 'features.denseblock2.denselayer6.conv1': 123, 'features.denseblock1.denselayer4.conv1': 123, 'features.denseblock2.denselayer7.conv1': 124, 'features.denseblock4.denselayer3.conv1': 125, 'features.denseblock4.denselayer2.conv1': 123, 'features.denseblock1.denselayer3.conv1': 122, 'features.denseblock3.denselayer23.conv1': 123, 'features.denseblock2.denselayer10.conv1': 125, 'features.denseblock3.denselayer7.conv1': 124, 'features.denseblock2.denselayer3.conv1': 122, 'features.denseblock3.denselayer21.conv1': 126, 'features.denseblock2.denselayer9.conv1': 125, 'features.denseblock3.denselayer13.conv1': 126, 'features.denseblock3.denselayer11.conv1': 127}
Pruning filters.. 
Filters pruned 4.264322916666667%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 97.22 - Val acc: 94.45 - Train loss: 0.0845 - Val loss: 0.1583 - Training time: 263.34s
end number of flops: 2790444800.0 	number of params: 7633814.0
diff number of flops: 0.04259732325049327 	diff number of params: 0.043244545333315954
Final Test:
	Score: 93.42
***  DenseNet121-degrad
number of flops: 2914598912.0 	number of params: 7978856.0
1 iterations to reduce 10.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 0}
Layers that will be pruned {'features.denseblock4.denselayer7.conv1': 18, 'features.denseblock4.denselayer9.conv1': 18, 'features.denseblock4.denselayer11.conv1': 21, 'features.denseblock4.denselayer13.conv1': 35, 'features.denseblock4.denselayer16.conv1': 43, 'features.denseblock1.denselayer1.conv1': 14, 'features.denseblock1.denselayer2.conv1': 18, 'features.denseblock4.denselayer10.conv1': 24, 'features.denseblock4.denselayer2.conv1': 9, 'features.denseblock2.denselayer10.conv1': 9, 'features.denseblock4.denselayer12.conv1': 31, 'features.denseblock3.denselayer11.conv1': 9, 'features.denseblock3.denselayer6.conv1': 11, 'features.denseblock1.denselayer4.conv1': 10, 'features.denseblock4.denselayer15.conv1': 25, 'features.denseblock3.denselayer4.conv1': 13, 'features.denseblock3.denselayer5.conv1': 16, 'features.denseblock2.denselayer6.conv1': 17, 'features.denseblock3.denselayer16.conv1': 7, 'features.denseblock4.denselayer6.conv1': 17, 'features.denseblock3.denselayer13.conv1': 11, 'features.denseblock2.denselayer8.conv1': 5, 'features.denseblock4.denselayer14.conv1': 30, 'features.denseblock4.denselayer5.conv1': 17, 'features.denseblock4.denselayer8.conv1': 25, 'features.denseblock2.denselayer5.conv1': 4, 'features.denseblock3.denselayer7.conv1': 14, 'features.denseblock3.denselayer22.conv1': 14, 'features.denseblock3.denselayer19.conv1': 8, 'features.denseblock2.denselayer1.conv1': 28, 'features.denseblock3.denselayer24.conv1': 10, 'features.denseblock3.denselayer12.conv1': 15, 'features.denseblock3.denselayer18.conv1': 10, 'features.denseblock3.denselayer23.conv1': 5, 'features.denseblock4.denselayer4.conv1': 13, 'features.denseblock3.denselayer20.conv1': 13, 'features.denseblock2.denselayer11.conv1': 14, 'features.denseblock2.denselayer12.conv1': 9, 'features.denseblock2.denselayer3.conv1': 7, 'features.denseblock1.denselayer6.conv1': 12, 'features.denseblock2.denselayer9.conv1': 10, 'features.denseblock2.denselayer4.conv1': 11, 'features.denseblock3.denselayer10.conv1': 6, 'features.denseblock3.denselayer1.conv1': 8, 'features.denseblock3.denselayer21.conv1': 8, 'features.denseblock3.denselayer8.conv1': 6, 'features.denseblock4.denselayer1.conv1': 14, 'features.denseblock1.denselayer3.conv1': 12, 'features.denseblock4.denselayer3.conv1': 13, 'features.denseblock3.denselayer17.conv1': 11, 'features.denseblock2.denselayer7.conv1': 7, 'features.denseblock3.denselayer15.conv1': 6, 'features.denseblock3.denselayer2.conv1': 7, 'features.denseblock3.denselayer14.conv1': 6, 'features.denseblock3.denselayer3.conv1': 8, 'features.denseblock2.denselayer2.conv1': 5, 'features.denseblock1.denselayer5.conv1': 13, 'features.denseblock3.denselayer9.conv1': 9}
convolution remaining after pruning {'features.denseblock4.denselayer7.conv1': 110, 'features.denseblock4.denselayer9.conv1': 110, 'features.denseblock4.denselayer11.conv1': 107, 'features.denseblock4.denselayer13.conv1': 93, 'features.denseblock4.denselayer16.conv1': 85, 'features.denseblock1.denselayer1.conv1': 114, 'features.denseblock1.denselayer2.conv1': 110, 'features.denseblock4.denselayer10.conv1': 104, 'features.denseblock4.denselayer2.conv1': 119, 'features.denseblock2.denselayer10.conv1': 119, 'features.denseblock4.denselayer12.conv1': 97, 'features.denseblock3.denselayer11.conv1': 119, 'features.denseblock3.denselayer6.conv1': 117, 'features.denseblock1.denselayer4.conv1': 118, 'features.denseblock4.denselayer15.conv1': 103, 'features.denseblock3.denselayer4.conv1': 115, 'features.denseblock3.denselayer5.conv1': 112, 'features.denseblock2.denselayer6.conv1': 111, 'features.denseblock3.denselayer16.conv1': 121, 'features.denseblock4.denselayer6.conv1': 111, 'features.denseblock3.denselayer13.conv1': 117, 'features.denseblock2.denselayer8.conv1': 123, 'features.denseblock4.denselayer14.conv1': 98, 'features.denseblock4.denselayer5.conv1': 111, 'features.denseblock4.denselayer8.conv1': 103, 'features.denseblock2.denselayer5.conv1': 124, 'features.denseblock3.denselayer7.conv1': 114, 'features.denseblock3.denselayer22.conv1': 114, 'features.denseblock3.denselayer19.conv1': 120, 'features.denseblock2.denselayer1.conv1': 100, 'features.denseblock3.denselayer24.conv1': 118, 'features.denseblock3.denselayer12.conv1': 113, 'features.denseblock3.denselayer18.conv1': 118, 'features.denseblock3.denselayer23.conv1': 123, 'features.denseblock4.denselayer4.conv1': 115, 'features.denseblock3.denselayer20.conv1': 115, 'features.denseblock2.denselayer11.conv1': 114, 'features.denseblock2.denselayer12.conv1': 119, 'features.denseblock2.denselayer3.conv1': 121, 'features.denseblock1.denselayer6.conv1': 116, 'features.denseblock2.denselayer9.conv1': 118, 'features.denseblock2.denselayer4.conv1': 117, 'features.denseblock3.denselayer10.conv1': 122, 'features.denseblock3.denselayer1.conv1': 120, 'features.denseblock3.denselayer21.conv1': 120, 'features.denseblock3.denselayer8.conv1': 122, 'features.denseblock4.denselayer1.conv1': 114, 'features.denseblock1.denselayer3.conv1': 116, 'features.denseblock4.denselayer3.conv1': 115, 'features.denseblock3.denselayer17.conv1': 117, 'features.denseblock2.denselayer7.conv1': 121, 'features.denseblock3.denselayer15.conv1': 122, 'features.denseblock3.denselayer2.conv1': 121, 'features.denseblock3.denselayer14.conv1': 122, 'features.denseblock3.denselayer3.conv1': 120, 'features.denseblock2.denselayer2.conv1': 123, 'features.denseblock1.denselayer5.conv1': 115, 'features.denseblock3.denselayer9.conv1': 119}
Pruning filters.. 
Filters pruned 8.561197916666666%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 97.39 - Val acc: 94.62 - Train loss: 0.0819 - Val loss: 0.1543 - Training time: 259.85s
end number of flops: 2690038272.0 	number of params: 7275934.0
diff number of flops: 0.07704684135969375 	diff number of params: 0.08809809326048747
Final Test:
	Score: 93.5
***  DenseNet121-degrad
number of flops: 2914598912.0 	number of params: 7978856.0
1 iterations to reduce 15.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 0}
Layers that will be pruned {'features.denseblock4.denselayer7.conv1': 22, 'features.denseblock4.denselayer13.conv1': 40, 'features.denseblock4.denselayer15.conv1': 41, 'features.denseblock1.denselayer1.conv1': 29, 'features.denseblock1.denselayer2.conv1': 41, 'features.denseblock3.denselayer19.conv1': 16, 'features.denseblock3.denselayer2.conv1': 13, 'features.denseblock3.denselayer13.conv1': 18, 'features.denseblock3.denselayer17.conv1': 18, 'features.denseblock4.denselayer8.conv1': 26, 'features.denseblock3.denselayer14.conv1': 20, 'features.denseblock2.denselayer4.conv1': 16, 'features.denseblock4.denselayer4.conv1': 26, 'features.denseblock3.denselayer24.conv1': 11, 'features.denseblock2.denselayer6.conv1': 27, 'features.denseblock2.denselayer9.conv1': 21, 'features.denseblock3.denselayer5.conv1': 14, 'features.denseblock4.denselayer10.conv1': 17, 'features.denseblock3.denselayer23.conv1': 24, 'features.denseblock3.denselayer3.conv1': 17, 'features.denseblock3.denselayer6.conv1': 14, 'features.denseblock4.denselayer6.conv1': 30, 'features.denseblock3.denselayer1.conv1': 27, 'features.denseblock4.denselayer3.conv1': 27, 'features.denseblock4.denselayer16.conv1': 32, 'features.denseblock1.denselayer4.conv1': 18, 'features.denseblock4.denselayer12.conv1': 24, 'features.denseblock3.denselayer21.conv1': 11, 'features.denseblock2.denselayer11.conv1': 13, 'features.denseblock2.denselayer5.conv1': 19, 'features.denseblock4.denselayer14.conv1': 29, 'features.denseblock1.denselayer3.conv1': 18, 'features.denseblock2.denselayer1.conv1': 42, 'features.denseblock2.denselayer8.conv1': 14, 'features.denseblock4.denselayer11.conv1': 30, 'features.denseblock3.denselayer16.conv1': 18, 'features.denseblock1.denselayer6.conv1': 24, 'features.denseblock3.denselayer9.conv1': 11, 'features.denseblock3.denselayer12.conv1': 15, 'features.denseblock3.denselayer8.conv1': 18, 'features.denseblock3.denselayer20.conv1': 13, 'features.denseblock2.denselayer3.conv1': 15, 'features.denseblock2.denselayer10.conv1': 15, 'features.denseblock3.denselayer11.conv1': 19, 'features.denseblock3.denselayer7.conv1': 15, 'features.denseblock4.denselayer5.conv1': 20, 'features.denseblock3.denselayer4.conv1': 8, 'features.denseblock4.denselayer2.conv1': 23, 'features.denseblock2.denselayer2.conv1': 23, 'features.denseblock2.denselayer12.conv1': 23, 'features.denseblock4.denselayer1.conv1': 9, 'features.denseblock4.denselayer9.conv1': 20, 'features.denseblock2.denselayer7.conv1': 17, 'features.denseblock3.denselayer10.conv1': 16, 'features.denseblock1.denselayer5.conv1': 12, 'features.denseblock3.denselayer22.conv1': 9, 'features.denseblock3.denselayer18.conv1': 19, 'features.denseblock3.denselayer15.conv1': 19}
convolution remaining after pruning {'features.denseblock4.denselayer7.conv1': 106, 'features.denseblock4.denselayer13.conv1': 88, 'features.denseblock4.denselayer15.conv1': 87, 'features.denseblock1.denselayer1.conv1': 99, 'features.denseblock1.denselayer2.conv1': 87, 'features.denseblock3.denselayer19.conv1': 112, 'features.denseblock3.denselayer2.conv1': 115, 'features.denseblock3.denselayer13.conv1': 110, 'features.denseblock3.denselayer17.conv1': 110, 'features.denseblock4.denselayer8.conv1': 102, 'features.denseblock3.denselayer14.conv1': 108, 'features.denseblock2.denselayer4.conv1': 112, 'features.denseblock4.denselayer4.conv1': 102, 'features.denseblock3.denselayer24.conv1': 117, 'features.denseblock2.denselayer6.conv1': 101, 'features.denseblock2.denselayer9.conv1': 107, 'features.denseblock3.denselayer5.conv1': 114, 'features.denseblock4.denselayer10.conv1': 111, 'features.denseblock3.denselayer23.conv1': 104, 'features.denseblock3.denselayer3.conv1': 111, 'features.denseblock3.denselayer6.conv1': 114, 'features.denseblock4.denselayer6.conv1': 98, 'features.denseblock3.denselayer1.conv1': 101, 'features.denseblock4.denselayer3.conv1': 101, 'features.denseblock4.denselayer16.conv1': 96, 'features.denseblock1.denselayer4.conv1': 110, 'features.denseblock4.denselayer12.conv1': 104, 'features.denseblock3.denselayer21.conv1': 117, 'features.denseblock2.denselayer11.conv1': 115, 'features.denseblock2.denselayer5.conv1': 109, 'features.denseblock4.denselayer14.conv1': 99, 'features.denseblock1.denselayer3.conv1': 110, 'features.denseblock2.denselayer1.conv1': 86, 'features.denseblock2.denselayer8.conv1': 114, 'features.denseblock4.denselayer11.conv1': 98, 'features.denseblock3.denselayer16.conv1': 110, 'features.denseblock1.denselayer6.conv1': 104, 'features.denseblock3.denselayer9.conv1': 117, 'features.denseblock3.denselayer12.conv1': 113, 'features.denseblock3.denselayer8.conv1': 110, 'features.denseblock3.denselayer20.conv1': 115, 'features.denseblock2.denselayer3.conv1': 113, 'features.denseblock2.denselayer10.conv1': 113, 'features.denseblock3.denselayer11.conv1': 109, 'features.denseblock3.denselayer7.conv1': 113, 'features.denseblock4.denselayer5.conv1': 108, 'features.denseblock3.denselayer4.conv1': 120, 'features.denseblock4.denselayer2.conv1': 105, 'features.denseblock2.denselayer2.conv1': 105, 'features.denseblock2.denselayer12.conv1': 105, 'features.denseblock4.denselayer1.conv1': 119, 'features.denseblock4.denselayer9.conv1': 108, 'features.denseblock2.denselayer7.conv1': 111, 'features.denseblock3.denselayer10.conv1': 112, 'features.denseblock1.denselayer5.conv1': 116, 'features.denseblock3.denselayer22.conv1': 119, 'features.denseblock3.denselayer18.conv1': 109, 'features.denseblock3.denselayer15.conv1': 109}
Pruning filters.. 
Filters pruned 12.86892361111111%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 97.07 - Val acc: 94.38 - Train loss: 0.0922 - Val loss: 0.1606 - Training time: 258.34s
end number of flops: 2524339200.0 	number of params: 6984804.0
diff number of flops: 0.13389825625516463 	diff number of params: 0.12458578021711383
Final Test:
	Score: 93.17999999999999
***  DenseNet121-degrad
number of flops: 2914598912.0 	number of params: 7978856.0
1 iterations to reduce 20.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 0}
Layers that will be pruned {'features.denseblock4.denselayer7.conv1': 25, 'features.denseblock4.denselayer13.conv1': 47, 'features.denseblock4.denselayer14.conv1': 39, 'features.denseblock4.denselayer15.conv1': 40, 'features.denseblock4.denselayer16.conv1': 41, 'features.denseblock1.denselayer1.conv1': 45, 'features.denseblock1.denselayer2.conv1': 32, 'features.denseblock3.denselayer16.conv1': 21, 'features.denseblock3.denselayer19.conv1': 22, 'features.denseblock4.denselayer12.conv1': 37, 'features.denseblock3.denselayer7.conv1': 21, 'features.denseblock4.denselayer9.conv1': 37, 'features.denseblock2.denselayer3.conv1': 22, 'features.denseblock4.denselayer11.conv1': 42, 'features.denseblock3.denselayer22.conv1': 24, 'features.denseblock4.denselayer10.conv1': 35, 'features.denseblock2.denselayer7.conv1': 15, 'features.denseblock2.denselayer1.conv1': 58, 'features.denseblock3.denselayer2.conv1': 31, 'features.denseblock1.denselayer4.conv1': 27, 'features.denseblock2.denselayer5.conv1': 21, 'features.denseblock3.denselayer21.conv1': 17, 'features.denseblock4.denselayer6.conv1': 35, 'features.denseblock3.denselayer10.conv1': 29, 'features.denseblock3.denselayer8.conv1': 24, 'features.denseblock3.denselayer3.conv1': 21, 'features.denseblock3.denselayer6.conv1': 30, 'features.denseblock4.denselayer4.conv1': 38, 'features.denseblock3.denselayer12.conv1': 22, 'features.denseblock3.denselayer23.conv1': 24, 'features.denseblock2.denselayer4.conv1': 18, 'features.denseblock3.denselayer24.conv1': 23, 'features.denseblock3.denselayer14.conv1': 26, 'features.denseblock4.denselayer8.conv1': 40, 'features.denseblock2.denselayer8.conv1': 24, 'features.denseblock4.denselayer5.conv1': 23, 'features.denseblock3.denselayer20.conv1': 27, 'features.denseblock2.denselayer9.conv1': 14, 'features.denseblock1.denselayer3.conv1': 32, 'features.denseblock3.denselayer5.conv1': 21, 'features.denseblock3.denselayer13.conv1': 14, 'features.denseblock3.denselayer18.conv1': 25, 'features.denseblock1.denselayer6.conv1': 31, 'features.denseblock4.denselayer2.conv1': 19, 'features.denseblock2.denselayer12.conv1': 19, 'features.denseblock1.denselayer5.conv1': 24, 'features.denseblock3.denselayer4.conv1': 23, 'features.denseblock3.denselayer9.conv1': 22, 'features.denseblock2.denselayer6.conv1': 24, 'features.denseblock2.denselayer2.conv1': 20, 'features.denseblock2.denselayer10.conv1': 25, 'features.denseblock3.denselayer11.conv1': 21, 'features.denseblock4.denselayer3.conv1': 28, 'features.denseblock3.denselayer1.conv1': 20, 'features.denseblock3.denselayer15.conv1': 18, 'features.denseblock2.denselayer11.conv1': 20, 'features.denseblock3.denselayer17.conv1': 16, 'features.denseblock4.denselayer1.conv1': 34}
convolution remaining after pruning {'features.denseblock4.denselayer7.conv1': 103, 'features.denseblock4.denselayer13.conv1': 81, 'features.denseblock4.denselayer14.conv1': 89, 'features.denseblock4.denselayer15.conv1': 88, 'features.denseblock4.denselayer16.conv1': 87, 'features.denseblock1.denselayer1.conv1': 83, 'features.denseblock1.denselayer2.conv1': 96, 'features.denseblock3.denselayer16.conv1': 107, 'features.denseblock3.denselayer19.conv1': 106, 'features.denseblock4.denselayer12.conv1': 91, 'features.denseblock3.denselayer7.conv1': 107, 'features.denseblock4.denselayer9.conv1': 91, 'features.denseblock2.denselayer3.conv1': 106, 'features.denseblock4.denselayer11.conv1': 86, 'features.denseblock3.denselayer22.conv1': 104, 'features.denseblock4.denselayer10.conv1': 93, 'features.denseblock2.denselayer7.conv1': 113, 'features.denseblock2.denselayer1.conv1': 70, 'features.denseblock3.denselayer2.conv1': 97, 'features.denseblock1.denselayer4.conv1': 101, 'features.denseblock2.denselayer5.conv1': 107, 'features.denseblock3.denselayer21.conv1': 111, 'features.denseblock4.denselayer6.conv1': 93, 'features.denseblock3.denselayer10.conv1': 99, 'features.denseblock3.denselayer8.conv1': 104, 'features.denseblock3.denselayer3.conv1': 107, 'features.denseblock3.denselayer6.conv1': 98, 'features.denseblock4.denselayer4.conv1': 90, 'features.denseblock3.denselayer12.conv1': 106, 'features.denseblock3.denselayer23.conv1': 104, 'features.denseblock2.denselayer4.conv1': 110, 'features.denseblock3.denselayer24.conv1': 105, 'features.denseblock3.denselayer14.conv1': 102, 'features.denseblock4.denselayer8.conv1': 88, 'features.denseblock2.denselayer8.conv1': 104, 'features.denseblock4.denselayer5.conv1': 105, 'features.denseblock3.denselayer20.conv1': 101, 'features.denseblock2.denselayer9.conv1': 114, 'features.denseblock1.denselayer3.conv1': 96, 'features.denseblock3.denselayer5.conv1': 107, 'features.denseblock3.denselayer13.conv1': 114, 'features.denseblock3.denselayer18.conv1': 103, 'features.denseblock1.denselayer6.conv1': 97, 'features.denseblock4.denselayer2.conv1': 109, 'features.denseblock2.denselayer12.conv1': 109, 'features.denseblock1.denselayer5.conv1': 104, 'features.denseblock3.denselayer4.conv1': 105, 'features.denseblock3.denselayer9.conv1': 106, 'features.denseblock2.denselayer6.conv1': 104, 'features.denseblock2.denselayer2.conv1': 108, 'features.denseblock2.denselayer10.conv1': 103, 'features.denseblock3.denselayer11.conv1': 107, 'features.denseblock4.denselayer3.conv1': 100, 'features.denseblock3.denselayer1.conv1': 108, 'features.denseblock3.denselayer15.conv1': 110, 'features.denseblock2.denselayer11.conv1': 108, 'features.denseblock3.denselayer17.conv1': 112, 'features.denseblock4.denselayer1.conv1': 94}
Pruning filters.. 
Filters pruned 17.06814236111111%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 95.91 - Val acc: 92.61 - Train loss: 0.1216 - Val loss: 0.2069 - Training time: 261.26s
end number of flops: 2406151424.0 	number of params: 6650846.0
diff number of flops: 0.17444852734508948 	diff number of params: 0.16644115396994255
Final Test:
	Score: 92.06
***  DenseNet121-degrad
number of flops: 2914598912.0 	number of params: 7978856.0
1 iterations to reduce 25.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 0}
Layers that will be pruned {'features.denseblock4.denselayer7.conv1': 37, 'features.denseblock4.denselayer11.conv1': 39, 'features.denseblock4.denselayer13.conv1': 67, 'features.denseblock4.denselayer14.conv1': 37, 'features.denseblock1.denselayer1.conv1': 35, 'features.denseblock1.denselayer2.conv1': 43, 'features.denseblock4.denselayer15.conv1': 60, 'features.denseblock3.denselayer23.conv1': 23, 'features.denseblock1.denselayer5.conv1': 36, 'features.denseblock4.denselayer16.conv1': 71, 'features.denseblock2.denselayer12.conv1': 24, 'features.denseblock4.denselayer5.conv1': 39, 'features.denseblock1.denselayer4.conv1': 41, 'features.denseblock2.denselayer1.conv1': 53, 'features.denseblock2.denselayer8.conv1': 31, 'features.denseblock4.denselayer12.conv1': 58, 'features.denseblock3.denselayer19.conv1': 30, 'features.denseblock2.denselayer7.conv1': 24, 'features.denseblock3.denselayer14.conv1': 30, 'features.denseblock4.denselayer2.conv1': 34, 'features.denseblock2.denselayer9.conv1': 20, 'features.denseblock3.denselayer12.conv1': 34, 'features.denseblock4.denselayer8.conv1': 43, 'features.denseblock3.denselayer9.conv1': 31, 'features.denseblock4.denselayer4.conv1': 39, 'features.denseblock2.denselayer2.conv1': 24, 'features.denseblock2.denselayer3.conv1': 27, 'features.denseblock1.denselayer3.conv1': 30, 'features.denseblock3.denselayer5.conv1': 24, 'features.denseblock3.denselayer1.conv1': 41, 'features.denseblock2.denselayer4.conv1': 28, 'features.denseblock2.denselayer11.conv1': 26, 'features.denseblock3.denselayer22.conv1': 24, 'features.denseblock2.denselayer10.conv1': 25, 'features.denseblock3.denselayer6.conv1': 33, 'features.denseblock4.denselayer10.conv1': 40, 'features.denseblock4.denselayer9.conv1': 40, 'features.denseblock4.denselayer6.conv1': 43, 'features.denseblock2.denselayer5.conv1': 38, 'features.denseblock3.denselayer20.conv1': 25, 'features.denseblock3.denselayer3.conv1': 26, 'features.denseblock3.denselayer8.conv1': 27, 'features.denseblock3.denselayer15.conv1': 30, 'features.denseblock4.denselayer3.conv1': 32, 'features.denseblock2.denselayer6.conv1': 28, 'features.denseblock3.denselayer11.conv1': 25, 'features.denseblock4.denselayer1.conv1': 32, 'features.denseblock3.denselayer24.conv1': 26, 'features.denseblock1.denselayer6.conv1': 42, 'features.denseblock3.denselayer2.conv1': 32, 'features.denseblock3.denselayer13.conv1': 24, 'features.denseblock3.denselayer17.conv1': 29, 'features.denseblock3.denselayer16.conv1': 28, 'features.denseblock3.denselayer10.conv1': 33, 'features.denseblock3.denselayer21.conv1': 25, 'features.denseblock3.denselayer18.conv1': 31, 'features.denseblock3.denselayer7.conv1': 25, 'features.denseblock3.denselayer4.conv1': 38}
convolution remaining after pruning {'features.denseblock4.denselayer7.conv1': 91, 'features.denseblock4.denselayer11.conv1': 89, 'features.denseblock4.denselayer13.conv1': 61, 'features.denseblock4.denselayer14.conv1': 91, 'features.denseblock1.denselayer1.conv1': 93, 'features.denseblock1.denselayer2.conv1': 85, 'features.denseblock4.denselayer15.conv1': 68, 'features.denseblock3.denselayer23.conv1': 105, 'features.denseblock1.denselayer5.conv1': 92, 'features.denseblock4.denselayer16.conv1': 57, 'features.denseblock2.denselayer12.conv1': 104, 'features.denseblock4.denselayer5.conv1': 89, 'features.denseblock1.denselayer4.conv1': 87, 'features.denseblock2.denselayer1.conv1': 75, 'features.denseblock2.denselayer8.conv1': 97, 'features.denseblock4.denselayer12.conv1': 70, 'features.denseblock3.denselayer19.conv1': 98, 'features.denseblock2.denselayer7.conv1': 104, 'features.denseblock3.denselayer14.conv1': 98, 'features.denseblock4.denselayer2.conv1': 94, 'features.denseblock2.denselayer9.conv1': 108, 'features.denseblock3.denselayer12.conv1': 94, 'features.denseblock4.denselayer8.conv1': 85, 'features.denseblock3.denselayer9.conv1': 97, 'features.denseblock4.denselayer4.conv1': 89, 'features.denseblock2.denselayer2.conv1': 104, 'features.denseblock2.denselayer3.conv1': 101, 'features.denseblock1.denselayer3.conv1': 98, 'features.denseblock3.denselayer5.conv1': 104, 'features.denseblock3.denselayer1.conv1': 87, 'features.denseblock2.denselayer4.conv1': 100, 'features.denseblock2.denselayer11.conv1': 102, 'features.denseblock3.denselayer22.conv1': 104, 'features.denseblock2.denselayer10.conv1': 103, 'features.denseblock3.denselayer6.conv1': 95, 'features.denseblock4.denselayer10.conv1': 88, 'features.denseblock4.denselayer9.conv1': 88, 'features.denseblock4.denselayer6.conv1': 85, 'features.denseblock2.denselayer5.conv1': 90, 'features.denseblock3.denselayer20.conv1': 103, 'features.denseblock3.denselayer3.conv1': 102, 'features.denseblock3.denselayer8.conv1': 101, 'features.denseblock3.denselayer15.conv1': 98, 'features.denseblock4.denselayer3.conv1': 96, 'features.denseblock2.denselayer6.conv1': 100, 'features.denseblock3.denselayer11.conv1': 103, 'features.denseblock4.denselayer1.conv1': 96, 'features.denseblock3.denselayer24.conv1': 102, 'features.denseblock1.denselayer6.conv1': 86, 'features.denseblock3.denselayer2.conv1': 96, 'features.denseblock3.denselayer13.conv1': 104, 'features.denseblock3.denselayer17.conv1': 99, 'features.denseblock3.denselayer16.conv1': 100, 'features.denseblock3.denselayer10.conv1': 95, 'features.denseblock3.denselayer21.conv1': 103, 'features.denseblock3.denselayer18.conv1': 97, 'features.denseblock3.denselayer7.conv1': 103, 'features.denseblock3.denselayer4.conv1': 90}
Pruning filters.. 
Filters pruned 21.484375%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 95.89 - Val acc: 92.84 - Train loss: 0.1258 - Val loss: 0.2100 - Training time: 263.65s
end number of flops: 2284340736.0 	number of params: 6294768.0
diff number of flops: 0.21624182092606228 	diff number of params: 0.21106885498372197
Final Test:
	Score: 91.79
***  DenseNet121-degrad
number of flops: 2914598912.0 	number of params: 7978856.0
1 iterations to reduce 30.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 0}
Layers that will be pruned {'features.denseblock4.denselayer7.conv1': 32, 'features.denseblock4.denselayer8.conv1': 48, 'features.denseblock4.denselayer9.conv1': 48, 'features.denseblock4.denselayer10.conv1': 46, 'features.denseblock4.denselayer11.conv1': 60, 'features.denseblock4.denselayer12.conv1': 64, 'features.denseblock4.denselayer13.conv1': 67, 'features.denseblock4.denselayer14.conv1': 69, 'features.denseblock4.denselayer15.conv1': 69, 'features.denseblock4.denselayer16.conv1': 70, 'features.denseblock1.denselayer1.conv1': 41, 'features.denseblock1.denselayer2.conv1': 42, 'features.denseblock3.denselayer10.conv1': 32, 'features.denseblock2.denselayer1.conv1': 59, 'features.denseblock4.denselayer6.conv1': 44, 'features.denseblock3.denselayer3.conv1': 46, 'features.denseblock3.denselayer13.conv1': 28, 'features.denseblock2.denselayer9.conv1': 38, 'features.denseblock1.denselayer4.conv1': 50, 'features.denseblock2.denselayer10.conv1': 31, 'features.denseblock3.denselayer15.conv1': 38, 'features.denseblock3.denselayer12.conv1': 33, 'features.denseblock3.denselayer23.conv1': 39, 'features.denseblock2.denselayer11.conv1': 38, 'features.denseblock3.denselayer6.conv1': 33, 'features.denseblock2.denselayer4.conv1': 48, 'features.denseblock3.denselayer2.conv1': 41, 'features.denseblock3.denselayer19.conv1': 23, 'features.denseblock4.denselayer3.conv1': 44, 'features.denseblock2.denselayer6.conv1': 48, 'features.denseblock3.denselayer9.conv1': 41, 'features.denseblock3.denselayer22.conv1': 37, 'features.denseblock4.denselayer5.conv1': 51, 'features.denseblock3.denselayer20.conv1': 34, 'features.denseblock2.denselayer12.conv1': 32, 'features.denseblock3.denselayer8.conv1': 38, 'features.denseblock3.denselayer14.conv1': 43, 'features.denseblock1.denselayer6.conv1': 37, 'features.denseblock3.denselayer21.conv1': 41, 'features.denseblock2.denselayer8.conv1': 40, 'features.denseblock4.denselayer2.conv1': 40, 'features.denseblock1.denselayer5.conv1': 49, 'features.denseblock3.denselayer24.conv1': 27, 'features.denseblock3.denselayer1.conv1': 41, 'features.denseblock1.denselayer3.conv1': 42, 'features.denseblock4.denselayer4.conv1': 44, 'features.denseblock3.denselayer4.conv1': 31, 'features.denseblock4.denselayer1.conv1': 39, 'features.denseblock3.denselayer16.conv1': 34, 'features.denseblock3.denselayer17.conv1': 26, 'features.denseblock3.denselayer7.conv1': 30, 'features.denseblock2.denselayer5.conv1': 31, 'features.denseblock3.denselayer5.conv1': 35, 'features.denseblock2.denselayer2.conv1': 50, 'features.denseblock2.denselayer3.conv1': 40, 'features.denseblock2.denselayer7.conv1': 27, 'features.denseblock3.denselayer11.conv1': 37, 'features.denseblock3.denselayer18.conv1': 37}
convolution remaining after pruning {'features.denseblock4.denselayer7.conv1': 96, 'features.denseblock4.denselayer8.conv1': 80, 'features.denseblock4.denselayer9.conv1': 80, 'features.denseblock4.denselayer10.conv1': 82, 'features.denseblock4.denselayer11.conv1': 68, 'features.denseblock4.denselayer12.conv1': 64, 'features.denseblock4.denselayer13.conv1': 61, 'features.denseblock4.denselayer14.conv1': 59, 'features.denseblock4.denselayer15.conv1': 59, 'features.denseblock4.denselayer16.conv1': 58, 'features.denseblock1.denselayer1.conv1': 87, 'features.denseblock1.denselayer2.conv1': 86, 'features.denseblock3.denselayer10.conv1': 96, 'features.denseblock2.denselayer1.conv1': 69, 'features.denseblock4.denselayer6.conv1': 84, 'features.denseblock3.denselayer3.conv1': 82, 'features.denseblock3.denselayer13.conv1': 100, 'features.denseblock2.denselayer9.conv1': 90, 'features.denseblock1.denselayer4.conv1': 78, 'features.denseblock2.denselayer10.conv1': 97, 'features.denseblock3.denselayer15.conv1': 90, 'features.denseblock3.denselayer12.conv1': 95, 'features.denseblock3.denselayer23.conv1': 89, 'features.denseblock2.denselayer11.conv1': 90, 'features.denseblock3.denselayer6.conv1': 95, 'features.denseblock2.denselayer4.conv1': 80, 'features.denseblock3.denselayer2.conv1': 87, 'features.denseblock3.denselayer19.conv1': 105, 'features.denseblock4.denselayer3.conv1': 84, 'features.denseblock2.denselayer6.conv1': 80, 'features.denseblock3.denselayer9.conv1': 87, 'features.denseblock3.denselayer22.conv1': 91, 'features.denseblock4.denselayer5.conv1': 77, 'features.denseblock3.denselayer20.conv1': 94, 'features.denseblock2.denselayer12.conv1': 96, 'features.denseblock3.denselayer8.conv1': 90, 'features.denseblock3.denselayer14.conv1': 85, 'features.denseblock1.denselayer6.conv1': 91, 'features.denseblock3.denselayer21.conv1': 87, 'features.denseblock2.denselayer8.conv1': 88, 'features.denseblock4.denselayer2.conv1': 88, 'features.denseblock1.denselayer5.conv1': 79, 'features.denseblock3.denselayer24.conv1': 101, 'features.denseblock3.denselayer1.conv1': 87, 'features.denseblock1.denselayer3.conv1': 86, 'features.denseblock4.denselayer4.conv1': 84, 'features.denseblock3.denselayer4.conv1': 97, 'features.denseblock4.denselayer1.conv1': 89, 'features.denseblock3.denselayer16.conv1': 94, 'features.denseblock3.denselayer17.conv1': 102, 'features.denseblock3.denselayer7.conv1': 98, 'features.denseblock2.denselayer5.conv1': 97, 'features.denseblock3.denselayer5.conv1': 93, 'features.denseblock2.denselayer2.conv1': 78, 'features.denseblock2.denselayer3.conv1': 88, 'features.denseblock2.denselayer7.conv1': 101, 'features.denseblock3.denselayer11.conv1': 91, 'features.denseblock3.denselayer18.conv1': 91}
Pruning filters.. 
Filters pruned 26.29123263888889%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 95.26 - Val acc: 92.38 - Train loss: 0.1416 - Val loss: 0.2217 - Training time: 261.47s
end number of flops: 2143170048.0 	number of params: 5929082.0
diff number of flops: 0.2646775379019973 	diff number of params: 0.25690073865225793
Final Test:
	Score: 91.33
***  DenseNet121-degrad
number of flops: 2914598912.0 	number of params: 7978856.0
1 iterations to reduce 35.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 0}
Layers that will be pruned {'features.denseblock4.denselayer7.conv1': 50, 'features.denseblock4.denselayer9.conv1': 57, 'features.denseblock4.denselayer11.conv1': 60, 'features.denseblock4.denselayer12.conv1': 69, 'features.denseblock4.denselayer13.conv1': 73, 'features.denseblock4.denselayer14.conv1': 58, 'features.denseblock4.denselayer15.conv1': 68, 'features.denseblock4.denselayer16.conv1': 58, 'features.denseblock1.denselayer1.conv1': 49, 'features.denseblock1.denselayer2.conv1': 58, 'features.denseblock2.denselayer3.conv1': 44, 'features.denseblock2.denselayer1.conv1': 55, 'features.denseblock3.denselayer1.conv1': 47, 'features.denseblock3.denselayer16.conv1': 32, 'features.denseblock3.denselayer7.conv1': 35, 'features.denseblock4.denselayer10.conv1': 57, 'features.denseblock3.denselayer21.conv1': 41, 'features.denseblock4.denselayer4.conv1': 47, 'features.denseblock3.denselayer24.conv1': 45, 'features.denseblock3.denselayer20.conv1': 39, 'features.denseblock3.denselayer6.conv1': 48, 'features.denseblock1.denselayer3.conv1': 50, 'features.denseblock1.denselayer5.conv1': 54, 'features.denseblock2.denselayer2.conv1': 47, 'features.denseblock1.denselayer4.conv1': 62, 'features.denseblock3.denselayer3.conv1': 42, 'features.denseblock4.denselayer1.conv1': 54, 'features.denseblock2.denselayer4.conv1': 55, 'features.denseblock3.denselayer18.conv1': 58, 'features.denseblock4.denselayer6.conv1': 74, 'features.denseblock4.denselayer2.conv1': 36, 'features.denseblock3.denselayer15.conv1': 32, 'features.denseblock4.denselayer8.conv1': 54, 'features.denseblock3.denselayer14.conv1': 44, 'features.denseblock3.denselayer9.conv1': 36, 'features.denseblock2.denselayer10.conv1': 47, 'features.denseblock2.denselayer7.conv1': 37, 'features.denseblock2.denselayer6.conv1': 52, 'features.denseblock4.denselayer3.conv1': 53, 'features.denseblock3.denselayer2.conv1': 50, 'features.denseblock3.denselayer12.conv1': 38, 'features.denseblock3.denselayer11.conv1': 42, 'features.denseblock1.denselayer6.conv1': 57, 'features.denseblock3.denselayer4.conv1': 41, 'features.denseblock2.denselayer8.conv1': 39, 'features.denseblock3.denselayer17.conv1': 43, 'features.denseblock2.denselayer5.conv1': 34, 'features.denseblock3.denselayer19.conv1': 37, 'features.denseblock3.denselayer13.conv1': 37, 'features.denseblock3.denselayer22.conv1': 39, 'features.denseblock2.denselayer9.conv1': 34, 'features.denseblock3.denselayer23.conv1': 38, 'features.denseblock3.denselayer5.conv1': 37, 'features.denseblock3.denselayer8.conv1': 48, 'features.denseblock2.denselayer12.conv1': 33, 'features.denseblock4.denselayer5.conv1': 56, 'features.denseblock2.denselayer11.conv1': 40, 'features.denseblock3.denselayer10.conv1': 44}
convolution remaining after pruning {'features.denseblock4.denselayer7.conv1': 78, 'features.denseblock4.denselayer9.conv1': 71, 'features.denseblock4.denselayer11.conv1': 68, 'features.denseblock4.denselayer12.conv1': 59, 'features.denseblock4.denselayer13.conv1': 55, 'features.denseblock4.denselayer14.conv1': 70, 'features.denseblock4.denselayer15.conv1': 60, 'features.denseblock4.denselayer16.conv1': 70, 'features.denseblock1.denselayer1.conv1': 79, 'features.denseblock1.denselayer2.conv1': 70, 'features.denseblock2.denselayer3.conv1': 84, 'features.denseblock2.denselayer1.conv1': 73, 'features.denseblock3.denselayer1.conv1': 81, 'features.denseblock3.denselayer16.conv1': 96, 'features.denseblock3.denselayer7.conv1': 93, 'features.denseblock4.denselayer10.conv1': 71, 'features.denseblock3.denselayer21.conv1': 87, 'features.denseblock4.denselayer4.conv1': 81, 'features.denseblock3.denselayer24.conv1': 83, 'features.denseblock3.denselayer20.conv1': 89, 'features.denseblock3.denselayer6.conv1': 80, 'features.denseblock1.denselayer3.conv1': 78, 'features.denseblock1.denselayer5.conv1': 74, 'features.denseblock2.denselayer2.conv1': 81, 'features.denseblock1.denselayer4.conv1': 66, 'features.denseblock3.denselayer3.conv1': 86, 'features.denseblock4.denselayer1.conv1': 74, 'features.denseblock2.denselayer4.conv1': 73, 'features.denseblock3.denselayer18.conv1': 70, 'features.denseblock4.denselayer6.conv1': 54, 'features.denseblock4.denselayer2.conv1': 92, 'features.denseblock3.denselayer15.conv1': 96, 'features.denseblock4.denselayer8.conv1': 74, 'features.denseblock3.denselayer14.conv1': 84, 'features.denseblock3.denselayer9.conv1': 92, 'features.denseblock2.denselayer10.conv1': 81, 'features.denseblock2.denselayer7.conv1': 91, 'features.denseblock2.denselayer6.conv1': 76, 'features.denseblock4.denselayer3.conv1': 75, 'features.denseblock3.denselayer2.conv1': 78, 'features.denseblock3.denselayer12.conv1': 90, 'features.denseblock3.denselayer11.conv1': 86, 'features.denseblock1.denselayer6.conv1': 71, 'features.denseblock3.denselayer4.conv1': 87, 'features.denseblock2.denselayer8.conv1': 89, 'features.denseblock3.denselayer17.conv1': 85, 'features.denseblock2.denselayer5.conv1': 94, 'features.denseblock3.denselayer19.conv1': 91, 'features.denseblock3.denselayer13.conv1': 91, 'features.denseblock3.denselayer22.conv1': 89, 'features.denseblock2.denselayer9.conv1': 94, 'features.denseblock3.denselayer23.conv1': 90, 'features.denseblock3.denselayer5.conv1': 91, 'features.denseblock3.denselayer8.conv1': 80, 'features.denseblock2.denselayer12.conv1': 95, 'features.denseblock4.denselayer5.conv1': 72, 'features.denseblock2.denselayer11.conv1': 88, 'features.denseblock3.denselayer10.conv1': 84}
Pruning filters.. 
Filters pruned 29.991319444444443%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 94.77 - Val acc: 92.15 - Train loss: 0.1578 - Val loss: 0.2314 - Training time: 257.96s
end number of flops: 1997903616.0 	number of params: 5656240.0
diff number of flops: 0.3145185062087884 	diff number of params: 0.29109636769983066
Final Test:
	Score: 91.24
***  DenseNet121-degrad
number of flops: 2914598912.0 	number of params: 7978856.0
1 iterations to reduce 40.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 0}
Layers that will be pruned {'features.denseblock4.denselayer7.conv1': 78, 'features.denseblock4.denselayer9.conv1': 48, 'features.denseblock4.denselayer11.conv1': 65, 'features.denseblock4.denselayer13.conv1': 90, 'features.denseblock4.denselayer14.conv1': 60, 'features.denseblock4.denselayer15.conv1': 81, 'features.denseblock1.denselayer1.conv1': 66, 'features.denseblock1.denselayer2.conv1': 72, 'features.denseblock3.denselayer5.conv1': 40, 'features.denseblock3.denselayer21.conv1': 46, 'features.denseblock4.denselayer12.conv1': 70, 'features.denseblock3.denselayer4.conv1': 43, 'features.denseblock4.denselayer6.conv1': 54, 'features.denseblock2.denselayer2.conv1': 52, 'features.denseblock4.denselayer16.conv1': 74, 'features.denseblock4.denselayer10.conv1': 63, 'features.denseblock2.denselayer3.conv1': 38, 'features.denseblock3.denselayer22.conv1': 49, 'features.denseblock1.denselayer6.conv1': 60, 'features.denseblock3.denselayer14.conv1': 52, 'features.denseblock4.denselayer5.conv1': 56, 'features.denseblock3.denselayer12.conv1': 56, 'features.denseblock3.denselayer18.conv1': 55, 'features.denseblock3.denselayer15.conv1': 40, 'features.denseblock4.denselayer8.conv1': 57, 'features.denseblock4.denselayer2.conv1': 53, 'features.denseblock1.denselayer4.conv1': 63, 'features.denseblock3.denselayer1.conv1': 55, 'features.denseblock2.denselayer6.conv1': 64, 'features.denseblock2.denselayer10.conv1': 51, 'features.denseblock2.denselayer12.conv1': 43, 'features.denseblock3.denselayer6.conv1': 59, 'features.denseblock3.denselayer16.conv1': 50, 'features.denseblock2.denselayer11.conv1': 49, 'features.denseblock2.denselayer7.conv1': 38, 'features.denseblock4.denselayer1.conv1': 56, 'features.denseblock1.denselayer5.conv1': 60, 'features.denseblock4.denselayer4.conv1': 62, 'features.denseblock2.denselayer5.conv1': 41, 'features.denseblock2.denselayer4.conv1': 66, 'features.denseblock3.denselayer9.conv1': 56, 'features.denseblock3.denselayer23.conv1': 50, 'features.denseblock3.denselayer19.conv1': 59, 'features.denseblock3.denselayer3.conv1': 44, 'features.denseblock3.denselayer11.conv1': 47, 'features.denseblock2.denselayer1.conv1': 69, 'features.denseblock3.denselayer2.conv1': 49, 'features.denseblock3.denselayer13.conv1': 45, 'features.denseblock2.denselayer9.conv1': 39, 'features.denseblock2.denselayer8.conv1': 54, 'features.denseblock3.denselayer10.conv1': 45, 'features.denseblock3.denselayer24.conv1': 41, 'features.denseblock3.denselayer20.conv1': 49, 'features.denseblock3.denselayer7.conv1': 49, 'features.denseblock3.denselayer8.conv1': 41, 'features.denseblock3.denselayer17.conv1': 41, 'features.denseblock4.denselayer3.conv1': 54, 'features.denseblock1.denselayer3.conv1': 51}
convolution remaining after pruning {'features.denseblock4.denselayer7.conv1': 50, 'features.denseblock4.denselayer9.conv1': 80, 'features.denseblock4.denselayer11.conv1': 63, 'features.denseblock4.denselayer13.conv1': 38, 'features.denseblock4.denselayer14.conv1': 68, 'features.denseblock4.denselayer15.conv1': 47, 'features.denseblock1.denselayer1.conv1': 62, 'features.denseblock1.denselayer2.conv1': 56, 'features.denseblock3.denselayer5.conv1': 88, 'features.denseblock3.denselayer21.conv1': 82, 'features.denseblock4.denselayer12.conv1': 58, 'features.denseblock3.denselayer4.conv1': 85, 'features.denseblock4.denselayer6.conv1': 74, 'features.denseblock2.denselayer2.conv1': 76, 'features.denseblock4.denselayer16.conv1': 54, 'features.denseblock4.denselayer10.conv1': 65, 'features.denseblock2.denselayer3.conv1': 90, 'features.denseblock3.denselayer22.conv1': 79, 'features.denseblock1.denselayer6.conv1': 68, 'features.denseblock3.denselayer14.conv1': 76, 'features.denseblock4.denselayer5.conv1': 72, 'features.denseblock3.denselayer12.conv1': 72, 'features.denseblock3.denselayer18.conv1': 73, 'features.denseblock3.denselayer15.conv1': 88, 'features.denseblock4.denselayer8.conv1': 71, 'features.denseblock4.denselayer2.conv1': 75, 'features.denseblock1.denselayer4.conv1': 65, 'features.denseblock3.denselayer1.conv1': 73, 'features.denseblock2.denselayer6.conv1': 64, 'features.denseblock2.denselayer10.conv1': 77, 'features.denseblock2.denselayer12.conv1': 85, 'features.denseblock3.denselayer6.conv1': 69, 'features.denseblock3.denselayer16.conv1': 78, 'features.denseblock2.denselayer11.conv1': 79, 'features.denseblock2.denselayer7.conv1': 90, 'features.denseblock4.denselayer1.conv1': 72, 'features.denseblock1.denselayer5.conv1': 68, 'features.denseblock4.denselayer4.conv1': 66, 'features.denseblock2.denselayer5.conv1': 87, 'features.denseblock2.denselayer4.conv1': 62, 'features.denseblock3.denselayer9.conv1': 72, 'features.denseblock3.denselayer23.conv1': 78, 'features.denseblock3.denselayer19.conv1': 69, 'features.denseblock3.denselayer3.conv1': 84, 'features.denseblock3.denselayer11.conv1': 81, 'features.denseblock2.denselayer1.conv1': 59, 'features.denseblock3.denselayer2.conv1': 79, 'features.denseblock3.denselayer13.conv1': 83, 'features.denseblock2.denselayer9.conv1': 89, 'features.denseblock2.denselayer8.conv1': 74, 'features.denseblock3.denselayer10.conv1': 83, 'features.denseblock3.denselayer24.conv1': 87, 'features.denseblock3.denselayer20.conv1': 79, 'features.denseblock3.denselayer7.conv1': 79, 'features.denseblock3.denselayer8.conv1': 87, 'features.denseblock3.denselayer17.conv1': 87, 'features.denseblock4.denselayer3.conv1': 74, 'features.denseblock1.denselayer3.conv1': 77}
Pruning filters.. 
Filters pruned 34.26649305555556%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 93.19 - Val acc: 90.24 - Train loss: 0.2016 - Val loss: 0.2824 - Training time: 262.28s
end number of flops: 1867405056.0 	number of params: 5323420.0
diff number of flops: 0.35929261199147805 	diff number of params: 0.332809114489596
Final Test:
	Score: 89.31
***  DenseNet121-degrad
number of flops: 2914598912.0 	number of params: 7978856.0
1 iterations to reduce 45.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 0}
Layers that will be pruned {'features.denseblock4.denselayer7.conv1': 58, 'features.denseblock4.denselayer11.conv1': 62, 'features.denseblock4.denselayer16.conv1': 90, 'features.denseblock1.denselayer1.conv1': 70, 'features.denseblock1.denselayer2.conv1': 88, 'features.denseblock3.denselayer19.conv1': 48, 'features.denseblock3.denselayer6.conv1': 57, 'features.denseblock4.denselayer15.conv1': 82, 'features.denseblock4.denselayer12.conv1': 82, 'features.denseblock1.denselayer4.conv1': 71, 'features.denseblock1.denselayer5.conv1': 59, 'features.denseblock3.denselayer2.conv1': 59, 'features.denseblock4.denselayer13.conv1': 88, 'features.denseblock4.denselayer6.conv1': 78, 'features.denseblock2.denselayer9.conv1': 54, 'features.denseblock3.denselayer21.conv1': 46, 'features.denseblock4.denselayer1.conv1': 56, 'features.denseblock3.denselayer23.conv1': 45, 'features.denseblock4.denselayer2.conv1': 52, 'features.denseblock4.denselayer14.conv1': 65, 'features.denseblock2.denselayer12.conv1': 49, 'features.denseblock3.denselayer13.conv1': 57, 'features.denseblock3.denselayer11.conv1': 57, 'features.denseblock4.denselayer9.conv1': 72, 'features.denseblock2.denselayer11.conv1': 50, 'features.denseblock3.denselayer5.conv1': 51, 'features.denseblock3.denselayer16.conv1': 51, 'features.denseblock3.denselayer9.conv1': 54, 'features.denseblock1.denselayer6.conv1': 70, 'features.denseblock4.denselayer10.conv1': 58, 'features.denseblock2.denselayer1.conv1': 92, 'features.denseblock1.denselayer3.conv1': 77, 'features.denseblock4.denselayer5.conv1': 81, 'features.denseblock2.denselayer2.conv1': 46, 'features.denseblock3.denselayer24.conv1': 48, 'features.denseblock2.denselayer10.conv1': 58, 'features.denseblock3.denselayer7.conv1': 40, 'features.denseblock3.denselayer12.conv1': 49, 'features.denseblock4.denselayer8.conv1': 70, 'features.denseblock3.denselayer20.conv1': 50, 'features.denseblock2.denselayer6.conv1': 73, 'features.denseblock3.denselayer1.conv1': 55, 'features.denseblock4.denselayer3.conv1': 56, 'features.denseblock2.denselayer7.conv1': 59, 'features.denseblock4.denselayer4.conv1': 76, 'features.denseblock3.denselayer18.conv1': 54, 'features.denseblock3.denselayer15.conv1': 41, 'features.denseblock2.denselayer8.conv1': 50, 'features.denseblock3.denselayer22.conv1': 51, 'features.denseblock2.denselayer3.conv1': 54, 'features.denseblock3.denselayer3.conv1': 53, 'features.denseblock3.denselayer14.conv1': 51, 'features.denseblock3.denselayer10.conv1': 55, 'features.denseblock3.denselayer4.conv1': 58, 'features.denseblock2.denselayer4.conv1': 66, 'features.denseblock3.denselayer17.conv1': 61, 'features.denseblock3.denselayer8.conv1': 53, 'features.denseblock2.denselayer5.conv1': 58}
convolution remaining after pruning {'features.denseblock4.denselayer7.conv1': 70, 'features.denseblock4.denselayer11.conv1': 66, 'features.denseblock4.denselayer16.conv1': 38, 'features.denseblock1.denselayer1.conv1': 58, 'features.denseblock1.denselayer2.conv1': 40, 'features.denseblock3.denselayer19.conv1': 80, 'features.denseblock3.denselayer6.conv1': 71, 'features.denseblock4.denselayer15.conv1': 46, 'features.denseblock4.denselayer12.conv1': 46, 'features.denseblock1.denselayer4.conv1': 57, 'features.denseblock1.denselayer5.conv1': 69, 'features.denseblock3.denselayer2.conv1': 69, 'features.denseblock4.denselayer13.conv1': 40, 'features.denseblock4.denselayer6.conv1': 50, 'features.denseblock2.denselayer9.conv1': 74, 'features.denseblock3.denselayer21.conv1': 82, 'features.denseblock4.denselayer1.conv1': 72, 'features.denseblock3.denselayer23.conv1': 83, 'features.denseblock4.denselayer2.conv1': 76, 'features.denseblock4.denselayer14.conv1': 63, 'features.denseblock2.denselayer12.conv1': 79, 'features.denseblock3.denselayer13.conv1': 71, 'features.denseblock3.denselayer11.conv1': 71, 'features.denseblock4.denselayer9.conv1': 56, 'features.denseblock2.denselayer11.conv1': 78, 'features.denseblock3.denselayer5.conv1': 77, 'features.denseblock3.denselayer16.conv1': 77, 'features.denseblock3.denselayer9.conv1': 74, 'features.denseblock1.denselayer6.conv1': 58, 'features.denseblock4.denselayer10.conv1': 70, 'features.denseblock2.denselayer1.conv1': 36, 'features.denseblock1.denselayer3.conv1': 51, 'features.denseblock4.denselayer5.conv1': 47, 'features.denseblock2.denselayer2.conv1': 82, 'features.denseblock3.denselayer24.conv1': 80, 'features.denseblock2.denselayer10.conv1': 70, 'features.denseblock3.denselayer7.conv1': 88, 'features.denseblock3.denselayer12.conv1': 79, 'features.denseblock4.denselayer8.conv1': 58, 'features.denseblock3.denselayer20.conv1': 78, 'features.denseblock2.denselayer6.conv1': 55, 'features.denseblock3.denselayer1.conv1': 73, 'features.denseblock4.denselayer3.conv1': 72, 'features.denseblock2.denselayer7.conv1': 69, 'features.denseblock4.denselayer4.conv1': 52, 'features.denseblock3.denselayer18.conv1': 74, 'features.denseblock3.denselayer15.conv1': 87, 'features.denseblock2.denselayer8.conv1': 78, 'features.denseblock3.denselayer22.conv1': 77, 'features.denseblock2.denselayer3.conv1': 74, 'features.denseblock3.denselayer3.conv1': 75, 'features.denseblock3.denselayer14.conv1': 77, 'features.denseblock3.denselayer10.conv1': 73, 'features.denseblock3.denselayer4.conv1': 70, 'features.denseblock2.denselayer4.conv1': 62, 'features.denseblock3.denselayer17.conv1': 67, 'features.denseblock3.denselayer8.conv1': 75, 'features.denseblock2.denselayer5.conv1': 70}
Pruning filters.. 
Filters pruned 38.12934027777778%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 92.42 - Val acc: 88.68 - Train loss: 0.2204 - Val loss: 0.3152 - Training time: 260.08s
end number of flops: 1717795328.0 	number of params: 5062900.0
diff number of flops: 0.4106237668148831 	diff number of params: 0.36546041186856865
Final Test:
	Score: 88.78
***  DenseNet121-degrad
number of flops: 2914598912.0 	number of params: 7978856.0
1 iterations to reduce 50.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 0}
Layers that will be pruned {'features.denseblock4.denselayer7.conv1': 73, 'features.denseblock4.denselayer11.conv1': 68, 'features.denseblock4.denselayer13.conv1': 92, 'features.denseblock1.denselayer1.conv1': 69, 'features.denseblock1.denselayer2.conv1': 74, 'features.denseblock2.denselayer11.conv1': 63, 'features.denseblock4.denselayer16.conv1': 100, 'features.denseblock3.denselayer3.conv1': 53, 'features.denseblock4.denselayer12.conv1': 91, 'features.denseblock3.denselayer23.conv1': 63, 'features.denseblock3.denselayer14.conv1': 68, 'features.denseblock1.denselayer6.conv1': 65, 'features.denseblock2.denselayer2.conv1': 74, 'features.denseblock4.denselayer10.conv1': 62, 'features.denseblock4.denselayer14.conv1': 75, 'features.denseblock4.denselayer6.conv1': 78, 'features.denseblock3.denselayer18.conv1': 72, 'features.denseblock4.denselayer9.conv1': 85, 'features.denseblock4.denselayer15.conv1': 89, 'features.denseblock2.denselayer7.conv1': 53, 'features.denseblock1.denselayer5.conv1': 77, 'features.denseblock3.denselayer17.conv1': 58, 'features.denseblock3.denselayer11.conv1': 71, 'features.denseblock3.denselayer2.conv1': 72, 'features.denseblock2.denselayer3.conv1': 65, 'features.denseblock3.denselayer8.conv1': 60, 'features.denseblock3.denselayer1.conv1': 58, 'features.denseblock3.denselayer12.conv1': 57, 'features.denseblock3.denselayer7.conv1': 53, 'features.denseblock3.denselayer16.conv1': 61, 'features.denseblock4.denselayer4.conv1': 68, 'features.denseblock4.denselayer3.conv1': 59, 'features.denseblock2.denselayer1.conv1': 91, 'features.denseblock4.denselayer5.conv1': 84, 'features.denseblock3.denselayer19.conv1': 67, 'features.denseblock3.denselayer21.conv1': 48, 'features.denseblock1.denselayer4.conv1': 86, 'features.denseblock2.denselayer8.conv1': 63, 'features.denseblock3.denselayer20.conv1': 60, 'features.denseblock3.denselayer9.conv1': 66, 'features.denseblock4.denselayer1.conv1': 78, 'features.denseblock4.denselayer8.conv1': 73, 'features.denseblock1.denselayer3.conv1': 78, 'features.denseblock2.denselayer9.conv1': 59, 'features.denseblock2.denselayer10.conv1': 70, 'features.denseblock3.denselayer4.conv1': 53, 'features.denseblock2.denselayer4.conv1': 69, 'features.denseblock2.denselayer12.conv1': 55, 'features.denseblock3.denselayer6.conv1': 70, 'features.denseblock4.denselayer2.conv1': 68, 'features.denseblock3.denselayer22.conv1': 51, 'features.denseblock3.denselayer24.conv1': 56, 'features.denseblock3.denselayer5.conv1': 57, 'features.denseblock3.denselayer10.conv1': 67, 'features.denseblock2.denselayer6.conv1': 68, 'features.denseblock3.denselayer13.conv1': 57, 'features.denseblock3.denselayer15.conv1': 59, 'features.denseblock2.denselayer5.conv1': 77}
convolution remaining after pruning {'features.denseblock4.denselayer7.conv1': 55, 'features.denseblock4.denselayer11.conv1': 60, 'features.denseblock4.denselayer13.conv1': 36, 'features.denseblock1.denselayer1.conv1': 59, 'features.denseblock1.denselayer2.conv1': 54, 'features.denseblock2.denselayer11.conv1': 65, 'features.denseblock4.denselayer16.conv1': 28, 'features.denseblock3.denselayer3.conv1': 75, 'features.denseblock4.denselayer12.conv1': 37, 'features.denseblock3.denselayer23.conv1': 65, 'features.denseblock3.denselayer14.conv1': 60, 'features.denseblock1.denselayer6.conv1': 63, 'features.denseblock2.denselayer2.conv1': 54, 'features.denseblock4.denselayer10.conv1': 66, 'features.denseblock4.denselayer14.conv1': 53, 'features.denseblock4.denselayer6.conv1': 50, 'features.denseblock3.denselayer18.conv1': 56, 'features.denseblock4.denselayer9.conv1': 43, 'features.denseblock4.denselayer15.conv1': 39, 'features.denseblock2.denselayer7.conv1': 75, 'features.denseblock1.denselayer5.conv1': 51, 'features.denseblock3.denselayer17.conv1': 70, 'features.denseblock3.denselayer11.conv1': 57, 'features.denseblock3.denselayer2.conv1': 56, 'features.denseblock2.denselayer3.conv1': 63, 'features.denseblock3.denselayer8.conv1': 68, 'features.denseblock3.denselayer1.conv1': 70, 'features.denseblock3.denselayer12.conv1': 71, 'features.denseblock3.denselayer7.conv1': 75, 'features.denseblock3.denselayer16.conv1': 67, 'features.denseblock4.denselayer4.conv1': 60, 'features.denseblock4.denselayer3.conv1': 69, 'features.denseblock2.denselayer1.conv1': 37, 'features.denseblock4.denselayer5.conv1': 44, 'features.denseblock3.denselayer19.conv1': 61, 'features.denseblock3.denselayer21.conv1': 80, 'features.denseblock1.denselayer4.conv1': 42, 'features.denseblock2.denselayer8.conv1': 65, 'features.denseblock3.denselayer20.conv1': 68, 'features.denseblock3.denselayer9.conv1': 62, 'features.denseblock4.denselayer1.conv1': 50, 'features.denseblock4.denselayer8.conv1': 55, 'features.denseblock1.denselayer3.conv1': 50, 'features.denseblock2.denselayer9.conv1': 69, 'features.denseblock2.denselayer10.conv1': 58, 'features.denseblock3.denselayer4.conv1': 75, 'features.denseblock2.denselayer4.conv1': 59, 'features.denseblock2.denselayer12.conv1': 73, 'features.denseblock3.denselayer6.conv1': 58, 'features.denseblock4.denselayer2.conv1': 60, 'features.denseblock3.denselayer22.conv1': 77, 'features.denseblock3.denselayer24.conv1': 72, 'features.denseblock3.denselayer5.conv1': 71, 'features.denseblock3.denselayer10.conv1': 61, 'features.denseblock2.denselayer6.conv1': 60, 'features.denseblock3.denselayer13.conv1': 71, 'features.denseblock3.denselayer15.conv1': 69, 'features.denseblock2.denselayer5.conv1': 51}
Pruning filters.. 
Filters pruned 42.92534722222222%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 89.44 - Val acc: 86.17 - Train loss: 0.3136 - Val loss: 0.4051 - Training time: 265.67s
end number of flops: 1603312896.0 	number of params: 4675200.0
diff number of flops: 0.4499027329630733 	diff number of params: 0.4140513376855028
Final Test:
	Score: 86.08
***  Squeeze-degrad
number of flops: 516412928.0 	number of params: 1235496.0
Epoch 0 - Train acc: 95.44 - Val acc: 94.31 - Train loss: 0.1358 - Val loss: 0.1688 - Training time: 119.45s
end number of flops: 516412928.0 	number of params: 1235496.0
diff number of flops: 0.0 	diff number of params: 0.0
Final Test:
	Score: 89.05
***  Squeeze-degrad
number of flops: 516412928.0 	number of params: 1235496.0
C:\Users\naked\AppData\Local\conda\conda\envs\7030\lib\site-packages\torch\onnx\symbolic.py:131: UserWarning: ONNX export failed on max_pool2d_with_indices because ceil_mode not supported
  warnings.warn("ONNX export failed on " + op + " because " + msg + " not supported")
1 iterations to reduce 5.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 1, 3: 0, 4: 0, 5: 0, 6: 0, 7: 3}
Layers that will be pruned {'features.12.expand1x1': 3, 'features.12.expand3x3': 3, 'features.6.expand3x3': 1, 'features.6.expand1x1': 1, 'features.9.squeeze': 1, 'features.0': 2, 'features.7.squeeze': 1, 'features.10.squeeze': 1}
convolution remaining after pruning {'features.12.expand1x1': 253, 'features.12.expand3x3': 253, 'features.6.expand3x3': 127, 'features.6.expand1x1': 127, 'features.9.squeeze': 47, 'features.0': 62, 'features.7.squeeze': 31, 'features.10.squeeze': 47}
Pruning filters.. 
Filters pruned 0.44157608695652173%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 95.36 - Val acc: 94.31 - Train loss: 0.1397 - Val loss: 0.1706 - Training time: 122.07s
end number of flops: 510157952.0 	number of params: 1221079.0
diff number of flops: 0.012112353624113763 	diff number of params: 0.011668997714278314
Final Test:
	Score: 89.16
***  Squeeze-degrad
number of flops: 516412928.0 	number of params: 1235496.0
1 iterations to reduce 10.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 1, 3: 1, 4: 3, 5: 1, 6: 2, 7: 8}
Layers that will be pruned {'features.9.expand3x3': 3, 'features.11.expand3x3': 2, 'features.12.expand1x1': 8, 'features.12.expand3x3': 8, 'features.6.expand3x3': 1, 'features.6.expand1x1': 1, 'features.0': 10, 'features.10.expand3x3': 1, 'features.11.expand1x1': 2, 'features.10.expand1x1': 1, 'features.9.expand1x1': 3, 'features.7.expand1x1': 1, 'features.10.squeeze': 2, 'features.7.expand3x3': 1, 'features.9.squeeze': 1, 'features.12.squeeze': 3, 'features.11.squeeze': 2}
convolution remaining after pruning {'features.9.expand3x3': 189, 'features.11.expand3x3': 254, 'features.12.expand1x1': 248, 'features.12.expand3x3': 248, 'features.6.expand3x3': 127, 'features.6.expand1x1': 127, 'features.0': 54, 'features.10.expand3x3': 191, 'features.11.expand1x1': 254, 'features.10.expand1x1': 191, 'features.9.expand1x1': 189, 'features.7.expand1x1': 127, 'features.10.squeeze': 46, 'features.7.expand3x3': 127, 'features.9.squeeze': 47, 'features.12.squeeze': 61, 'features.11.squeeze': 62}
Pruning filters.. 
Filters pruned 1.6983695652173914%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 94.99 - Val acc: 93.67 - Train loss: 0.1459 - Val loss: 0.1875 - Training time: 121.15s
end number of flops: 498704832.0 	number of params: 1187696.0
diff number of flops: 0.03429057453805649 	diff number of params: 0.03868891522109339
Final Test:
	Score: 88.75999999999999
***  Squeeze-degrad
number of flops: 516412928.0 	number of params: 1235496.0
1 iterations to reduce 15.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 0, 1: 0, 2: 5, 3: 2, 4: 2, 5: 3, 6: 10, 7: 22}
Layers that will be pruned {'features.9.expand3x3': 2, 'features.11.expand3x3': 10, 'features.12.expand1x1': 22, 'features.12.expand3x3': 22, 'features.6.expand1x1': 5, 'features.6.expand3x3': 5, 'features.11.expand1x1': 10, 'features.10.expand3x3': 3, 'features.7.expand1x1': 2, 'features.10.expand1x1': 3, 'features.12.squeeze': 2, 'features.9.expand1x1': 2, 'features.7.squeeze': 2, 'features.11.squeeze': 2, 'features.7.expand3x3': 2, 'features.4.squeeze': 1, 'features.10.squeeze': 1, 'features.9.squeeze': 2, 'features.0': 6}
convolution remaining after pruning {'features.9.expand3x3': 190, 'features.11.expand3x3': 246, 'features.12.expand1x1': 234, 'features.12.expand3x3': 234, 'features.6.expand1x1': 123, 'features.6.expand3x3': 123, 'features.11.expand1x1': 246, 'features.10.expand3x3': 189, 'features.7.expand1x1': 126, 'features.10.expand1x1': 189, 'features.12.squeeze': 62, 'features.9.expand1x1': 190, 'features.7.squeeze': 30, 'features.11.squeeze': 62, 'features.7.expand3x3': 126, 'features.4.squeeze': 15, 'features.10.squeeze': 47, 'features.9.squeeze': 46, 'features.0': 58}
Pruning filters.. 
Filters pruned 3.532608695652174%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 96.23 - Val acc: 95.07 - Train loss: 0.1173 - Val loss: 0.1514 - Training time: 121.31s
end number of flops: 480818048.0 	number of params: 1141952.0
diff number of flops: 0.06892716674978361 	diff number of params: 0.07571372145276067
Final Test:
	Score: 89.92
***  Squeeze-degrad
number of flops: 516412928.0 	number of params: 1235496.0
1 iterations to reduce 20.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 4, 1: 2, 2: 7, 3: 2, 4: 8, 5: 9, 6: 9, 7: 30}
Layers that will be pruned {'features.9.expand3x3': 8, 'features.11.expand3x3': 9, 'features.12.expand1x1': 30, 'features.12.expand3x3': 30, 'features.3.expand1x1': 4, 'features.6.expand1x1': 7, 'features.6.expand3x3': 7, 'features.4.expand3x3': 2, 'features.12.squeeze': 4, 'features.10.expand3x3': 9, 'features.11.expand1x1': 9, 'features.4.expand1x1': 2, 'features.0': 15, 'features.7.expand3x3': 2, 'features.7.expand1x1': 2, 'features.3.expand3x3': 4, 'features.10.expand1x1': 9, 'features.11.squeeze': 5, 'features.9.expand1x1': 8, 'features.6.squeeze': 2, 'features.3.squeeze': 1, 'features.4.squeeze': 2, 'features.10.squeeze': 2, 'features.7.squeeze': 2}
convolution remaining after pruning {'features.9.expand3x3': 184, 'features.11.expand3x3': 247, 'features.12.expand1x1': 226, 'features.12.expand3x3': 226, 'features.3.expand1x1': 60, 'features.6.expand1x1': 121, 'features.6.expand3x3': 121, 'features.4.expand3x3': 62, 'features.12.squeeze': 60, 'features.10.expand3x3': 183, 'features.11.expand1x1': 247, 'features.4.expand1x1': 62, 'features.0': 49, 'features.7.expand3x3': 126, 'features.7.expand1x1': 126, 'features.3.expand3x3': 60, 'features.10.expand1x1': 183, 'features.11.squeeze': 59, 'features.9.expand1x1': 184, 'features.6.squeeze': 30, 'features.3.squeeze': 15, 'features.4.squeeze': 14, 'features.10.squeeze': 46, 'features.7.squeeze': 30}
Pruning filters.. 
Filters pruned 5.944293478260869%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 93.06 - Val acc: 91.64 - Train loss: 0.2032 - Val loss: 0.2398 - Training time: 120.77s
end number of flops: 452852480.0 	number of params: 1096355.0
diff number of flops: 0.12308066772487927 	diff number of params: 0.11261954712925011
Final Test:
	Score: 88.12
***  Squeeze-degrad
number of flops: 516412928.0 	number of params: 1235496.0
1 iterations to reduce 25.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 3, 1: 0, 2: 8, 3: 6, 4: 6, 5: 6, 6: 32, 7: 39}
Layers that will be pruned {'features.9.expand3x3': 6, 'features.11.expand3x3': 32, 'features.12.expand1x1': 39, 'features.12.expand3x3': 39, 'features.3.expand1x1': 3, 'features.6.expand1x1': 8, 'features.10.expand3x3': 6, 'features.6.expand3x3': 8, 'features.9.expand1x1': 6, 'features.0': 20, 'features.10.expand1x1': 6, 'features.7.expand1x1': 6, 'features.11.expand1x1': 32, 'features.3.expand3x3': 3, 'features.12.squeeze': 5, 'features.7.expand3x3': 6, 'features.7.squeeze': 3, 'features.9.squeeze': 3, 'features.11.squeeze': 4, 'features.10.squeeze': 6, 'features.6.squeeze': 3, 'features.4.squeeze': 1, 'features.3.squeeze': 1}
convolution remaining after pruning {'features.9.expand3x3': 186, 'features.11.expand3x3': 224, 'features.12.expand1x1': 217, 'features.12.expand3x3': 217, 'features.3.expand1x1': 61, 'features.6.expand1x1': 120, 'features.10.expand3x3': 186, 'features.6.expand3x3': 120, 'features.9.expand1x1': 186, 'features.0': 44, 'features.10.expand1x1': 186, 'features.7.expand1x1': 122, 'features.11.expand1x1': 224, 'features.3.expand3x3': 61, 'features.12.squeeze': 59, 'features.7.expand3x3': 122, 'features.7.squeeze': 29, 'features.9.squeeze': 45, 'features.11.squeeze': 60, 'features.10.squeeze': 42, 'features.6.squeeze': 29, 'features.4.squeeze': 15, 'features.3.squeeze': 15}
Pruning filters.. 
Filters pruned 8.355978260869565%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 93.05 - Val acc: 91.59 - Train loss: 0.2023 - Val loss: 0.2412 - Training time: 121.16s
end number of flops: 437624000.0 	number of params: 1040584.0
diff number of flops: 0.15256962738159802 	diff number of params: 0.15776012225049696
Final Test:
	Score: 88.19
***  Squeeze-degrad
number of flops: 516412928.0 	number of params: 1235496.0
1 iterations to reduce 30.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 3, 1: 3, 2: 17, 3: 9, 4: 15, 5: 12, 6: 35, 7: 58}
Layers that will be pruned {'features.9.expand3x3': 15, 'features.11.expand3x3': 35, 'features.12.expand1x1': 58, 'features.12.expand3x3': 58, 'features.3.expand1x1': 3, 'features.6.expand1x1': 17, 'features.6.expand3x3': 17, 'features.10.expand3x3': 12, 'features.4.expand1x1': 3, 'features.7.expand1x1': 9, 'features.9.expand1x1': 15, 'features.4.expand3x3': 3, 'features.7.expand3x3': 9, 'features.11.expand1x1': 35, 'features.3.expand3x3': 3, 'features.10.expand1x1': 12, 'features.0': 13, 'features.11.squeeze': 7, 'features.9.squeeze': 8, 'features.12.squeeze': 6, 'features.3.squeeze': 1, 'features.7.squeeze': 2, 'features.6.squeeze': 3, 'features.4.squeeze': 1}
convolution remaining after pruning {'features.9.expand3x3': 177, 'features.11.expand3x3': 221, 'features.12.expand1x1': 198, 'features.12.expand3x3': 198, 'features.3.expand1x1': 61, 'features.6.expand1x1': 111, 'features.6.expand3x3': 111, 'features.10.expand3x3': 180, 'features.4.expand1x1': 61, 'features.7.expand1x1': 119, 'features.9.expand1x1': 177, 'features.4.expand3x3': 61, 'features.7.expand3x3': 119, 'features.11.expand1x1': 221, 'features.3.expand3x3': 61, 'features.10.expand1x1': 180, 'features.0': 51, 'features.11.squeeze': 57, 'features.9.squeeze': 40, 'features.12.squeeze': 58, 'features.3.squeeze': 15, 'features.7.squeeze': 30, 'features.6.squeeze': 29, 'features.4.squeeze': 15}
Pruning filters.. 
Filters pruned 11.71875%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 90.27 - Val acc: 88.68 - Train loss: 0.2756 - Val loss: 0.3199 - Training time: 121.27s
end number of flops: 417530368.0 	number of params: 970637.0
diff number of flops: 0.1914796370086227 	diff number of params: 0.21437463172685303
Final Test:
	Score: 85.6
***  Squeeze-degrad
number of flops: 516412928.0 	number of params: 1235496.0
1 iterations to reduce 35.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 6, 1: 3, 2: 18, 3: 8, 4: 24, 5: 24, 6: 35, 7: 74}
Layers that will be pruned {'features.9.expand3x3': 24, 'features.11.expand3x3': 35, 'features.12.expand1x1': 74, 'features.12.expand3x3': 74, 'features.3.expand1x1': 6, 'features.6.expand3x3': 18, 'features.6.expand1x1': 18, 'features.4.expand3x3': 3, 'features.10.expand3x3': 24, 'features.0': 19, 'features.4.expand1x1': 3, 'features.7.expand3x3': 8, 'features.11.expand1x1': 35, 'features.3.expand3x3': 6, 'features.7.expand1x1': 8, 'features.9.expand1x1': 24, 'features.10.expand1x1': 24, 'features.7.squeeze': 5, 'features.12.squeeze': 8, 'features.11.squeeze': 11, 'features.9.squeeze': 7, 'features.6.squeeze': 4, 'features.3.squeeze': 1, 'features.10.squeeze': 5}
convolution remaining after pruning {'features.9.expand3x3': 168, 'features.11.expand3x3': 221, 'features.12.expand1x1': 182, 'features.12.expand3x3': 182, 'features.3.expand1x1': 58, 'features.6.expand3x3': 110, 'features.6.expand1x1': 110, 'features.4.expand3x3': 61, 'features.10.expand3x3': 168, 'features.0': 45, 'features.4.expand1x1': 61, 'features.7.expand3x3': 120, 'features.11.expand1x1': 221, 'features.3.expand3x3': 58, 'features.7.expand1x1': 120, 'features.9.expand1x1': 168, 'features.10.expand1x1': 168, 'features.7.squeeze': 27, 'features.12.squeeze': 56, 'features.11.squeeze': 53, 'features.9.squeeze': 41, 'features.6.squeeze': 28, 'features.3.squeeze': 15, 'features.10.squeeze': 43}
Pruning filters.. 
Filters pruned 15.081521739130435%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 89.84 - Val acc: 88.10 - Train loss: 0.2966 - Val loss: 0.3375 - Training time: 121.00s
end number of flops: 389559040.0 	number of params: 889280.0
diff number of flops: 0.2456442918485573 	diff number of params: 0.28022429858129855
Final Test:
	Score: 85.84
***  Squeeze-degrad
number of flops: 516412928.0 	number of params: 1235496.0
1 iterations to reduce 40.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 8, 1: 3, 2: 26, 3: 12, 4: 35, 5: 29, 6: 58, 7: 91}
Layers that will be pruned {'features.9.expand3x3': 35, 'features.11.expand3x3': 58, 'features.12.expand1x1': 91, 'features.12.expand3x3': 91, 'features.3.expand1x1': 8, 'features.6.expand3x3': 26, 'features.6.expand1x1': 26, 'features.4.expand1x1': 3, 'features.10.expand3x3': 29, 'features.3.expand3x3': 8, 'features.0': 20, 'features.11.expand1x1': 58, 'features.10.expand1x1': 29, 'features.9.squeeze': 11, 'features.4.expand3x3': 3, 'features.7.expand3x3': 12, 'features.9.expand1x1': 35, 'features.7.expand1x1': 12, 'features.11.squeeze': 15, 'features.10.squeeze': 10, 'features.12.squeeze': 12, 'features.7.squeeze': 4, 'features.6.squeeze': 3, 'features.3.squeeze': 1}
convolution remaining after pruning {'features.9.expand3x3': 157, 'features.11.expand3x3': 198, 'features.12.expand1x1': 165, 'features.12.expand3x3': 165, 'features.3.expand1x1': 56, 'features.6.expand3x3': 102, 'features.6.expand1x1': 102, 'features.4.expand1x1': 61, 'features.10.expand3x3': 163, 'features.3.expand3x3': 56, 'features.0': 44, 'features.11.expand1x1': 198, 'features.10.expand1x1': 163, 'features.9.squeeze': 37, 'features.4.expand3x3': 61, 'features.7.expand3x3': 116, 'features.9.expand1x1': 157, 'features.7.expand1x1': 116, 'features.11.squeeze': 49, 'features.10.squeeze': 38, 'features.12.squeeze': 52, 'features.7.squeeze': 28, 'features.6.squeeze': 29, 'features.3.squeeze': 15}
Pruning filters.. 
Filters pruned 20.380434782608695%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 82.89 - Val acc: 81.71 - Train loss: 0.4947 - Val loss: 0.5224 - Training time: 119.73s
end number of flops: 359977856.0 	number of params: 786386.0
diff number of flops: 0.3029263279791477 	diff number of params: 0.36350583085659527
Final Test:
	Score: 80.17999999999999
***  Squeeze-degrad
number of flops: 516412928.0 	number of params: 1235496.0
1 iterations to reduce 45.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 6, 1: 4, 2: 34, 3: 16, 4: 36, 5: 49, 6: 68, 7: 102}
Layers that will be pruned {'features.9.expand3x3': 36, 'features.11.expand3x3': 68, 'features.12.expand1x1': 102, 'features.12.expand3x3': 102, 'features.3.expand1x1': 6, 'features.6.expand3x3': 34, 'features.6.expand1x1': 34, 'features.4.expand1x1': 4, 'features.4.expand3x3': 4, 'features.10.expand3x3': 49, 'features.3.expand3x3': 6, 'features.10.expand1x1': 49, 'features.11.expand1x1': 68, 'features.9.expand1x1': 36, 'features.0': 26, 'features.7.expand3x3': 16, 'features.9.squeeze': 5, 'features.7.expand1x1': 16, 'features.6.squeeze': 6, 'features.12.squeeze': 15, 'features.7.squeeze': 8, 'features.10.squeeze': 10, 'features.11.squeeze': 15, 'features.3.squeeze': 2, 'features.4.squeeze': 1}
convolution remaining after pruning {'features.9.expand3x3': 156, 'features.11.expand3x3': 188, 'features.12.expand1x1': 154, 'features.12.expand3x3': 154, 'features.3.expand1x1': 58, 'features.6.expand3x3': 94, 'features.6.expand1x1': 94, 'features.4.expand1x1': 60, 'features.4.expand3x3': 60, 'features.10.expand3x3': 143, 'features.3.expand3x3': 58, 'features.10.expand1x1': 143, 'features.11.expand1x1': 188, 'features.9.expand1x1': 156, 'features.0': 38, 'features.7.expand3x3': 112, 'features.9.squeeze': 43, 'features.7.expand1x1': 112, 'features.6.squeeze': 26, 'features.12.squeeze': 49, 'features.7.squeeze': 24, 'features.10.squeeze': 38, 'features.11.squeeze': 49, 'features.3.squeeze': 14, 'features.4.squeeze': 15}
Pruning filters.. 
Filters pruned 24.38858695652174%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 85.65 - Val acc: 84.23 - Train loss: 0.4185 - Val loss: 0.4560 - Training time: 119.42s
end number of flops: 329656960.0 	number of params: 733522.0
diff number of flops: 0.3616407682187228 	diff number of params: 0.40629350479483545
Final Test:
	Score: 82.17999999999999
***  Squeeze-degrad
number of flops: 516412928.0 	number of params: 1235496.0
1 iterations to reduce 50.00% filters
Perform pruning iteration: 0
junction pruning size: {0: 10, 1: 7, 2: 24, 3: 24, 4: 54, 5: 56, 6: 90, 7: 118}
Layers that will be pruned {'features.9.expand3x3': 54, 'features.11.expand3x3': 90, 'features.12.expand1x1': 118, 'features.12.expand3x3': 118, 'features.3.expand1x1': 10, 'features.6.expand3x3': 24, 'features.6.expand1x1': 24, 'features.4.expand1x1': 7, 'features.0': 26, 'features.10.expand1x1': 56, 'features.10.expand3x3': 56, 'features.11.expand1x1': 90, 'features.7.expand1x1': 24, 'features.9.expand1x1': 54, 'features.4.expand3x3': 7, 'features.7.expand3x3': 24, 'features.9.squeeze': 15, 'features.3.expand3x3': 10, 'features.6.squeeze': 5, 'features.11.squeeze': 19, 'features.12.squeeze': 14, 'features.7.squeeze': 6, 'features.4.squeeze': 3, 'features.10.squeeze': 8, 'features.3.squeeze': 1}
convolution remaining after pruning {'features.9.expand3x3': 138, 'features.11.expand3x3': 166, 'features.12.expand1x1': 138, 'features.12.expand3x3': 138, 'features.3.expand1x1': 54, 'features.6.expand3x3': 104, 'features.6.expand1x1': 104, 'features.4.expand1x1': 57, 'features.0': 38, 'features.10.expand1x1': 136, 'features.10.expand3x3': 136, 'features.11.expand1x1': 166, 'features.7.expand1x1': 104, 'features.9.expand1x1': 138, 'features.4.expand3x3': 57, 'features.7.expand3x3': 104, 'features.9.squeeze': 33, 'features.3.expand3x3': 54, 'features.6.squeeze': 27, 'features.11.squeeze': 45, 'features.12.squeeze': 50, 'features.7.squeeze': 26, 'features.4.squeeze': 13, 'features.10.squeeze': 40, 'features.3.squeeze': 15}
Pruning filters.. 
Filters pruned 29.313858695652176%
Fine tuning to recover from prunning iteration.
Epoch 0 - Train acc: 83.57 - Val acc: 82.13 - Train loss: 0.4727 - Val loss: 0.5106 - Training time: 119.64s
end number of flops: 304051904.0 	number of params: 651581.0
diff number of flops: 0.41122329145098396 	diff number of params: 0.472615856303865
Final Test:
	Score: 80.88
***  AlexNet-degrad
number of flops: 823247104.0 	number of params: 61100840.0
Process finished with exit code 0
